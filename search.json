[
  {
    "objectID": "session2/notebook.html",
    "href": "session2/notebook.html",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "",
    "text": "%matplotlib inline\nimport matplotlib.colors as colors\nimport matplotlib.pyplot as plt\nfrom mpltools import annotation\nimport networkx as nx\nimport numpy as np\nfrom firedrake.adjoint import pyadjoint"
  },
  {
    "objectID": "session2/notebook.html#motivation",
    "href": "session2/notebook.html#motivation",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Motivation",
    "text": "Motivation\n\nWere you interested in yesterday’s session but unclear how to make use of differentiable programming in practice?\nWe computed first-order derivatives for various programs. However, some advanced applications like uncertainty quantification requires Hessians (matrices of second derivatives). How do we calculate those?\nHave you ever wondered how neural network training actually works under the hood?\n\n\n\n\n\nFigure 1: Neural network schematic created with https://alexlenail.me/NN-SVG/index.html.\n\n\n Machine learning (ML) models typically have large numbers of parameters to be tuned and small numbers of outputs. Forward mode doesn’t seem like such a great fit…"
  },
  {
    "objectID": "session2/notebook.html#learning-objectives",
    "href": "session2/notebook.html#learning-objectives",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Learning objectives",
    "text": "Learning objectives\nIn today’s session we will:\n\nLearn about reverse mode and the operator overloading approach, comparing them with forward mode and source transformation, respectively.\nVerify the consistency of code generated by Tapenade under forward and reverse mode using the dot product test.\nCalculate higher order derivatives using Tapenade.\nTry out the Pyadjoint operator overloading AD tool underpinnning the Firedrake finite element library and see showcases of more advanced AD usage.\nLearn about checkpointing and using AD to compute higher order derivatives."
  },
  {
    "objectID": "session2/notebook.html#preparations",
    "href": "session2/notebook.html#preparations",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Preparations",
    "text": "Preparations\n\nTerminology\nRecall from session 1: This course introduces the concept of differentiable programming, a.k.a. automatic differentiation (AD), or algorithmic differentiation. We will use the acronym AD henceforth.\n\n\nNotation\nRecall from session 1: For a differentiable mathematical function \\(f:A\\rightarrow\\mathbb{R}\\) with scalar input (i.e., a single value) from \\(A\\subseteq\\mathbb{R}\\), we make use of both the Lagrange notation \\(f'(x)\\) and Leibniz notation \\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\) for its derivative.\n\nCaution with the physics notation for derivatives \\(\\dot{x}\\). It can refer to both seeds for forward mode and forward mode derivatives.\n\n\nCaution For seed vectors in reverse mode we will use the “bar” notation \\(\\bar{y}\\). This is not a mean value.\n\nRecall from session 1: When it comes to forward derivatives in code, we use the _d notation, which is standard in the AD literature.\nWhen it comes to reverse mode derivatives in code, we use the _b notation (for “backward” or “bar”)."
  },
  {
    "objectID": "session2/notebook.html#brief-history-of-reverse-mode",
    "href": "session2/notebook.html#brief-history-of-reverse-mode",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Brief history of reverse mode",
    "text": "Brief history of reverse mode\n\nReverse mode (a.k.a. backpropagation) was discovered by Linnainmaa in the 1970s.\nThe terminology ‘back-propagating error correction’ had already been introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this.\nSpeelpenning introduced the modern formulation of reverse mode in the late 1980s.\n\n\n Figure 2: Schematic from (Speelpenning, 1980).\n\n\nGriewank improved the feasibility of reverse mode in 1992 by introducing checkpointing."
  },
  {
    "objectID": "session2/notebook.html#recall-directional-derivative-a.k.a.-jacobian-vector-product-jvp",
    "href": "session2/notebook.html#recall-directional-derivative-a.k.a.-jacobian-vector-product-jvp",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)",
    "text": "Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)\n\nConsider a vector-valued function \\(\\mathbf{f}\\) mapping from a subspace \\(A\\subseteq\\mathbb{R}^n\\) into \\(\\mathbb{R}^m\\), for some \\(m,n\\in\\mathbb{N}\\): \\[\\mathbf{f}:A\\rightarrow\\mathbb R^m.\\]\nGiven input \\(\\mathbf{x}\\in A\\) and a seed vector \\(\\dot{\\mathbf{x}}\\in\\mathbb{R}^n\\), forward mode AD allows us to compute the action (matrix-vector product) \\[\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.\\] Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\nHere \\(\\nabla\\mathbf{f}\\) is referred to as the Jacobian for the map, so the above is known as a Jacobian-vector product (JVP).\n\n\nNote The computation is matrix-free. We don’t actually need the Jacobian when we compute this product."
  },
  {
    "objectID": "session2/notebook.html#jacobian-transpose-vector-product-jtvp",
    "href": "session2/notebook.html#jacobian-transpose-vector-product-jtvp",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Jacobian-transpose-vector product (JTVP)",
    "text": "Jacobian-transpose-vector product (JTVP)\nConsider a vector-valued function \\(\\mathbf{f}\\) mapping from a subspace \\(A\\subseteq\\mathbb{R}^n\\) into \\(\\mathbb{R}^m\\), for some \\(m,n\\in\\mathbb{N}\\): \\[\\mathbf{f}:A\\rightarrow\\mathbb{R}^m.\\]\nGiven \\(\\mathbf{x}\\in A\\) and a seed vector \\(\\bar{\\mathbf{y}}\\in\\mathbb{R}^m\\), reverse mode AD allows us to compute the transpose action (transposed matrix-vector product) \\[\\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}.\\]\n\nOptional exercise\nConvince yourself that the JTVP is well defined.\nSolution\n\nWe have \\(\\nabla\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{m\\times n}\\), so \\(\\nabla\\mathbf{f}(\\mathbf{x})^T\\in\\mathbb{R}^{n\\times m}\\). Since \\(\\bar{\\mathbf{y}}\\in\\mathbb{R}^m\\), the dimensions are appropriate to take the JTVP.\n\n\n\nNote Again, the computation is matrix-free. We don’t actually need the Jacobian or its transpose when we compute this product."
  },
  {
    "objectID": "session2/notebook.html#forward-mode-vs.-reverse-mode",
    "href": "session2/notebook.html#forward-mode-vs.-reverse-mode",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Forward mode vs. reverse mode",
    "text": "Forward mode vs. reverse mode\nFor seed vectors \\(\\dot{\\mathbf{x}}\\in\\mathbb{R}^n\\) and \\(\\bar{\\mathbf{y}}\\in\\mathbb{R}^m\\), forward mode and reverse mode compute\n\\[\n    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n    \\quad\\text{and}\\quad\n    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n\\] respectively.\n\nForward mode is more appropriate if \\(n\\ll m\\), i.e., \\(\\#inputs\\ll\\#outputs\\).\n\ne.g., sensitivity analysis or optimisation w.r.t. a small number of parameters.\n\nReverse mode is more appropriate if \\(n\\gg m\\), i.e., \\(\\#inputs\\gg\\#outputs\\).\n\ne.g., ODE/PDE-constrained optimisation (cost function), machine learning training (loss function), goal-oriented error estimation (quantity of interest).\n\nForward mode is computed eagerly, whereas reverse mode is done separately from the primal run.\nReverse mode tends to have higher memory requirements."
  },
  {
    "objectID": "session2/notebook.html#f-g-example-directed-acyclic-graph",
    "href": "session2/notebook.html#f-g-example-directed-acyclic-graph",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "\\(f\\) & \\(g\\) example: Directed Acyclic Graph",
    "text": "\\(f\\) & \\(g\\) example: Directed Acyclic Graph\nRecall the introductory example from yesterday and the DAG representations of the \\(f\\) and \\(g\\) functions.\nRecalling that \\[f(x_1,x_2)=x_1x_2\\] and \\[g(y)=(\\sin(y),\\cos(y)),\\] we have\n\n Figure 3: Directed Acyclic Graph (DAG) for the \\(f\\) function in the \\(f\\) & \\(g\\) example. Generated using tikZ and \\(\\LaTeX\\).\n\n\n Figure 4: Directed Acyclic Graph (DAG) for the \\(g\\) function in the \\(f\\) & \\(g\\) example. Generated using tikZ and \\(\\LaTeX\\)."
  },
  {
    "objectID": "session2/notebook.html#f-g-example-reverse-mode-seed-vectors",
    "href": "session2/notebook.html#f-g-example-reverse-mode-seed-vectors",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "\\(f\\) & \\(g\\) example: reverse mode seed vectors",
    "text": "\\(f\\) & \\(g\\) example: reverse mode seed vectors\nIn reverse mode, gradient information propagates in the opposite direction.\n\n Figure 5: Directed Acyclic Graph (DAG) for the composition of the functions in the \\(f\\) & \\(g\\) example. Generated using tikZ and \\(\\LaTeX\\)."
  },
  {
    "objectID": "session2/notebook.html#f-g-example-reverse-mode",
    "href": "session2/notebook.html#f-g-example-reverse-mode",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "\\(f\\) & \\(g\\) example: reverse mode",
    "text": "\\(f\\) & \\(g\\) example: reverse mode\nThe Fortran code for the two functions is copied in the repository at session2/exercises/fg/f_mod.f90 and session2/exercises/fg/g_mod.f90. This time, they are defined inside modules.\nmodule f_mod\n  implicit none\n\n  private\n  public :: f\n\n  contains\n\n    subroutine f(x, y)\n      implicit none\n      real, dimension(2), intent(in)  :: x\n      real, intent(out) :: y\n      y = x(1) * x(2)\n    end subroutine f\n\nend module f_mod\nmodule g_mod\n  implicit none\n\n  private\n  public :: g\n\n  contains\n\n    subroutine g(y, z)\n      implicit none\n      real, intent(in) :: y\n      real, intent(out), dimension(2) :: z\n      z = [sin(y), cos(y)]\n    end subroutine g\n\nend module g_mod\n\nExercise\n\nRun tapenade -h to review the options.*\nApply Tapenade to each of these subroutines in reverse mode, which will compute the JTVP for some seed vector. Inspect the output files f_mod_b.f90 and g_mod_b.f90 and check they are as you expect.\n(Optional) Inspect the message files f_mod_b.msg and g_mod_b.msg.\n\n*Recall that “tangent mode” is another term for “forward mode”. Similarly, “adjoint mode” is another term for “reverse mode”.\nSolution 1\n\n$ tapenade -h\nTapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n Builds a differentiated program.\n Usage: tapenade [options]* filenames\n  options:\n   -head, -root &lt;proc&gt;     set the differentiation root procedure(s)\n                           See FAQ for refined invocation syntax, e.g.\n                           independent and dependent arguments, multiple heads...\n   -tangent, -d            differentiate in forward/tangent mode (default)\n   -reverse, -b            differentiate in reverse/adjoint mode\n   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n   -specializeactivity &lt;unit_names or %all%&gt;  Allow for several activity patterns per routine\n   -primal, -p             turn off differentiation. Show pointer destinations\n   -output, -o &lt;file&gt;      put all generated code into a single &lt;file&gt;\n   -splitoutputfiles       split generated code, one file per top unit\n   -outputdirectory, -O &lt;directory&gt;  put all generated files in &lt;directory&gt; (default: .)\n   -I &lt;includePath&gt;        add a new search path for include files\n   -tgtvarname &lt;str&gt;       set extension for tangent variables  (default %d)\n   -tgtfuncname &lt;str&gt;      set extension for tangent procedures (default %_d)\n   -tgtmodulename &lt;str&gt;    set extension for tangent modules and types (default %_diff)\n   -adjvarname &lt;str&gt;       set extension for adjoint variables  (default %b)\n   -adjfuncname &lt;str&gt;      set extension for adjoint procedures (default %_b)\n   -adjmodulename &lt;str&gt;    set extension for adjoint modules and types (default %_diff)\n   -modulename &lt;str&gt;       set extension for tangent&adjoint modules and types (default %_diff)\n   -inputlanguage &lt;lang&gt;   language of  input files (fortran, fortran90,\n                           fortran95, or C)\n   -outputlanguage &lt;lang&gt;  language of output files (fortran, fortran90,\n                           fortran95, or C)\n   -ext &lt;file&gt;             incorporate external library description &lt;file&gt;\n   -nolib                  don't load standard libraries descriptions\n   -i&lt;n&gt;                   count &lt;n&gt; bytes for an integer (default -i4)\n   -r&lt;n&gt;                   count &lt;n&gt; bytes for a real (default -r4)\n   -dr&lt;n&gt;                  count &lt;n&gt; bytes for a double real (default -dr8)\n   -p&lt;n&gt;                   count &lt;n&gt; bytes for a pointer (default -p8)\n   -fixinterface           don't use activity to filter user-given (in)dependent vars\n   -noinclude              inline include files\n   -debugTGT               insert instructions for debugging tangent mode\n   -debugADJ               insert instructions for debugging adjoint mode\n   -tracelevel &lt;n&gt;         set the level of detail of trace milestones\n   -msglevel &lt;n&gt;           set the level of detail of error messages\n   -msginfile              insert error messages in output files\n   -dump &lt;file&gt;            write a dump &lt;file&gt;\n   -html                   display results in a web browser\n   -nooptim &lt;str&gt;          turn off optimization &lt;str&gt; (in {activity, difftypes,\n                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n                           deadcontrol, recomputeintermediates,\n                           everyoptim}\n   -version                display Tapenade version information\n Report bugs to &lt;tapenade@inria.fr&gt;.\n\n Solution 2\n\nRunning\n$ cd session2/exercises/fg\n$ ls\nf_mod.f90 g_mod.f90\n$ tapenade -b -head \"f(x)\\(y)\" -adjmodulename _b f_mod.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nCommand: Procedure f understood as f_mod.f\n@@ Created ./f_mod_b.f90 \n@@ Created ./f_mod_b.msg\n$ cat f_mod_b.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE F_MOD_B\n  IMPLICIT NONE\n  PRIVATE \n  PUBLIC :: f\n  PUBLIC :: f_b\n\nCONTAINS\n!  Differentiation of f in reverse (adjoint) mode:\n!   gradient     of useful results: y\n!   with respect to varying inputs: x y\n!   RW status of diff variables: x:out y:in-zero\n  SUBROUTINE F_B(x, xb, y, yb)\n    IMPLICIT NONE\n    REAL, DIMENSION(2), INTENT(IN) :: x\n    REAL, DIMENSION(2) :: xb\n    REAL :: y\n    REAL :: yb\n    xb = 0.0\n    xb(1) = xb(1) + x(2)*yb\n    xb(2) = xb(2) + x(1)*yb\n    yb = 0.0\n  END SUBROUTINE F_B\n\n  SUBROUTINE F(x, y)\n    IMPLICIT NONE\n    REAL, DIMENSION(2), INTENT(IN) :: x\n    REAL, INTENT(OUT) :: y\n    y = x(1)*x(2)\n  END SUBROUTINE F\n\nEND MODULE F_MOD_B\nRunning\n$ tapenade -b -head \"g(y)\\(z)\" -adjmodulename _b g_mod.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nCommand: Procedure g understood as g_mod.g\n@@ Created ./g_mod_b.f90 \n@@ Created ./g_mod_b.msg\n$ cat g_mod_b.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE G_MOD_B\n  IMPLICIT NONE\n  PRIVATE \n  PUBLIC :: g\n  PUBLIC :: g_b\n\nCONTAINS\n!  Differentiation of g in reverse (adjoint) mode:\n!   gradient     of useful results: z\n!   with respect to varying inputs: y z\n!   RW status of diff variables: y:out z:in-zero\n  SUBROUTINE G_B(y, yb, z, zb)\n    IMPLICIT NONE\n    REAL, INTENT(IN) :: y\n    REAL :: yb\n    REAL, DIMENSION(2) :: z\n    REAL, DIMENSION(2) :: zb\n    INTRINSIC COS\n    INTRINSIC SIN\n    yb = COS(y)*zb(1) - SIN(y)*zb(2)\n    zb = 0.0\n  END SUBROUTINE G_B\n\n  SUBROUTINE G(y, z)\n    IMPLICIT NONE\n    REAL, INTENT(IN) :: y\n    REAL, DIMENSION(2), INTENT(OUT) :: z\n    INTRINSIC COS\n    INTRINSIC SIN\n    z = (/SIN(y), COS(y)/)\n  END SUBROUTINE G\n\nEND MODULE G_MOD_B"
  },
  {
    "objectID": "session2/notebook.html#verification-the-dot-product-test",
    "href": "session2/notebook.html#verification-the-dot-product-test",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Verification: the dot product test",
    "text": "Verification: the dot product test\nWe have approaches for computing derivatives with both forward mode and reverse mode. But how do we know the outputs are consistent? This can be verified using the so-called dot product test.\nIn the dot product test, we define the inner product \\(\\psi:=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}\\) of \\(\\bar{\\mathbf{y}}\\) and \\(\\dot{\\mathbf{y}}\\), where \\(\\dot{\\mathbf{y}}=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\\) is the output of forward mode for some seed \\(\\dot{\\mathbf{x}}\\) and \\(\\bar{\\mathbf{y}}\\) is a seed for reverse mode. We check that \\(\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}\\), where \\(\\bar{\\mathbf{x}}^T=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\\) is the output of reverse mode.\n\nOptional exercise\nProve that we do indeed expect \\(\\psi=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}=\\bar{\\mathbf{x}}^T\\dot{\\mathbf{x}}\\).\nSolution\n\n\\[\n    \\psi:=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}\n    =\\bar{\\mathbf{y}}^T(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}})\n    =(\\bar{\\mathbf{y}}^T\\nabla\\mathbf{f}(\\mathbf{x}))\\dot{\\mathbf{x}}\n    =(\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}})^T\\dot{\\mathbf{x}}\n    =\\bar{\\mathbf{x}}^T\\dot{\\mathbf{x}}\n\\]"
  },
  {
    "objectID": "session2/notebook.html#f-g-example-dot-product-test",
    "href": "session2/notebook.html#f-g-example-dot-product-test",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "\\(f\\) & \\(g\\) example: dot product test",
    "text": "\\(f\\) & \\(g\\) example: dot product test\nWe’ve already computed forward mode and reverse mode derivatives for the function \\(f\\) using Tapenade. We can run a dot product test using the code in session2/exercises/fg/dot_product_test.f90.\n! Program for verifying the consistency of the forward mode and reverse mode derivatives of the\n! function f.\nprogram dot_product_test\n  use f_mod_d, only: f_d\n  use f_mod_b, only: f_b\n  implicit none\n\n  real, dimension(2) :: x   ! Primal input\n  real, dimension(2) :: xd  ! Forward mode seed\n  real, dimension(2) :: xb  ! Reverse mode derivative\n  real :: y                 ! Primal output\n  real :: yd                ! Forward mode derivative\n  real :: yb                ! Reverse mode seed\n\n  real :: result1                 ! LHS of dot product test\n  real :: result2                 ! RHS of dot product test\n  real, parameter :: atol = 1e-05 ! Absolute tolerance for the dot product test\n\n  ! Set arbitrary primal input\n  x(:) = [1.2, -2.3]\n\n  ! Call forward mode with some arbitrary seeds\n  xd(:) = [4.2, -0.7]\n  call f_d(x, xd, y, yd)\n\n  ! Choose a seed for reverse mode and evaluate the first result\n  yb = 3.0\n  result1 = dot_product([yd], [yb])\n\n  ! Call reverse mode and evaluate the second result\n  xb(:) = [0.0, 0.0]\n  call f_b(x, xb, y, yb)\n  result2 = dot_product(xd, xb)\n\n  ! Check the two results match within the prespecified tolerance\n  if (abs(result1 - result2) &lt; atol) then\n    write(unit=6, fmt=\"('PASS')\")\n  else\n    write(unit=6, fmt=\"('FAIL with atol=',e10.4)\") atol\n  end if\n\nend program dot_product_test\n\nExercise\n\nRegenerate the forward mode derivative code for f.f90 using Tapenade.\nBuild and run the dot_product_test program.\n(Optional) Why does the test fail if the result1 assignment is moved to after the call to f_b?\n\nSolution 1\n\n$ tapenade -d -tgtmodulename _d -head \"f(x)\\(y)\" f_mod.f90\n\n Solution 2\n\n$ cd exercises/fg\n$ make dot_product_test\n$ ./dot_product_test\nPASS\n\n Solution 3\n\nBecause yb gets reset to zero by f_b. This is done by default because it’s almost always the Right Thing to do."
  },
  {
    "objectID": "session2/notebook.html#second-order",
    "href": "session2/notebook.html#second-order",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Second order",
    "text": "Second order\nFor seed vectors \\(\\dot{\\mathbf{x}}\\in\\mathbb{R}^n\\) and \\(\\bar{\\mathbf{y}}\\in\\mathbb{R}^m\\), forward mode and reverse mode compute\n\\[\n    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n    \\quad\\text{and}\\quad\n    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n\\] respectively.\n\nQuestion\nWhat are two ways we can we use these to compute the Hessian of \\(f\\)?\nSolution 1\n\nGiven a seed vector \\(\\dot{\\mathbf{x}}\\), first apply forward mode to compute \\(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\\). Then apply forward mode to compute the gradient of this (i.e., apply forward mode to the forward mode derivative code). Use vector mode (preferably with compression!) to get the full Hessian.\n\n Solution 2\n\nGiven a seed vector \\(\\dot{\\mathbf{x}}\\), first apply forward mode to compute \\(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\\). Then apply reverse mode to compute the gradient of this (i.e., apply reverse mode to the forward mode derivative code). That is, \\((\\nabla(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}))^T\\bar{\\mathbf{y}}=\\dot{\\mathbf{x}}^T\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})\\bar{\\mathbf{y}}\\). Here the Hessian \\(\\mathbf{H}(\\mathbf{f}):=\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})\\) is symmetric and so the two applications give the Hessian-vector product with the seed. Use vector mode (preferably with compression!) to get the full Hessian."
  },
  {
    "objectID": "session2/notebook.html#speelpenning-example",
    "href": "session2/notebook.html#speelpenning-example",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Speelpenning example",
    "text": "Speelpenning example\nWe compute the Hessian for the classic test case introduced by Speelpenning, which amounts to a reduction of a vector using the product: \\[\n    f(\\mathbf{x})=\\prod_{i=1}^nx_i.\n\\] This test case is of interest because its Hessian is dense.\nThe Speelpenning function is implemented as a Fortran subroutine in session2/exercises/speelpenning/speelpenning.f90:\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\nsubroutine speelpenning(x, f, n)\n  implicit none\n\n  integer, intent(in) :: n            ! Size of the input vector\n  real, dimension(n), intent(in) :: x ! Input vector\n  real, intent(out) :: f              ! Output value, product of all entries\n  integer :: i                        ! Dummy loop index\n\n  f = 1.0\n  do i = 1, n\n    f = f * x(i)\n  end do\nend subroutine speelpenning\n\nExercises\n\nCompute the Hessian of the speelpenning subroutine using Tapenade using two applications of forward mode.*\nBuild and run the view_hessian program in session2/exercises/speelpenning/view_hessian.f90.\n(Optional) Derive the expected Hessian and convince yourself that the output is as you expect.\n(Optional) Compute the Hessian of the speelpenning subroutine using Tapenade using forward mode followed by reverse mode.*\n\n*Hint: You will need to pass the diffsizes_gradient.f90 file as an additional input file for the second derivative calculation. The diffsizes_hessian.f90 file will be used when computing Hessians.\nSolution 1\n\nForward-then-forward approach.\n$ cd session2/exercises/speelpenning\n$ ls\ndiffsizes_gradient.f90  diffsizes_hessian.f90  expected_hessian.f90  Makefile  speelpenning.f90  view_hessian.f90\n$ tapenade -d -vector -tgtmodulename _d -head \"speelpenning(x)\\(f)\" speelpenning.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n@@ Options:  multiDirectional\nCommand: Procedure speelpenning understood as speelpenning_mod.speelpenning\nDiff-liveness analysis turned off\n@@ Created ./speelpenning_dv.f90 \n@@ Created ./speelpenning_dv.msg\n$ cat speelpenning_dv.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE SPEELPENNING_MOD_DV\n  USE DIFFSIZES\n!  Hint: nbdirsmax should be the maximum number of differentiation directions\n  IMPLICIT NONE\n\nCONTAINS\n!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: f\n!   with respect to varying inputs: x\n!   RW status of diff variables: f:out x:in\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n    USE DIFFSIZES\n!  Hint: nbdirsmax should be the maximum number of differentiation directions\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n    REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n! Dummy loop index\n    INTEGER :: i\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    f = 1.0\n    fd = 0.0\n    DO i=1,n\n      DO nd=1,nbdirs\n        fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n      END DO\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING_DV\n\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING(x, f, n)\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n! Dummy loop index\n    INTEGER :: i\n    f = 1.0\n    DO i=1,n\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING\n\nEND MODULE SPEELPENNING_MOD_DV\nAs mentioned in the hint, we need to make use of the diffsizes module defined in diffsizes_gradient.f90:\n! Module containing the nbdirsmax variable required by speelpenning_dv\nmodule diffsizes\n  implicit none\n  integer, parameter :: nbdirsmax = 7\nend module diffsizes\nWe can apply vector forward mode again, passing this in as an additional input:\n$ tapenade -vector -head \"speelpenning_dv(fd)/(x)\" -tgtmodulename _d diffsizes_gradient.f90 speelpenning_dv.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n@@ Options:  multiDirectional\nCommand: Procedure speelpenning_dv understood as speelpenning_mod_dv.speelpenning_dv\nDiff-liveness analysis turned off\n@@ Created ./speelpenning_dv_dv.f90 \n@@ Created ./diffsizes_gradient_dv.msg\n$ cat speelpenning_dv_dv.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE SPEELPENNING_MOD_DV_DV\n  USE DIFFSIZES\n  USE DIFFSIZES\n!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n  IMPLICIT NONE\n\nCONTAINS\n!  Differentiation of speelpenning_dv in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: fd\n!   with respect to varying inputs: x\n!   RW status of diff variables: x:in fd:out\n!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: f\n!   with respect to varying inputs: x\n!   RW status of diff variables: f:out x:in\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING_DV_DV(x, xd0, xd, f, fd, fdd, n, nbdirs, &\n&   nbdirs0)\n    USE DIFFSIZES\n    USE DIFFSIZES\n!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n    REAL, DIMENSION(nbdirsmax0, n), INTENT(IN) :: xd0\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n    REAL, DIMENSION(nbdirsmax0) :: fd0\n    REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n    REAL, DIMENSION(nbdirsmax0, nbdirsmax), INTENT(OUT) :: fdd\n! Dummy loop index\n    INTEGER :: i\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    INTEGER :: nd0\n    INTEGER :: nbdirs0\n    f = 1.0\n    fd = 0.0\n    fd0 = 0.0\n    fdd = 0.0\n    DO i=1,n\n      DO nd=1,nbdirs\n        DO nd0=1,nbdirs0\n          fdd(nd0, nd) = fd(nd)*xd0(nd0, i) + x(i)*fdd(nd0, nd) + xd(nd&\n&           , i)*fd0(nd0)\n        END DO\n        fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n      END DO\n      DO nd0=1,nbdirs0\n        fd0(nd0) = x(i)*fd0(nd0) + f*xd0(nd0, i)\n      END DO\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING_DV_DV\n\n!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: f\n!   with respect to varying inputs: x\n!   RW status of diff variables: f:out x:in\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n    USE DIFFSIZES\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n    REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n! Dummy loop index\n    INTEGER :: i\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    f = 1.0\n    fd = 0.0\n    DO i=1,n\n      DO nd=1,nbdirs\n        fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n      END DO\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING_DV\n\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING(x, f, n)\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n! Dummy loop index\n    INTEGER :: i\n    f = 1.0\n    DO i=1,n\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING\n\nEND MODULE SPEELPENNING_MOD_DV_DV\n\n Solution 2\n\nRunning\n$ cd session2/exercises/speelpenning\n$ make view_hessian\n$ ./view_hessian\nshould give the output\n   0.0 2520.0 1680.0 1260.0 1008.0  840.0  720.0\n2520.0    0.0  840.0  630.0  504.0  420.0  360.0\n1680.0  840.0    0.0  420.0  336.0  280.0  240.0\n1260.0  630.0  420.0    0.0  252.0  210.0  180.0\n1008.0  504.0  336.0  252.0    0.0  168.0  144.0\n 840.0  420.0  280.0  210.0  168.0    0.0  120.0\n 720.0  360.0  240.0  180.0  144.0  120.0    0.0\n\n Solution 3\n\nIt makes sense that the diagonal entries are zero. The matrix is symmetric, as expected. The \\((i,j)^{th}\\) entry of the Hessian is given by \\[\n\\frac{\\partial^2}{\\partial x_i\\partial x_j}\\prod_{k=1}^nx_k\n=\\frac{\\partial}{\\partial x_i}\\prod_{k=1,\\,j\\neq k}^nx_k\n=\\prod_{k=1,\\,j\\neq k,\\,j\\neq i}^nx_k\n\\] As such, the \\((6,7)^{th}\\) and \\((7,6)^{th}\\) entries are both \\[120=5!=\\prod_{i=1}^5x_i=\\frac{\\partial^2}{\\partial x_6\\partial x_7}\\prod_{i=1}^7x_i.\\] Further checks left as an exercise to the reader.\n\n Solution 4\n\nForward-then-reverse approach.\n$ tapenade -b -vector -head \"speelpenning_dv(fd)/(x)\" -adjmodulename _b diffsizes_gradient.f90 speelpenning_dv.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n@@ Options:  multiDirectional\nCommand: Procedure speelpenning_dv understood as speelpenning_mod_dv.speelpenning_dv\n@@ Created ./speelpenning_dv_bv.f90 \n@@ Created ./diffsizes_gradient_bv.msg\n$ cat speelpenning_dv_bv.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE SPEELPENNING_MOD_DV_BV\n  USE DIFFSIZES\n  USE DIFFSIZES\n!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n  IMPLICIT NONE\n\nCONTAINS\n!  Differentiation of speelpenning_dv in reverse (adjoint) mode (with options multiDirectional):\n!   gradient     of useful results: fd\n!   with respect to varying inputs: x fd\n!   RW status of diff variables: x:out fd:in-zero\n!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: f\n!   with respect to varying inputs: x\n!   RW status of diff variables: f:out x:in\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING_DV_BV(x, xb, xd, f, fd, fdb, n, nbdirs, &\n&   nbdirs0)\n    USE DIFFSIZES\n    USE DIFFSIZES\n!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n    REAL, DIMENSION(nbdirsmax0, n) :: xb\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n! Output value, product of all entries\n    REAL :: f\n    REAL, DIMENSION(nbdirsmax0) :: fb\n    REAL, DIMENSION(nbdirsmax) :: fd\n    REAL, DIMENSION(nbdirsmax0, nbdirsmax) :: fdb\n! Dummy loop index\n    INTEGER :: i\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    INTEGER :: nd0\n    INTEGER :: nbdirs0\n    f = 1.0\n    fd = 0.0\n    DO i=1,n\n      DO nd=1,nbdirs\n        CALL PUSHREAL4(fd(nd))\n        fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n      END DO\n      CALL PUSHREAL4(f)\n      f = f*x(i)\n    END DO\n    fb = 0.0\n    xb = 0.0\n    DO i=n,1,-1\n      CALL POPREAL4(f)\n      DO nd0=1,nbdirs0\n        xb(nd0, i) = xb(nd0, i) + f*fb(nd0)\n        fb(nd0) = x(i)*fb(nd0)\n      END DO\n      DO nd=nbdirs,1,-1\n        CALL POPREAL4(fd(nd))\n        DO nd0=1,nbdirs0\n          xb(nd0, i) = xb(nd0, i) + fd(nd)*fdb(nd0, nd)\n          fb(nd0) = fb(nd0) + xd(nd, i)*fdb(nd0, nd)\n          fdb(nd0, nd) = x(i)*fdb(nd0, nd)\n        END DO\n      END DO\n    END DO\n    fdb = 0.0\n  END SUBROUTINE SPEELPENNING_DV_BV\n\n!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: f\n!   with respect to varying inputs: x\n!   RW status of diff variables: f:out x:in\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n    USE DIFFSIZES\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n    REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n! Dummy loop index\n    INTEGER :: i\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    f = 1.0\n    fd = 0.0\n    DO i=1,n\n      DO nd=1,nbdirs\n        fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n      END DO\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING_DV\n\n! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n  SUBROUTINE SPEELPENNING(x, f, n)\n    IMPLICIT NONE\n! Size of the input vector\n    INTEGER, INTENT(IN) :: n\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: x\n! Output value, product of all entries\n    REAL, INTENT(OUT) :: f\n! Dummy loop index\n    INTEGER :: i\n    f = 1.0\n    DO i=1,n\n      f = f*x(i)\n    END DO\n  END SUBROUTINE SPEELPENNING\n\nEND MODULE SPEELPENNING_MOD_DV_BV\nNote that running the code requires either PUSHREAL4 and POPREAL4 to be imported from Tapenade’s ‘ADFirstAidKit’, or for these subroutines for pushing to and popping from a stack to be implemented manually."
  },
  {
    "objectID": "session2/notebook.html#approach-2-operator-overloading",
    "href": "session2/notebook.html#approach-2-operator-overloading",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Approach 2: Operator overloading",
    "text": "Approach 2: Operator overloading\nThe operator overloading approach is fundamentally different to source transformation. It does not generate additional source files and instead generates derivatives at runtime.\nThe general idea is to record all operations applied to variables involved in the computation so that derivatives can be computed by applying the chain rule to the derivatives of those operations.\nDepending on the AD tool, the user experience is something like the following:\n\nMark (input) variables to compute derivatives with respect to as independent.\nMark (output) variables to compute derivatives of as dependent.\nCall driver methods supported by the AD tool to compute derivatives.\nRun the program."
  },
  {
    "objectID": "session2/notebook.html#operator-overloading",
    "href": "session2/notebook.html#operator-overloading",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Operator overloading",
    "text": "Operator overloading\nThe approach fits well with another ‘OO’: object orientation. In classic implementations such as ADOL-C, basic types are enriched by introducing corresponding structs (derived types in Fortran) that hold both the standard (forward) value and also the gradient. For example,\ntype adouble\n  double precision :: val\n  double precision :: grad\nend type adouble\nIn order to compute derivatives of expressions involving adoubles, we need to overload the operators that act on doubles so that they operate on the val attribute and also provide code for the derivative(s). That is, we extend the definition of such operators (+, -, *, /, **, and others) so that they may be applied to adoubless.\nSince forward mode evaluates eagerly (i.e., computes the derivative at the same time as the function evaluation), the adouble approach facilitates this. This is sometimes referred to as the dual number approach."
  },
  {
    "objectID": "session2/notebook.html#operator-overloading-tape",
    "href": "session2/notebook.html#operator-overloading-tape",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Operator overloading: tape",
    "text": "Operator overloading: tape\nIn the case of reverse mode, the dual number approach is insufficient. As mentioned previously, we need to keep track of operations from the primal code, as well as primal values when executing reverse mode.\nA fundamental concept related to the operator overloading AD approach is tape. This is the name used for the record of all the operations that have been executed. Consider the following lines of Fortran code for an example evaluation of the \\(f\\) function considered previously:\ntype(adouble) :: x1\ntype(adouble) :: x2\ntype(adouble) :: y\n\nx1 = 2.0\nx2 = 3.0\ny = x1 * x2\nConceptually, the tape would be something like:\n\n\n\nIndex\nOperation\nDependencies\nOutputs\n\n\n\n\n0\nAssignment\n2.0\nx1\n\n\n1\nAssignment\n3.0\nx2\n\n\n2\nMultiplication\nx1, x2\ny\n\n\n\nThat is, it records each operation in order, as well as the relevant dependencies and outputs. In practice, tapes are much more sophisticated than this, but hopefully the idea is clear. The tape can also be visualised as a DAG, as illustrated in subsequent cells.\nGiven that we know the derivatives of addition, exponentiation, and multiplication, we can unroll the tape to compute the desired derivative code. As an alternative to the dual number approach, forward mode can be implemented in this way.\n\nQuestion\nWhat’s the difference between forward and reverse mode as far as tape unrolling is concerned?\nSolution\n\nIn forward mode, we unroll the tape in the same order it was written (increasing index). Values for the primal code are computed eagerly as part of this unrolling.\nFor reverse mode, we do need to first unroll the tape in the forward direction to compute primal variables. Then we unroll the tape in the reverse direction (decreasing index) to accumulate derivatives."
  },
  {
    "objectID": "session2/notebook.html#operator-overloading-example-in-pyadjoint",
    "href": "session2/notebook.html#operator-overloading-example-in-pyadjoint",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Operator overloading example in Pyadjoint",
    "text": "Operator overloading example in Pyadjoint\nWe demonstrate operator overloading using Pyadjoint in the following. We define some utility functions for interrogating the Tape class.\n\ndef print_tape():\n    \"\"\"\n    Print the current working tape entry-by-entry.\n    \"\"\"\n    tape = pyadjoint.get_working_tape()\n    print(\"Tape:\")\n    for i, block in enumerate(tape.get_blocks()):\n        print(f\"{i}: {block}\")\n\n\ndef plot_dag(pos=None, n=4):\n    \"\"\"\n    Plot the DAG for the current working tape.\n\n    :kwarg pos: list of tuples for the node positions\n    :kwarg n: number of nodes in final graph for tracking sub-DAGs as they progress\n    \"\"\"\n    tape = pyadjoint.get_working_tape()\n    graph = tape.create_graph()\n    fig, axes = plt.subplots(figsize=(5, 3))\n\n    # Use the default automatic node positioning if none is specified\n    if pos is not None:\n        pos = {node: p for node, p in zip(graph.nodes, pos)}\n\n    # Plot the DAG with the truncated colormap\n    nx.draw(graph, pos, ax=axes, node_color=range(len(graph.nodes)), node_size=800, cmap=plt.cm.coolwarm)\n    plt.show()\n\nWe start by enabling annotation of the tape with pyadjoint.continue_annotation.\nHaving done so, we create two AdjFloats and assign them the values 2 and 3.\n\ndef f(x1, x2):\n    \"\"\"f function as defined in previous examples.\"\"\"\n    return x1 * x2\n\n\n# Enable annotation of the tape\npyadjoint.continue_annotation();\n\n# Create some AdjFloats\nx1 = pyadjoint.AdjFloat(2.0)\nx2 = pyadjoint.AdjFloat(3.0)\n\nLet’s now combine the two AdjFloats using a multiplication operation.\n\ny = f(x1, x2)\n\n# Interrogate the tape\nprint_tape()\nplot_dag(pos=[(0, 0), (1, 0.5), (0, 1), (2, 0.5)])\n\nThe tape shows an entry for the product, displaying the float values for x1 and x2. Note that AdjFloat assignments are not shown on the tape in pyadjoint, although assignments for finite element fields are. The DAG shows four nodes: two for the input AdjFloats, one for the multiplication operation, and one for the output AdjFloats.\nHaving finished the computation of interest, we can stop annotating the tape.\n\npyadjoint.pause_annotation();\n\nTo compute derivatives, we need to specify which are the independent variables. These are declared as Controls, as follows.\n\nxc = [pyadjoint.Control(x1), pyadjoint.Control(x2)]\n\nHaving marked variables as independent, we can stop annotating the tape and then compute the gradient.\n\n# Inspect the docstring for the compute_gradient driver\nprint(\"    \\\"\\\"\\\"\\n\" + pyadjoint.compute_gradient.__doc__ + \"\\n    \\\"\\\"\\\"\\n\")\n\n# Compute the gradient of the output with respect to the inputs\ndydx = pyadjoint.compute_gradient(y, xc)\n\nprint(f\"x = [x1, x2] = [{float(x1)}, {float(x2)}]\")\nprint(f\"y = x1 * x2 = {float(y)}\")\nprint(f\"dydx = [{float(dydx[0])}, {float(dydx[1])}] ?= [x2, x1] = [{float(x2)}, {float(x1)}]\")\n\nNote that we haven’t specified whether to use forward or reverse mode. Pyadjoint is set up such that it will choose which approach to used, depending on whether there are more inputs or outputs.\n\nExercise\nWe’ve computed first derivatives of \\(f\\). Now let’s consider its second derivatives. 1. What would you expect the Hessian of \\(f\\) to be? 2. Take a look at pyadjoint.compute_hessian.__doc__. Compute the compressed Hessian with a single call to the compute_hessian driver function.\nSolution 1\n\n\\[\n    \\frac{\\partial^2f}{\\partial x_1^2}=\\frac{\\partial}{\\partial x_1}x_2=0,\n    \\quad\\frac{\\partial^2f}{\\partial x_1x_2}=\\frac{\\partial}{\\partial x_1}x_1=1,\n    \\quad\\frac{\\partial^2f}{\\partial x_2x_1}=\\frac{\\partial}{\\partial x_2}x_2=1,\n    \\quad\\frac{\\partial^2f}{\\partial x_2^2}=\\frac{\\partial}{\\partial x_2}x_1=0,\n\\] so we expect \\(H(f)=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}\\).\n\n Solution 2\n\nWith the code\n# Define seed vector for second derivative\nxdd = [pyadjoint.AdjFloat(1.0), pyadjoint.AdjFloat(1.0)]\n\n# Apply driver function\nd2ydx2 = pyadjoint.compute_hessian(y, xc, xdd)\n\nprint(f\"x = [x1, x2] = [{float(x1)}, {float(x2)}]\")\nprint(f\"y = x1 * x2\")\n\n# Decompress Hessian before printing\nprint(f\"d2ydx2 = [0.0, {float(d2ydx2[0])}; {float(d2ydx2[1])}, 0.0] ?= [0.0, 1.0; 1.0, 0.0]\")\nwe should get\nx = [x1, x2] = [2.0, 3.0]\ny = x1 * x2\nd2ydx2 = [0.0, 1.0; 1.0, 0.0] ?= [0.0, 1.0; 1.0, 0.0]"
  },
  {
    "objectID": "session2/notebook.html#source-transformation-vs.-operator-overloading",
    "href": "session2/notebook.html#source-transformation-vs.-operator-overloading",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Source transformation vs. operator overloading",
    "text": "Source transformation vs. operator overloading\nWe already evaluated the differences between modes (forward and reverse). How about the differences between approaches?\n\nST is done as a preprocessing step, whereas OO is done at runtime.\nST is fairly clear, whereas OO is somewhat of a ‘black box’ (unless you’re able to inspect the tape).\nOO’s tape requires memory.\nThere are only a few ST tools, but very many OO tools! See below.\n\nLLVM: Enzyme \nC/C++: ADIC, ADOL-C, Torch Autograd, CoDiPack, Sacado, dco/c++ [commercial] (about two dozen!)\nFortran:Differentia, lots of abandonware…\nPython: PyTorch Autograd, Jax, PyADOL-C.\nJulia: Enzyme, Zygote, ForwardDiff, DifferentiationInterface (about two dozen overall!)\nDomain-specific: dolfin-adjoint/pyadjoint (Python/UFL - Firedrake & FEniCS)\nAnd many more! https://autodiff.org/?module=Tools"
  },
  {
    "objectID": "session2/notebook.html#levels-of-abstraction",
    "href": "session2/notebook.html#levels-of-abstraction",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Levels of abstraction",
    "text": "Levels of abstraction\nSo far, we have considered AD applied to elementary operations (+, -, *, /, **) applied to intrinsic Fortran types like reals and arrays thereof. Most of the early AD tools worked in this way, tracking operations applied to real numbers and chaining together their derivatives. However, code differentiated with a source transformation tool in this manner can quickly become difficult to read for large codes. And under the operator overloading approach, the memory footprint of the tape can become very large.\nBy raising the level of abstraction, some of these difficulties can be avoided.\n\nMedium-level: API calls\nExample:\n\nAD in PETSc\n\nLatest implementation.\n(Note: old draft implementation differentiated elementary operators.)\n\n\n\n\nHigh-level: operations on fields\nExamples:\n\nAD in Firedrake using Pyadjoint/dolfin-adjoint.\nAD in PSyclone using PSyAD.\n\n\nShowcase\nOpen and work through the following Firedrake Jupyter notebooks that were copied over.\nsession2/exercises/firedrake/06-pde-constrained-optimisation.ipynb\nsession2/exercises/firedrake/11-extract-adjoint-solutions.ipynb"
  },
  {
    "objectID": "session2/notebook.html#goal-oriented-error-estimation",
    "href": "session2/notebook.html#goal-oriented-error-estimation",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Goal-oriented error estimation",
    "text": "Goal-oriented error estimation\nNavigate to https://mesh-adaptation.github.io/docs/demos/point_discharge2d.py.html."
  },
  {
    "objectID": "session2/notebook.html#checkpointing",
    "href": "session2/notebook.html#checkpointing",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Checkpointing",
    "text": "Checkpointing\nRecall our discussion about tape unrolling and how for reverse mode we first unroll the tape to run the primal code.\n\nQuestion\nUnder what circumstances can we get away without an initial forward pass when running reverse mode?\nSolution\n\nTwo possible answers:\nIf the variables involved in the forward computation had their values assigned exactly once and those values are still available in memory then we already have the forward data required for the reverse pass.*\nIf the problem is linear in the independent variables then the reverse mode code will be independent of those variables. As such, there is no need to compute them.\n*Although your AD tool might not be aware of this or be able to make use of the information."
  },
  {
    "objectID": "session2/notebook.html#checkpointing-1",
    "href": "session2/notebook.html#checkpointing-1",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Checkpointing",
    "text": "Checkpointing\nIn scientific programming, time-dependent problems are typically solved using a timestepping method such as the theta-method we saw in the first session. When writing code for such methods, it’s common practice to overwrite the variable for the approximation at a given timestep as we progress through the steps. In such a formulation, the value is no longer in memory and needs to be recomputed.\nIn some cases, it’s possible to keep the full state on the tape at every timestep, whereby all the information required for reverse mode is available - see Figure 4.\n\n Figure 4: Time-dependent adjoint problem solved without loading checkpoints. Generated using tikZ and \\(\\LaTeX\\).\n\n\nThis quickly becomes infeasible for real world problems, with the amount of memory required at each timestep taking a large chunk of the available RAM. In the extreme case where only one timestep’s worth of data fits in memory, a reverse mode propagation requires checkpointing at each timestep as in Figure 5.\n\n Figure 5: Time-dependent adjoint problem solved with checkpointing at every timestep. Generated using tikZ and \\(\\LaTeX\\).\n\n\nIn intermediate cases, checkpointing can be done on the basis of some fixed frequency or in a more intelligent way using the revolve algorithm (Griewank & Walther, 2000)."
  },
  {
    "objectID": "session2/notebook.html#further-applications",
    "href": "session2/notebook.html#further-applications",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Further applications",
    "text": "Further applications\n\nSensitivity analysis. \nData assimilation. \nUncertainty quantification. \nOnline training in machine learning. \nPDE-constrained optimisation. \nGoal-oriented error estimation and mesh adaptation."
  },
  {
    "objectID": "session2/notebook.html#summary",
    "href": "session2/notebook.html#summary",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "Summary",
    "text": "Summary\nIn today’s session we:\n\nLearnt about reverse mode and the operator overloading approach, comparing them with forward mode and source transformation, respectively.\nVerified the consistency of code generated by Tapenade under forward and reverse mode using the dot product test.\nCalculated higher order derivatives using Tapenade.\nTried out the Pyadjoint operator overloading AD tool underpinnning the Firedrake finite element library and saw showcases of more advanced AD usage.\nLearnt about checkpointing and using AD to compute higher order derivatives."
  },
  {
    "objectID": "session2/notebook.html#references",
    "href": "session2/notebook.html#references",
    "title": "Session 2: Reverse mode differentiation and operator overloading",
    "section": "References",
    "text": "References\n\nS. Linnainmaa. Taylor expansion of the accumulated rounding error. BIT, 16(2):146–160, 1976, doi:10.1007/BF01931367.\nB. Speelpenning. Compiling fast partial derivatives of functions given by algorithms. University of Illinois, 1980, doi:10.2172/5254402.\nA. Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods & Software, 1:35–54, 1992, doi:10.1080/10556789208805505.\nA. Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS) 26.1: 19-45, 2000, doi:10.1145/347837.347846.\nJ. Huckelheim, et al. A taxonomy of automatic differentiation pitfalls, WIREs Data Mining Knowl Discovery, 2024, doi:10.1002/widm.1555."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Differentiable Programming",
    "section": "",
    "text": "Derivatives are at the heart of scientific programming. From the Jacobian matrices used to solve nonlinear systems to the gradient vectors used for optimisation methods, from the backpropagation operation in machine learning to the data assimilation methods used in weather forecasting, all of these techniques rely on derivative information. Differentiable programming (also known as automatic/algorithmic differentiation (AD)) provides a suite of tools for users to compute derivatives of quantities in their code without any manual encoding.\nIn Session 1, we will learn about the history and mathematical background of differentiable programming and investigate “forward mode” using the Tapenade AD tool.\nIn Session 2, we will learn about adjoint methods and “reverse mode”, investigate deploying reverse mode using pyadjoint (an operator-overloading algorithmic differentiation framework for Python), and see some demonstrations of more advanced usage."
  },
  {
    "objectID": "index.html#notebooks",
    "href": "index.html#notebooks",
    "title": "Introduction to Differentiable Programming",
    "section": "📓 Notebooks",
    "text": "📓 Notebooks\n\n🔗 Session 1: Forward Mode AD\n🔗 Session 2: Reverse Mode AD"
  },
  {
    "objectID": "session1/notebook.html",
    "href": "session1/notebook.html",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "",
    "text": "%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpltools import annotation\nimport numpy as np\nimport pandas\ndef print_matrix(filename):\n    \"\"\"Function for printing a matrix saved to a text file from Fortran.\"\"\"\n    print(np.array([[float(entry) for entry in line[:-2].split(\",\")] for line in open(filename).readlines()]))"
  },
  {
    "objectID": "session1/notebook.html#motivation",
    "href": "session1/notebook.html#motivation",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Motivation",
    "text": "Motivation\nDifferentiable programming is an enabling technology. Given scientific code for computing some quantity, it allows us to generate derivatives of quantities involved in the computation without any need for deriving or hand-coding the derivative expressions involved.\nSome motivating examples: * Computing the Jacobian for a nonlinear system. * Computing the gradient required for an ODE- or PDE-constrained optimisation method. * The backpropagation operation used for training machine learning models. * Computing Hessians for uncertainty quantification methods. * Solving the adjoint problems involved in data assimilation methods commonly used for weather forecasting."
  },
  {
    "objectID": "session1/notebook.html#learning-objectives",
    "href": "session1/notebook.html#learning-objectives",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Learning objectives",
    "text": "Learning objectives\nIn today’s session we will:\n\nGet a brief history of automatic differentiation.\nLearn about forward mode and the source transformation approach.\nTry out the Tapenade source transformation AD tool applied to some test problems.\nVerify the code generated by Tapenade both manually and using the Taylor test.\nLearn about compressed and matrix-free approaches for sparse problems."
  },
  {
    "objectID": "session1/notebook.html#preparations",
    "href": "session1/notebook.html#preparations",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Preparations",
    "text": "Preparations\n\nTerminology\nThis course introduces the concept of differentiable programming, a.k.a. automatic differentiation (AD), or algorithmic differentiation. We will use the acronym AD henceforth.\n\n\nNotation\nFor a differentiable mathematical function \\(f:A\\rightarrow\\mathbb{R}\\) with scalar input (i.e., a single value) from \\(A\\subseteq\\mathbb{R}\\), we make use of both the Lagrange notation \\(f'(x)\\) and Leibniz notation \\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\) for its derivative.\n\nCaution with the physics notation for derivatives \\(\\dot{x}\\). It won’t always mean what you expect! (See later.)\n\nSimilarly, for \\(m\\in\\mathbb{N}\\) dimensional, differentiable, vector-valued function \\(\\mathbf{f}:A\\rightarrow\\mathbb{R}^m\\) with scalar input, we have derivative notations \\(\\mathbf{f}'(x)\\) and \\(\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}\\).\nFor a differentiable function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) is written as \\(f=f(x,y)\\) then we have the partial derivatives \\(\\frac{\\partial f}{\\partial x}\\) and \\(\\frac{\\partial f}{\\partial y}\\) with respect to first and second components, respectively. We use \\[\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)\\] to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\nWhen it comes to derivatives in code, we use the _d notation (for “derivative” or “dot”), which is standard in the AD literature. Its meaning will be described in due course."
  },
  {
    "objectID": "session1/notebook.html#history",
    "href": "session1/notebook.html#history",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "History",
    "text": "History\n\nOrigins of AD in 1950s.\nHowever, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\nForward mode (the subject of this session) was discovered by Wengert in 1964.\nFurther developed by Griewank in the late 1980s.\n\n\n Figure 1: Header of (R. E. Wengert, 1964)."
  },
  {
    "objectID": "session1/notebook.html#idea",
    "href": "session1/notebook.html#idea",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Idea",
    "text": "Idea\nThe idea of AD is to treat a model as a sequence of elementary instructions (e.g., addition, multiplication, exponentiation). Here a model could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the chain rule.\n\nRecap on A-level maths: the Chain Rule\nConsider two composable, differentiable (mathematical) functions, \\(f\\) and \\(g\\), with composition \\(h=f\\circ g\\). By definition, this means \\[h(x)=(f\\circ g)(x)=g(f(x)).\\]\nThen the chain rule states that the derivative of \\(h\\) may be computed in terms of the derivatives of \\(f\\) and \\(g\\) using the formula \\[h'(x)=(f\\circ g)'(x)=(f\\circ g')(x)\\,f'(x)=g'(f(x))\\,f'(x).\\]\nEquivalently, in Leibniz notation: \\[\\frac{\\mathrm{d}h}{\\mathrm{d}x}=\\frac{\\mathrm{d}g}{\\mathrm{d}f}\\frac{\\mathrm{d}f}{\\mathrm{d}x}.\\]\nFor variables with multiple arguments, the result is equivalent for each partial derivative, e.g., \\[\\frac{\\partial h}{\\partial x}=\\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x}.\\]"
  },
  {
    "objectID": "session1/notebook.html#f-g-example",
    "href": "session1/notebook.html#f-g-example",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example",
    "text": "\\(f\\) & \\(g\\) example\nConsider two functions acting on real numbers: \\[f(x,y)=xy\\] and \\[g(z)=(\\sin(z),\\cos(z)).\\] Here \\(f:\\mathbb{R}^2\\rightarrow\\mathbb{R}\\) takes two inputs and returns a single output, while \\(g:\\mathbb{R}\\rightarrow\\mathbb{R}^2\\) takes a single input and returns two outputs.\n\nOptional exercise\nConvince yourself that it is well defined for these functions may be composed in either order. (Although they won’t necessarily give the same value!)\nSolution\n\nThe image of \\(f\\) is the set of all real numbers, so its image is the same as the domain of \\(g\\) (i.e., \\(\\text{im}(f)=\\mathbb{R}=\\text{dom}(g)\\)).\nThe image of \\(g\\) is \\([-1,1]^2=[-1,1]\\times[-1,1]\\) because \\(\\sin\\) and \\(\\cos\\) give values between -1 and 1. Since this is a subset of \\(\\mathbb{R}^2\\), the image of \\(g\\) is a subset of the domain of \\(f\\) (i.e., \\(\\text{im}(g)\\subset\\text{dom}(f)\\))."
  },
  {
    "objectID": "session1/notebook.html#f-g-example-directed-acyclic-graph",
    "href": "session1/notebook.html#f-g-example-directed-acyclic-graph",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example: Directed Acyclic Graph",
    "text": "\\(f\\) & \\(g\\) example: Directed Acyclic Graph\nWe can visualise the functions in terms of DAGs.\nRecalling that \\[f(x_1,x_2)=x_1x_2\\] and \\[g(y)=(\\sin(y),\\cos(y))\\, ,\\] we have\n\n Figure 2: Directed Acyclic Graph (DAG) for the \\(f\\) function in the \\(f\\) & \\(g\\) example. Generated using tikZ and \\(\\LaTeX\\).\n\n\n Figure 3: Directed Acyclic Graph (DAG) for the \\(g\\) function in the \\(f\\) & \\(g\\) example. Generated using tikZ and \\(\\LaTeX\\)."
  },
  {
    "objectID": "session1/notebook.html#f-g-example-composition",
    "href": "session1/notebook.html#f-g-example-composition",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example: composition",
    "text": "\\(f\\) & \\(g\\) example: composition\nConsider the composition \\(h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2\\), which is given by \\[h(x_1,x_2)=(f\\circ g)(x_1,x_2)=g(f(x_1,x_2))=g(x_1x_2)=(\\sin(x_1x_2),\\cos(x_1x_2)).\\]\nFor the derivative of each component, \\[\n\\frac{\\partial f}{\\partial x_1}=\\frac{\\partial}{\\partial x_1}x_1x_2=x_2,\n\\quad\\frac{\\partial f}{\\partial x_2}=\\frac{\\partial}{\\partial x_2}x_1x_2=x_1,\n\\quad\\frac{\\partial g}{\\partial y}=\\frac{\\partial}{\\partial y}(\\sin(y),\\cos(y))=(\\cos(y),-\\sin(y)).\n\\]\nIntroduce the notation \\(g(y)=(g_1(y),g_2(y))\\) so that \\[\n\\frac{\\partial g_1}{\\partial y}=\\cos(y),\n\\quad\\frac{\\partial g_2}{\\partial y}=-\\sin(y).\n\\] Similarly \\(h(x_1,x_2)=(h_1(x_1,x_2),h_2(x_1,x_2))=(g_1(f(x_1,x_2)),g_2(f(x_1,x_2)))\\).\nLet’s use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs. \\[\n\\frac{\\partial h_1}{\\partial x_1}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x_1}=\\cos(y)x_2=x_2\\cos(x_1x_2),\n\\quad\\frac{\\partial h_1}{\\partial x_2}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x_2}=\\cos(y)x_1=x_1\\cos(x_1x_2),\n\\] where \\(y=f(x_1,x_2)\\) and \\[\n\\quad\\frac{\\partial h_2}{\\partial x_1}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x_1}=-\\sin(y)x_2=-x_2\\sin(x_1x_2),\n\\quad\\frac{\\partial h_2}{\\partial x_2}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x_2}=-\\sin(y)x_1=-x_1\\sin(x_1x_2).\n\\]\nWe will come back to these formulae to verify the correctness of our AD computations."
  },
  {
    "objectID": "session1/notebook.html#directional-derivative-a.k.a.-jacobian-vector-product-jvp",
    "href": "session1/notebook.html#directional-derivative-a.k.a.-jacobian-vector-product-jvp",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Directional derivative, a.k.a. Jacobian-vector product (JVP)",
    "text": "Directional derivative, a.k.a. Jacobian-vector product (JVP)\nConsider a vector-valued function \\(\\mathbf{f}\\) mapping from a subspace \\(A\\subseteq\\mathbb{R}^n\\) into \\(\\mathbb{R}^m\\), for some \\(m,n\\in\\mathbb{N}\\): \\[\\mathbf{f}:A\\rightarrow\\mathbb R^m.\\]\nGiven input \\(\\mathbf{x}\\in A\\) and a seed vector \\(\\dot{\\mathbf{x}}\\in\\mathbb{R}^n\\), forward mode AD allows us to compute the action (matrix-vector product) \\[\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.\\] Here \\(\\nabla\\mathbf{f}\\) is referred to as the Jacobian for the map, so the above is known as a Jacobian-vector product (JVP). You will also hear the related term tangent linear model (TLM).\nThink of the seed vector as the direction in which we want to compute the derivative. In practice, the seed vector is often a derivative of some upstream code from outside of the part of the program being differentiated. That is, the upstream code is passive, whereas the part we are interested in is active, as far as AD is concerned.\n\nNote The computation is matrix-free. We don’t actually need to assemble the Jacobian when we compute this product."
  },
  {
    "objectID": "session1/notebook.html#f-g-example-seed-vectors",
    "href": "session1/notebook.html#f-g-example-seed-vectors",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example: Seed vectors",
    "text": "\\(f\\) & \\(g\\) example: Seed vectors\nLet’s revisit the DAG interpretation and consider how the derivatives work.\n\n Figure 4: Directed Acyclic Graph (DAG) for the composition of the functions in the \\(f\\) & \\(g\\) example."
  },
  {
    "objectID": "session1/notebook.html#approach-1-source-transformation",
    "href": "session1/notebook.html#approach-1-source-transformation",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Approach 1: Source transformation",
    "text": "Approach 1: Source transformation\nHigh level idea: Given some (code) function f(x) and a seed vector x_d, generate the code for the function f_d(x, x_d) for its (directional) derivative.\n\nNotes on the source transformation approach:\n\nStatic analysis, done ahead of time.\nOften the difficult part is hooking the differentiated code into the wider model/build system.\nLimited number of tools.\n\n\n\nSource transformation tools:\n\nTapenade (C, Fortran, Julia*)\nOpenAD (Fortran) [no longer maintained]\nTAF (Fortran) [commercial]\nPSyAD* (domain-specific)\n\n*Work in progress"
  },
  {
    "objectID": "session1/notebook.html#f-g-example-source-transformation",
    "href": "session1/notebook.html#f-g-example-source-transformation",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example: source transformation",
    "text": "\\(f\\) & \\(g\\) example: source transformation\nBelow we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at session1/exercises/fg/f.f90 and session1/exercises/fg/g.f90, respectively.\nsubroutine f(x, y)\n  implicit none\n  real, dimension(2), intent(in)  :: x\n  real, intent(out) :: y\n  y = x(1) * x(2)\nend subroutine f\nsubroutine g(y, z)\n  implicit none\n  real, intent(in) :: y\n  real, intent(out), dimension(2) :: z\n  z = [sin(y), cos(y)]\nend subroutine g\n\nExercise\n\nRun tapenade -h to see all the options.\nApply Tapenade to each of these subroutines using the -head argument.* Inspect the output files f_d.f90 and g_d.f90 and check they are as you expect.\n(Optional) Inspect the message files f_d.msg and g_d.msg.\n(Optional) See https://gitlab.inria.fr/tapenade/tapenade/-/blob/develop/src/README.md for details on the internals of Tapenade.\n\n*The syntax is -head \"procedure_name(dependent_variables)/(independent_variables)\", where here procedure_name, dependent_variables, and independent_variables all need substituting as appropriate. If multiple variables are used then they should be comma-separated.\nSolution 1\n\n$ tapenade -h\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n Builds a differentiated program.\n Usage: tapenade [options]* filenames\n  options:\n   -head, -root &lt;proc&gt;     set the differentiation root procedure(s)\n                           See FAQ for refined invocation syntax, e.g.\n                           independent and dependent arguments, multiple heads...\n   -tangent, -d            differentiate in forward/tangent mode (default)\n   -reverse, -b            differentiate in reverse/adjoint mode\n   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n   -specializeactivity &lt;unit_names or %all%&gt;  Allow for several activity patterns per routine\n   -primal, -p             turn off differentiation. Show pointer destinations\n   -output, -o &lt;file&gt;      put all generated code into a single &lt;file&gt;\n   -splitoutputfiles       split generated code, one file per top unit\n   -outputdirectory, -O &lt;directory&gt;  put all generated files in &lt;directory&gt; (default: .)\n   -I &lt;includePath&gt;        add a new search path for include files\n   -tgtvarname &lt;str&gt;       set extension for tangent variables  (default %d)\n   -tgtfuncname &lt;str&gt;      set extension for tangent procedures (default %_d)\n   -tgtmodulename &lt;str&gt;    set extension for tangent modules and types (default %_diff)\n   -adjvarname &lt;str&gt;       set extension for adjoint variables  (default %b)\n   -adjfuncname &lt;str&gt;      set extension for adjoint procedures (default %_b)\n   -adjmodulename &lt;str&gt;    set extension for adjoint modules and types (default %_diff)\n   -modulename &lt;str&gt;       set extension for tangent&adjoint modules and types (default %_diff)\n   -inputlanguage &lt;lang&gt;   language of  input files (fortran, fortran90,\n                           fortran95, or C)\n   -outputlanguage &lt;lang&gt;  language of output files (fortran, fortran90,\n                           fortran95, or C)\n   -ext &lt;file&gt;             incorporate external library description &lt;file&gt;\n   -nolib                  don't load standard libraries descriptions\n   -i&lt;n&gt;                   count &lt;n&gt; bytes for an integer (default -i4)\n   -r&lt;n&gt;                   count &lt;n&gt; bytes for a real (default -r4)\n   -dr&lt;n&gt;                  count &lt;n&gt; bytes for a double real (default -dr8)\n   -p&lt;n&gt;                   count &lt;n&gt; bytes for a pointer (default -p8)\n   -fixinterface           don't use activity to filter user-given (in)dependent vars\n   -noinclude              inline include files\n   -debugTGT               insert instructions for debugging tangent mode\n   -debugADJ               insert instructions for debugging adjoint mode\n   -tracelevel &lt;n&gt;         set the level of detail of trace milestones\n   -msglevel &lt;n&gt;           set the level of detail of error messages\n   -msginfile              insert error messages in output files\n   -dump &lt;file&gt;            write a dump &lt;file&gt;\n   -html                   display results in a web browser\n   -nooptim &lt;str&gt;          turn off optimization &lt;str&gt; (in {activity, difftypes,\n                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n                           deadcontrol, recomputeintermediates,\n                           everyoptim}\n   -version                display Tapenade version information\n Report bugs to &lt;tapenade@inria.fr&gt;.\n\n Solution 2\n\nRunning\n$ cd session1/exercises/fg\n$ ls\nf.f90 g.f90\n$ tapenade -head \"f(y)/(x)\" f.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nDiff-liveness analysis turned off\n@@ Created ./f_d.f90 \n@@ Created ./f_d.msg\n$ cat f_d.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n!\n!  Differentiation of f in forward (tangent) mode:\n!   variations   of useful results: y\n!   with respect to varying inputs: x\n!   RW status of diff variables: x:in y:out\nSUBROUTINE F_D(x, xd, y, yd)\n  IMPLICIT NONE\n  REAL, DIMENSION(2), INTENT(IN) :: x\n  REAL, DIMENSION(2), INTENT(IN) :: xd\n  REAL, INTENT(OUT) :: y\n  REAL, INTENT(OUT) :: yd\n  yd = x(2)*xd(1) + x(1)*xd(2)\n  y = x(1)*x(2)\nEND SUBROUTINE F_D\nRunning\n$ tapenade -head \"g(z)/(y)\" g.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nDiff-liveness analysis turned off\n@@ Created ./g_d.f90 \n@@ Created ./g_d.msg\n$ cat g_d.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n!\n!  Differentiation of g in forward (tangent) mode:\n!   variations   of useful results: z\n!   with respect to varying inputs: y\n!   RW status of diff variables: y:in z:out\nSUBROUTINE G_D(y, yd, z, zd)\n  IMPLICIT NONE\n  REAL, INTENT(IN) :: y\n  REAL, INTENT(IN) :: yd\n  REAL, DIMENSION(2), INTENT(OUT) :: z\n  REAL, DIMENSION(2), INTENT(OUT) :: zd\n  INTRINSIC COS\n  INTRINSIC SIN\n  zd = (/COS(y)*yd, -(SIN(y)*yd)/)\n  z = (/SIN(y), COS(y)/)\nEND SUBROUTINE G_D"
  },
  {
    "objectID": "session1/notebook.html#verification-the-taylor-test",
    "href": "session1/notebook.html#verification-the-taylor-test",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Verification: the Taylor test",
    "text": "Verification: the Taylor test\nRecall the Taylor expansion of a differentiable scalar function \\(f:A\\rightarrow\\mathbb{R}\\), for \\(A\\subseteq\\mathbb{R}\\): \\[\n    f(x+\\epsilon)=f(x)+\\epsilon f'(x) + \\frac{1}{2!}\\epsilon^2f''(x) + \\dots + \\frac{1}{n!}\\epsilon^nf^{(n)}(x) + \\dots,\n\\] for some \\(\\epsilon\\in\\mathbb{R}\\), i.e., \\[\n    f(x+\\epsilon)=f(x)+\\epsilon f'(x) + \\mathcal{O}(\\epsilon^2).\n\\] This gives rise to the first-order forward difference approximation of the first derivative as follows: \\[\n    f'(x) \\approx \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}.\n\\]\nThis is a rather crude approximation to a derivative, but it can do the job, given an appropriate choice of \\(\\epsilon\\).\nThe idea of the Taylor test is to take smaller and smaller spacing values \\(\\epsilon\\) from a given input value and to check that the difference between the forward difference and the AD-generated result converges quadratically. That is, we verify that \\[\n    \\|f(x+\\epsilon)-f(x)-\\epsilon\\:\\texttt{f\\_d}(x)\\|=\\mathcal{O}(\\epsilon^2),\n\\] where \\(\\texttt{f\\_d}\\) is the AD-generated result."
  },
  {
    "objectID": "session1/notebook.html#f-g-example-taylor-test",
    "href": "session1/notebook.html#f-g-example-taylor-test",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "\\(f\\) & \\(g\\) example: Taylor test",
    "text": "\\(f\\) & \\(g\\) example: Taylor test\nWe return to the previous example and double-check that we are happy with the output of the AD tool.\nFor simplicity of demonstration, we translate f, g, f_d, and g_d into Python and conduct the Taylor test here.\n\ndef f(x):\n    \"\"\"Python translation of the `f` subroutine from `session1/exercises/fg/f.f90` using NumPy.\"\"\"\n    return x[0] * x[1]\n\ndef g(y):\n    \"\"\"Python translation of the `g` subroutine from `session1/exercises/fg/g.f90` using NumPy.\"\"\"\n    return np.array([np.sin(y), np.cos(y)])\n\n\ndef f_d(x, xd):\n    \"\"\"Python translation of the `f_d` subroutine generated by Tapenade in `session1/exercises/fg/f_d.f90` using NumPy.\"\"\"\n    yd = x[1] * xd[0] + x[0] * xd[1]\n    y = x[0] * x[1]\n    return y, yd\n\ndef g_d(y, yd):\n    \"\"\"Python translation of the `g_d` subroutine generated by Tapenade in `session1/exercises/fg/g_d.f90` using NumPy.\"\"\"\n    zd = np.array([np.cos(y) * yd, -np.sin(y) * yd])\n    z = np.array([np.sin(y), np.cos(y)])\n    return z, zd\n\n\n# Choose arbitrary inputs for the composition\nx = np.array([1.0, 1.0])\n\n# Choose seed vector\nxd = np.array([1.0, 0.0])\n\n# Compute the derivative using forward mode AD\ny, yd = f_d(x, xd)\n_, derivative_ad = g_d(y, yd)\n\n# Run the Taylor test over several spacing values\nspacings = [1.0, 0.1, 0.01, 0.001]\nerrors = []\nfor spacing in spacings:\n\n    # Compute the perturbation in the seed vector direction\n    epsilon = spacing * xd\n\n    # Compute the discrepancy\n    errors.append(np.linalg.norm(g(f(x + epsilon)) - g(f(x)) - spacing * derivative_ad))\n\n# Plot the solution, demonstrating that the expected quadratic convergence is achieved\nfig, axes = plt.subplots()\naxes.loglog(spacings, errors, \"--x\")\naxes.set_xlabel(r\"$\\epsilon$ spacing\")\naxes.set_ylabel(r\"$\\ell_2$ error\")\nannotation.slope_marker((1e-2, 1e-4), 2, ax=axes, invert=True)\naxes.grid()\n\nAbove, we computed the derivative of \\(h=f\\circ g\\) with respect to the first component, \\(x_1\\).\n\nOptional exercise\nPerform the Taylor test to double-check you are happy with the AD-generated derivative with respect to the second component, \\(x_2\\), too.\nHint: You only need to change one line."
  },
  {
    "objectID": "session1/notebook.html#example-ode-constrained-optimisation",
    "href": "session1/notebook.html#example-ode-constrained-optimisation",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Example: ODE-constrained optimisation",
    "text": "Example: ODE-constrained optimisation\nConsider the scalar ordinary differential equation (ODE) \\[\n    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=f(u),\\quad u(0)=u_0,\n\\] where \\(t\\in[0,T]\\) is the time variable, \\(T&gt;0\\) is the end time, and \\(u_0\\in\\mathbb{R}\\) is the initial condition. Given some \\(f:A\\rightarrow\\mathbb{R}\\) with \\(A\\subseteq\\mathbb{R}\\), we seek to solve the ODE for \\(u:[0,T]\\rightarrow\\mathbb{R}\\).\nFor simplicity, let’s consider the ODE \\[\n    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=u,\\quad u(0)=1,\n\\] i.e., \\(f(u)=u\\).\n\nOptional exercise\nConvince yourself that the analytical solution of the ODE is \\(u(t)=\\mathrm{e}^t\\).\nSolutions\n\nPlugging \\(u(t)=\\mathrm{e}^t\\) into the LHS gives \\(\\frac{\\mathrm{d}u}{\\mathrm{d}t}=\\mathrm{e}^t=u\\), which satisfies the ODE. Checking the initial condition, we have \\(u(0)=\\mathrm{e}^0=1\\), which also satisfies."
  },
  {
    "objectID": "session1/notebook.html#ode-example-forward-vs-backward-euler",
    "href": "session1/notebook.html#ode-example-forward-vs-backward-euler",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "ODE example: forward vs backward Euler",
    "text": "ODE example: forward vs backward Euler\nYou’re probably aware of the simplest example of an explicit timestepping method to approximate the solution of the ODE. This is the forward Euler (a.k.a. explicit Euler): \\[\n    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_{k-1}),\n\\] for \\(k\\in\\mathbb{N}\\) and some timestep \\(\\Delta t&gt;0\\).\nYou’re probably also aware that the simplest example of an implicit timestepping method to approximate the solution of the ODE is backward Euler (a.k.a. implicit Euler): \\[\n    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_k),\n\\] for \\(k\\in\\mathbb{N}\\) and some timestep \\(\\Delta t&gt;0\\).\nThese are actually special cases of a more general theta-method, \\[\n    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)f(u_{k-1})+\\theta f(u_k),\n\\] where \\(\\theta\\in[0,1]\\)."
  },
  {
    "objectID": "session1/notebook.html#ode-example-applying-forward-and-backward-euler",
    "href": "session1/notebook.html#ode-example-applying-forward-and-backward-euler",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "ODE example: applying forward and backward Euler",
    "text": "ODE example: applying forward and backward Euler\nUsing the method above, our problem reads \\[\n    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)u_{k-1}+\\theta u_k,\n\\] which can be rearranged to give \\[\n    u_{k}=\\frac{1+\\Delta t(1-\\theta)}{1-\\Delta t\\theta}u_{k-1}.\n\\]\nThe forward and backward Euler approaches have been implemented for the case \\(f(u)=u\\) in the session1/exercises/ode subdirectory.\n$ cd exercises/ode\n$ ls\nbackward_euler.f90  cost_mod.f90  forward_euler.f90  gradient_descent.f90  Makefile  theta_method_mod.f90\nwhere theta_methods.f90 is a fortran module containing several subroutines, including\n! Single iteration of a theta method for solving the ODE initial value problem\n!   du/dt = u, u(0) = 1\nsubroutine theta_step(u, u_, dt, theta)\n  implicit none\n  real, intent(out) :: u    ! Solution at current timestep\n  real, intent(in) :: u_    ! Solution at previous timestep\n  real, intent(in) :: dt    ! Timestep length\n  real, intent(in) :: theta ! Theta parameter\n\n  u = u_ * (1 + dt * (1 - theta)) / (1 - dt * theta)\nend subroutine theta_step\nHere forward_euler.f90 and backward_euler.f90 contain programs to solve the ODE, making use of code from the module above, but with values \\(\\theta=0\\) and \\(\\theta=1\\), respectively. In the former case, we have\n! Program for running the forward Euler method to solve the ODE initial value problem\n!   du/dt = u, u(0) = 1\nprogram forward_euler\n  use theta_method_mod, only: theta_method\n  implicit none\n\n  real, parameter :: theta = 0.0  ! Forward Euler corresponds to theta = 0.0\n  real :: u                       ! Solution variable\n  character(len=100), parameter :: filename = \"forward.csv\" ! Filename to save results to\n\n  call theta_method(theta, u, filename)\nend program forward_euler\n\nDetails on running the code\n\nCompile with\n$ make forward_euler\n$ make backward_euler\nand then run with\n$ ./forward_euler\n$ ./backward_euler\n$ ls\nbackward.csv    backward_euler.f90  forward.csv    forward_euler.f90     Makefile              theta_method_mod.mod\nbackward_euler  cost_mod.f90        forward_euler  gradient_descent.f90  theta_method_mod.f90  theta_method_mod.o\n\n\n\ndf_forward = pandas.read_csv(\"exercises/ode/forward.csv\")\ndf_backward = pandas.read_csv(\"exercises/ode/backward.csv\")\n\nfig, axes = plt.subplots()\n\ntimes = np.linspace(0, 1, 101)\n\naxes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\naxes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\naxes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\naxes.legend()\naxes.grid()"
  },
  {
    "objectID": "session1/notebook.html#ode-example-source-transformation",
    "href": "session1/notebook.html#ode-example-source-transformation",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "ODE example: source transformation",
    "text": "ODE example: source transformation\nAs we see from the plot above, the forward Euler method tends to underestimate the solution, whereas the backward Euler method tends to overestimate it. Let’s try to optimise the value of \\(\\theta\\) to best match the solution using a gradient-based optimisation method. To do that, we first need the gradient. AD enables us to do this automatically.\nThe optimisation problem we seek to solve is to minimise some error measure \\(J\\) for the approximation of \\(u\\) by varying \\(\\theta\\). That is, \\[\n    \\min_{\\theta\\in[0,1]}J(u;\\theta).\n\\] where the notation \\(J(u;\\theta)\\) refers to the implicit dependence of the solution approximation \\(u\\) on \\(\\theta\\).\n\nNote Forward and backward Euler are first-order accurate methods. By optimising the \\(\\theta\\) parameter, we can arrive at a second-order accurate method.\n\n\nExercise\nNavigate to session1/exercises/ode and apply forward mode AD to the theta_method subroutine in theta_method_mod.f90 by differentiating the module with Tapenade.\nSolutions\n\n$ tapenade -tgtmodulename %_d -head \"theta_method_mod.theta_method(theta)\\(u)\" theta_method_mod.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nDiff-liveness analysis turned off\n@@ Created ./theta_method_mod_d.f90 \n@@ Created ./theta_method_mod_d.msg\n\n$ cat theta_method_mod_d.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\n! Module containing subroutines for solving the ODE initial value problem\n!   du/dt = u, u(0) = 1\nMODULE THETA_METHOD_MOD_D\n  IMPLICIT NONE\n  PRIVATE \n  PUBLIC :: theta_method\n  PUBLIC :: theta_method_d\n\nCONTAINS\n! Apply the initial condition for the ODE initial value problem\n!   du/dt = u, u(0) = 1\n  SUBROUTINE INITIAL_CONDITION(u0)\n    IMPLICIT NONE\n! Initial condition value\n    REAL, INTENT(OUT) :: u0\n    u0 = 1.0\n  END SUBROUTINE INITIAL_CONDITION\n\n!  Differentiation of theta_step in forward (tangent) mode:\n!   variations   of useful results: u\n!   with respect to varying inputs: u_ theta\n! Single iteration of a theta method for solving the ODE initial value problem\n!   du/dt = u, u(0) = 1\n  SUBROUTINE THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n    IMPLICIT NONE\n! Solution at current timestep\n    REAL, INTENT(OUT) :: u\n    REAL, INTENT(OUT) :: ud\n! Solution at previous timestep\n    REAL, INTENT(IN) :: u_\n    REAL, INTENT(IN) :: u_d\n! Timestep length\n    REAL, INTENT(IN) :: dt\n! Theta parameter\n    REAL, INTENT(IN) :: theta\n    REAL, INTENT(IN) :: thetad\n    REAL :: temp\n    temp = u_/(-(dt*theta)+1)\n    ud = (dt*(1-theta)+1)*(u_d+temp*dt*thetad)/(1-dt*theta) - temp*dt*&\n&     thetad\n    u = (dt*(1-theta)+1)*temp\n  END SUBROUTINE THETA_STEP_D\n\n! Single iteration of a theta method for solving the ODE initial value problem\n!   du/dt = u, u(0) = 1\n  SUBROUTINE THETA_STEP(u, u_, dt, theta)\n    IMPLICIT NONE\n! Solution at current timestep\n    REAL, INTENT(OUT) :: u\n! Solution at previous timestep\n    REAL, INTENT(IN) :: u_\n! Timestep length\n    REAL, INTENT(IN) :: dt\n! Theta parameter\n    REAL, INTENT(IN) :: theta\n    u = u_*(1+dt*(1-theta))/(1-dt*theta)\n  END SUBROUTINE THETA_STEP\n\n!  Differentiation of theta_method in forward (tangent) mode:\n!   variations   of useful results: u\n!   with respect to varying inputs: theta\n!   RW status of diff variables: u:out theta:in\n! Solve the ODE initial value problem\n!   du/dt = u, u(0) = 1\n! using a theta timestepping method, writing the solution trajectory to file\n  SUBROUTINE THETA_METHOD_D(theta, thetad, u, ud, filename)\n    IMPLICIT NONE\n! Theta parameter\n    REAL, INTENT(IN) :: theta\n    REAL, INTENT(IN) :: thetad\n! Solution at the final time\n    REAL, INTENT(OUT) :: u\n    REAL, INTENT(OUT) :: ud\n! Output filename\n    CHARACTER(len=*), INTENT(IN) :: filename\n! Timestep length\n    REAL, PARAMETER :: dt=0.1\n! End time of the simulation\n    REAL, PARAMETER :: end_time=1.0\n! Dummy variable for time\n    REAL :: t\n! Lagged solution variable (previous timestep)\n    REAL :: u_\n    REAL :: u_d\n! Initialisation\n    t = 0.0\n    CALL INITIAL_CONDITION(u_)\n! Create a CSV file and write out the header and initial condition\n    OPEN(unit=10, file=filename) \n    WRITE(unit=10, fmt='(''t,u'')') \n    WRITE(unit=10, fmt=100) t, u_\n    ud = 0.0\n    u_d = 0.0\n! Timestepping loop\n    DO WHILE (t .LT. end_time - 1e-05)\n      CALL THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n      u_d = ud\n      u_ = u\n      t = t + dt\n      WRITE(unit=10, fmt=100) t, u\n    END DO\n! Remember to close the CSV file\n    CLOSE(unit=10) \n 100 FORMAT(f4.2,',',f4.2)\n  END SUBROUTINE THETA_METHOD_D\n\n! Solve the ODE initial value problem\n!   du/dt = u, u(0) = 1\n! using a theta timestepping method, writing the solution trajectory to file\n  SUBROUTINE THETA_METHOD(theta, u, filename)\n    IMPLICIT NONE\n! Theta parameter\n    REAL, INTENT(IN) :: theta\n! Solution at the final time\n    REAL, INTENT(OUT) :: u\n! Output filename\n    CHARACTER(len=*), INTENT(IN) :: filename\n! Timestep length\n    REAL, PARAMETER :: dt=0.1\n! End time of the simulation\n    REAL, PARAMETER :: end_time=1.0\n! Dummy variable for time\n    REAL :: t\n! Lagged solution variable (previous timestep)\n    REAL :: u_\n! Initialisation\n    t = 0.0\n    CALL INITIAL_CONDITION(u_)\n! Create a CSV file and write out the header and initial condition\n    OPEN(unit=10, file=filename) \n    WRITE(unit=10, fmt='(''t,u'')') \n    WRITE(unit=10, fmt=100) t, u_\n! Timestepping loop\n    DO WHILE (t .LT. end_time - 1e-05)\n      CALL THETA_STEP(u, u_, dt, theta)\n      u_ = u\n      t = t + dt\n      WRITE(unit=10, fmt=100) t, u\n    END DO\n! Remember to close the CSV file\n    CLOSE(unit=10) \n 100 FORMAT(f4.2,',',f4.2)\n  END SUBROUTINE THETA_METHOD\n\nEND MODULE THETA_METHOD_MOD_D"
  },
  {
    "objectID": "session1/notebook.html#ode-example-optimisation-with-gradient-descent",
    "href": "session1/notebook.html#ode-example-optimisation-with-gradient-descent",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "ODE example: optimisation with gradient descent",
    "text": "ODE example: optimisation with gradient descent\nLet’s solve this ODE problem with one of the simplest gradient-based optimisation approaches: gradient descent. This amounts to an initial guess \\(\\theta_0\\), followed by iterative updates \\[\n    \\theta_{k+1}=\\theta_k+\\alpha\\:p_k,\n\\] where \\(\\alpha&gt;0\\) is the step length and \\(p_k\\) is the descent direction. For gradient descent, we simply take \\[\n    p_k=-\\frac{\\mathrm{d}J_k}{\\mathrm{d}\\theta_k}.\n\\]\nSince we know the analytical solution for this problem, we may make an ‘artificial’ choice of cost function such as \\[\n    J(u;\\theta)=(u(1)-\\mathrm{e}^1)^2,\n\\] where here \\(\\mathrm{e}^1\\) is the analytical solution at the end time \\(t=1\\). This is implemented in cost_mod.f90 module as a subroutine\n! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\nsubroutine cost_function(u, J)\n  implicit none\n  real, intent(in) :: u           ! Numerical solution at the end time\n  real, intent(out) :: J          ! Cost function value\n  real, parameter :: e = exp(1.0) ! The exponential constant, e=2.71828...\n\n  J = (u - e) ** 2\nend subroutine cost_function\n\nExercise\nThe gradient descent algorithm is implemented for our ODE problem in session1/exercises/ode/gradient_descent.f90.\n\nHowever, there is another missing piece: you will also need to differentiate the cost function. Convince yourself you are satisfied with the output.\nRun the gradient descent optimisation algorithm and execute the following two cells to visualise the results.\nPlay around with the optimisation parameters defined in gradient_descent.f90. Can you achieve convergence in fewer iterations without changing the gtol convergence tolerance?\n\nSolution 1\n\n$ tapenade -tgtmodulename %_d cost_mod.f90 \n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\nCommand: Took subroutine cost_function as default differentiation root\nDiff-liveness analysis turned off\n@@ Created ./cost_mod_d.f90 \n@@ Created ./cost_mod_d.msg\ncat cost_mod_d.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\n! Module containing subroutines for cost functions\nMODULE COST_MOD_D\n  IMPLICIT NONE\n  PRIVATE \n  PUBLIC :: cost_function\n  PUBLIC :: cost_function_d\n\nCONTAINS\n!  Differentiation of cost_function in forward (tangent) mode:\n!   variations   of useful results: j\n!   with respect to varying inputs: u\n!   RW status of diff variables: j:out u:in\n! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\n  SUBROUTINE COST_FUNCTION_D(u, ud, j, jd)\n    IMPLICIT NONE\n! Numerical solution at the end time\n    REAL, INTENT(IN) :: u\n    REAL, INTENT(IN) :: ud\n! Cost function value\n    REAL, INTENT(OUT) :: j\n    REAL, INTENT(OUT) :: jd\n    INTRINSIC EXP\n! The exponential constant, e=2.71828...\n    REAL, PARAMETER :: e=EXP(1.0)\n    jd = 2*(u-e)*ud\n    j = (u-e)**2\n  END SUBROUTINE COST_FUNCTION_D\n\n! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\n  SUBROUTINE COST_FUNCTION(u, j)\n    IMPLICIT NONE\n! Numerical solution at the end time\n    REAL, INTENT(IN) :: u\n! Cost function value\n    REAL, INTENT(OUT) :: j\n    INTRINSIC EXP\n! The exponential constant, e=2.71828...\n    REAL, PARAMETER :: e=EXP(1.0)\n    j = (u-e)**2\n  END SUBROUTINE COST_FUNCTION\n\nEND MODULE COST_MOD_D\n\n Solution 2\n\nCompile gradient_descent.f90` and its dependencies with\n$ make gradient_descent\nYou can then run\n$ ./gradient_descent\nYou should get convergence in around 755 iterations.\n\n Solution 3\n\nIncreasing the step length slighly to \\(\\alpha=0.199\\) you should get convergence in around 378 iterations - about half. Interestingly, the choice \\(\\alpha=0.2\\) fails to converge. Bonus exercise: plot the progress for \\(\\alpha=0.2\\) in the cell below to see what went wrong.\n\n\n\ndf_opt = pandas.read_csv(\"exercises/ode/optimisation_progress.csv\")\n\ncosts = np.array(df_opt[\"J\"])\ncosts[0] = np.nan  # Remove the first entry because it's uninitialised garbage\ncontrols = np.array(df_opt[\"theta\"])\n\nfig, axes = plt.subplots(ncols=2, figsize=(12, 5))\naxes[0].loglog(df_opt[\"it\"], costs, \"--\", label=\"Cost function value\")\naxes[0].legend()\naxes[0].grid()\naxes[1].plot(df_opt[\"it\"], df_opt[\"theta\"], \"--\", label=\"Control value\")\naxes[1].legend()\naxes[1].grid()\n\n\ndf_optimised = pandas.read_csv(\"exercises/ode/optimised.csv\")\n\nfig, axes = plt.subplots()\naxes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\naxes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\naxes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\naxes.plot(df_optimised[\"t\"], df_optimised[\"u\"], \"-.^\", label=rf\"Optimised ($\\theta={controls[-1]:.4f}$)\")\naxes.legend()\naxes.grid()\n\nAs we might hope, the optimised value of \\(\\theta\\) gives a much better approximation."
  },
  {
    "objectID": "session1/notebook.html#calculating-the-full-jacobian",
    "href": "session1/notebook.html#calculating-the-full-jacobian",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Calculating the full Jacobian",
    "text": "Calculating the full Jacobian\n\nQuestion\nGiven a map \\(\\mathbf{f}\\), some input \\(\\mathbf{x}\\), and some seed \\(\\dot{\\mathbf{x}}\\), we have the Jacobian vector product \\[\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}.\\]\nHow can we use this to compute the full Jacobian matrix \\(\\nabla\\mathbf{f}(\\mathbf{x})\\)?\nSolution\n\n\\[\n\\nabla\\mathbf{f}(\\mathbf{x})\n=\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{I}_n\n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}\\mathbf{e}_1,\\mathbf{e}_2,\\dots,\\mathbf{e}_n\\end{bmatrix}\n=\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_n\\end{bmatrix}.\n\\]\nApply JVP to the \\(n\\) canonical unit vectors."
  },
  {
    "objectID": "session1/notebook.html#vector-mode",
    "href": "session1/notebook.html#vector-mode",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Vector mode",
    "text": "Vector mode\nThe expression \\(\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_n\\end{bmatrix}\\) above is a concatenation of \\(n\\) JVPs. Whilst it’s possible to apply these in a loop, most AD tools support a ‘vector’ mode, which allows this to be done in a single call. This is the vector-Jacobian product (VJP):\n\\[\\text{VJP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{X}}):=\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_k\\end{bmatrix},\\]\nfor a seed matrix \\(\\dot{\\mathbf{X}}:=\\begin{bmatrix}\\dot{\\mathbf{x}}_1,\\dot{\\mathbf{x}}_2,\\dots,\\dot{\\mathbf{x}}_k\\end{bmatrix}\\in\\mathbb{R}^{k\\times n}\\), \\(k\\in\\mathbb{N}\\)."
  },
  {
    "objectID": "session1/notebook.html#example-tridiagonal-matrix",
    "href": "session1/notebook.html#example-tridiagonal-matrix",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Example: tridiagonal matrix",
    "text": "Example: tridiagonal matrix\nConsider the classic central difference approximation to the second derivative (yes, more derivatives!): \\[\n    \\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\approx\\frac{u(x-h)-2u(x)+u(x+h)}{h^2},\n\\] for some uniform spacing \\(h&gt;0\\). For a uniform discretisation \\(\\{x_k\\}_{k=1}^n\\) of an interval, this can be written as \\[\n    \\left.\\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\right|_{x=x_k}\\approx\\frac{u_{k-1}-2u_k+u_{k+1}}{h^2},\n\\] where \\(u_k\\) is an approximation of \\(u(x_k)\\). This is implemented as a Fortran subroutine central_diff inside the Fortran module at session1/exercises/sparse/central_diff.f90.\nmodule central_diff_mod\n  implicit none\n\n  private\n  public :: central_diff\n\ncontains\n\n  ! Central difference approximation for the second derivative of a periodic 1D function\n  subroutine central_diff(u, approx, h, n)\n    implicit none\n\n    integer, intent(in) :: n                   ! Number of grid points\n    real, intent(in) :: h                      ! Uniform grid spacing\n    real, dimension(n), intent(in) :: u        ! Input vector\n    real, dimension(n), intent(out) :: approx  ! Central difference approximation\n    integer :: i                               ! Dummy index for looping\n\n    if (size(u, 1) /= n) then\n      print *, \"Invalid input array size\"\n      stop 1\n    end if\n    if (size(approx, 1) /= n) then\n      print *, \"Invalid output array size\"\n      stop 1\n    end if\n\n    do i = 1, n\n      if (i == 1) then\n        ! Periodic boundary on the left\n        approx(i) = (u(n) - 2 * u(i) + u(i+1)) / h ** 2\n      else if (i == n) then\n        ! Periodic boundary on the right\n        approx(i) = (u(i-1) - 2 * u(i) + u(1)) / h ** 2\n      else\n        ! Interior points\n        approx(i) = (u(i-1) - 2 * u(i) + u(i+1)) / h ** 2\n      end if\n    end do\n\n  end subroutine central_diff\n\nend module central_diff_mod\n\nExercise\nApply Tapenade in vector mode to compute the derivative of the output array approx with respect to the input array u.\nSolution\n\nRunning\n$ tapenade -tgtmodulename %_d -head \"central_diff(approx)/(u)\" -vector central_diff_mod.f90\nTapenade 3.16 (develop) - 25 Jun 2025 16:38 - Java 21.0.7 Linux\n@@ TAPENADE_HOME=/workspaces/differentiable-programming-summer-school-2025/tapenade_3.16/bin/..\n@@ Options:  multiDirectional\nCommand: Procedure central_diff understood as central_difference.central_diff\nDiff-liveness analysis turned off\n@@ Created ./central_diff_mod_dv.f90 \n@@ Created ./central_diff_mod_dv.msg\n$ cat central_diff_mod_dv.f90\ngives\n!        Generated by TAPENADE     (INRIA, Ecuador team)\n!  Tapenade 3.16 (develop) - 25 Jun 2025 16:38\n!\nMODULE CENTRAL_DIFF_MOD_DV\n  USE DIFFSIZES\n!  Hint: nbdirsmax should be the maximum number of differentiation directions\n  IMPLICIT NONE\n  PRIVATE \n  PUBLIC :: central_diff\n  PUBLIC :: central_diff_dv\n\nCONTAINS\n!  Differentiation of central_diff in forward (tangent) mode (with options multiDirectional):\n!   variations   of useful results: approx\n!   with respect to varying inputs: u\n!   RW status of diff variables: approx:out u:in\n! Central difference approximation for the second derivative of a periodic 1D function\n  SUBROUTINE CENTRAL_DIFF_DV(u, ud, approx, approxd, h, n, nbdirs)\n    USE DIFFSIZES\n!  Hint: nbdirsmax should be the maximum number of differentiation directions\n    IMPLICIT NONE\n! Number of grid points\n    INTEGER, INTENT(IN) :: n\n! Uniform grid spacing\n    REAL, INTENT(IN) :: h\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: u\n    REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: ud\n! Central difference approximation\n    REAL, DIMENSION(n), INTENT(OUT) :: approx\n    REAL, DIMENSION(nbdirsmax, n), INTENT(OUT) :: approxd\n! Dummy index for looping\n    INTEGER :: i\n    INTRINSIC SIZE\n    INTEGER :: nd\n    INTEGER :: nbdirs\n    IF (SIZE(u, 1) .NE. n) THEN\n      PRINT*, 'Invalid input array size'\n      STOP\n    ELSE IF (SIZE(approx, 1) .NE. n) THEN\n      PRINT*, 'Invalid output array size'\n      STOP\n    ELSE\n      approxd = 0.0\n      DO i=1,n\n        IF (i .EQ. 1) THEN\n! Periodic boundary on the left\n          DO nd=1,nbdirs\n            approxd(nd, i) = (ud(nd, n)-2*ud(nd, i)+ud(nd, i+1))/h**2\n          END DO\n          approx(i) = (u(n)-2*u(i)+u(i+1))/h**2\n        ELSE IF (i .EQ. n) THEN\n! Periodic boundary on the right\n          DO nd=1,nbdirs\n            approxd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, 1))/h**2\n          END DO\n          approx(i) = (u(i-1)-2*u(i)+u(1))/h**2\n        ELSE\n! Interior points\n          DO nd=1,nbdirs\n            approxd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, i+1))/h**2\n          END DO\n          approx(i) = (u(i-1)-2*u(i)+u(i+1))/h**2\n        END IF\n      END DO\n    END IF\n  END SUBROUTINE CENTRAL_DIFF_DV\n\n! Central difference approximation for the second derivative of a periodic 1D function\n  SUBROUTINE CENTRAL_DIFF(u, approx, h, n)\n    IMPLICIT NONE\n! Number of grid points\n    INTEGER, INTENT(IN) :: n\n! Uniform grid spacing\n    REAL, INTENT(IN) :: h\n! Input vector\n    REAL, DIMENSION(n), INTENT(IN) :: u\n! Central difference approximation\n    REAL, DIMENSION(n), INTENT(OUT) :: approx\n! Dummy index for looping\n    INTEGER :: i\n    INTRINSIC SIZE\n    IF (SIZE(u, 1) .NE. n) THEN\n      PRINT*, 'Invalid input array size'\n      STOP\n    ELSE IF (SIZE(approx, 1) .NE. n) THEN\n      PRINT*, 'Invalid output array size'\n      STOP\n    ELSE\n      DO i=1,n\n        IF (i .EQ. 1) THEN\n! Periodic boundary on the left\n          approx(i) = (u(n)-2*u(i)+u(i+1))/h**2\n        ELSE IF (i .EQ. n) THEN\n! Periodic boundary on the right\n          approx(i) = (u(i-1)-2*u(i)+u(1))/h**2\n        ELSE\n! Interior points\n          approx(i) = (u(i-1)-2*u(i)+u(i+1))/h**2\n        END IF\n      END DO\n    END IF\n  END SUBROUTINE CENTRAL_DIFF\n\nEND MODULE CENTRAL_DIFF_MOD_DV\n\n\n\nNote: The differentiated module central_diff_mod_dv depends on a module diffsizes which specifies the number of differentiation directions. The examples that will follow will each define this module (diffsizes_dense.f90, diffsizes_compressed.f90)."
  },
  {
    "objectID": "session1/notebook.html#tridiagonal-example-compute-dense-jacobian",
    "href": "session1/notebook.html#tridiagonal-example-compute-dense-jacobian",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Tridiagonal example: compute dense Jacobian",
    "text": "Tridiagonal example: compute dense Jacobian\nIn session1/exercises/sparse/dense_jacobian.f90 we provide a program for computing the Jacobian of the centred difference function over the uniform partition of the interval.\n! Naive program for computing a tridiagonal Jacobian\nprogram dense_jacobian\n  use diffsizes, only: m =&gt; nbdirsmax  ! Number of seed vectors, from diffsizes_dense.f90\n  use central_diff_mod_dv, only: central_diff_dv ! Tapenade Forward mode derivative\n\n  implicit none\n\n  logical, parameter :: output = .true. ! Flag for whether to write output to file\n  integer, parameter :: n = 10          ! Number of grid points\n  real, parameter :: h = 1.0            ! Uniform grid spacing\n  real, dimension(n) :: u               ! Input vector\n  real, dimension(n) :: approx          ! Central difference approximation\n  real, dimension(m,n) :: seed          ! Seed matrix for the VJP\n  real, dimension(m,n) :: jacobian      ! Jacobian for the central difference calculation\n  integer :: i                          ! Dummy index for looping\n\n  if (m /= n) then\n    print *, \"Error: number of grid points must match number of seed vectors.\"\n    stop 1\n  end if\n\n  ! Specify some arbitrary input\n  u(:) = 1.0\n\n  ! Set up the seed matrix as the nxn identity\n  seed(:,:) = 0.0\n  do i = 1, n\n    seed(i,i) = 1.0\n  end do\n\n  ! Propagate the seed matrix through the VJP\n  call central_diff_dv(u, seed, approx, jacobian, h, n, n)\n\n  ! Write out the result to file\n  if (output) then\n    open(unit=10, file=\"dense_jacobian.dat\")\n    do i = 1, n\n      write(unit=10, fmt=100) jacobian(i,:)\n    end do\n    100 format(10(f4.1,\",\"))\n    close(unit=10)\n  end if\n\nend program dense_jacobian\n\nDetails on running the code\n\nCompile with\n$ make dense_jacobian\nand then run with\n$ ./dense_jacobian\n\n\nCompiling and running produces a file dense_jacobian.dat, which we can read and view in the notebook. Note that the computation here required \\(n\\) applications of forward mode AD, where \\(n\\) is the number of gridpoints.\n\nprint_matrix(\"exercises/sparse/dense_jacobian.dat\")"
  },
  {
    "objectID": "session1/notebook.html#sparse-ad",
    "href": "session1/notebook.html#sparse-ad",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Sparse AD",
    "text": "Sparse AD\nWe can compute the full Jacobian with \\[\\nabla\\mathbf{f}(\\mathbf{x})=\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{I}_n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}\\mathbf{e}_1,\\mathbf{e}_2,\\dots,\\mathbf{e}_n\\end{bmatrix}.\\]\n\nBut what about when \\(n\\) gets very large?\nAnd what about when the Jacobian is sparse?"
  },
  {
    "objectID": "session1/notebook.html#diagonal-jacobian-example",
    "href": "session1/notebook.html#diagonal-jacobian-example",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Diagonal Jacobian example",
    "text": "Diagonal Jacobian example\nSuppose \\(\\nabla\\mathbf{f}(\\mathbf{x})\\) is diagonal, say\n\\[\\nabla\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}.\\]\nThen, for a seed vector \\(\\dot{\\mathbf{x}}=\\begin{bmatrix}\\dot{x}_1 & \\dot{x}_2 & \\dots & \\dot{x}_n\\end{bmatrix}^T\\), we have\n\\[\n\\nabla\\mathbf{f}(\\mathbf{x})\\:\\dot{\\mathbf{x}}\n=\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}\n\\begin{bmatrix}\\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\\end{bmatrix}\n=\\begin{bmatrix}J_1\\dot{x}_1 \\\\ J_2\\dot{x}_2 \\\\ \\vdots \\\\ J_n\\dot{x}_n\\end{bmatrix}.\n\\]\nMaking the clever choice of seed vector \\(\\dot{\\mathbf{x}}:=\\mathbf{e}=\\begin{bmatrix}1 & 1 & \\dots & 1\\end{bmatrix}^T\\), we can apply a single JVP and then back out the full Jacobian by putting each entry on the diagonal: \\[\n    \\mathrm{diag}(\\nabla\\mathbf{f}(\\mathbf{x})\\:\\mathbf{e})\n    =\\mathrm{diag}\\left(\\begin{bmatrix}J_1 \\\\ J_2 \\\\ \\vdots \\\\ J_n \\end{bmatrix}\\right)\n    =\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}\n    =\\nabla\\mathbf{f}(\\mathbf{x}).\n\\]"
  },
  {
    "objectID": "session1/notebook.html#sparse-ad-what-colour-is-your-jacobian",
    "href": "session1/notebook.html#sparse-ad-what-colour-is-your-jacobian",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Sparse AD: what colour is your Jacobian?",
    "text": "Sparse AD: what colour is your Jacobian?\nThe example above is the simplest case of what is known as colouring. The idea is to group columns of the Jacobian such that the columns in each group are linearly independent. This gives rise to compression approaches for sparse AD.\n\n Figure 5: Jacobian colouring diagram taken from (Gebremedhin et al., 2005)."
  },
  {
    "objectID": "session1/notebook.html#tridiagonal-example-compute-compressed-jacobian",
    "href": "session1/notebook.html#tridiagonal-example-compute-compressed-jacobian",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Tridiagonal example: compute compressed Jacobian",
    "text": "Tridiagonal example: compute compressed Jacobian\nIn session1/exercises/sparse/compressed_jacobian.f90 we provide a program for computing the same Jacobian of the centred difference function but this time using a clever choice of seed matrix: \\[\n    \\dot{\\mathbf{X}}:=\\begin{bmatrix}\n        1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n        0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n        0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\n    \\end{bmatrix}.\n\\]\n! Program for computing a tridiagonal Jacobian using compression\nprogram compressed_jacobian\n  use diffsizes, only: m =&gt; nbdirsmax  ! Number of seed vectors, from diffsizes_compressed.f90\n  use central_diff_mod_dv, only: central_diff_dv ! Tapenade Forward mode derivative\n\n  implicit none\n\n  logical, parameter :: output = .true. ! Flag for whether to write output to file\n  integer, parameter :: n = 10          ! Number of grid points\n  real, parameter :: h = 1.0            ! Uniform grid spacing\n  real, dimension(n) :: u               ! Input vector\n  real, dimension(n) :: approx          ! Central difference approximation\n  real, dimension(m,n) :: seed          ! Seed matrix for the VJP\n  real, dimension(m,n) :: compressed    ! Compressed Jacobian for the central difference calculation\n  real, dimension(n,n) :: jacobian      ! Jacobian for the central difference calculation\n  integer :: i, j                       ! Dummy indices for looping\n\n  ! Specify some arbitrary input\n  u(:) = 1.0\n\n  ! Set up the seed matrix as a 3xn array\n  seed(:,:) = 0.0\n  do i = 1, m\n    seed(i,i::m) = 1.0\n  end do\n\n  ! Apply the VJP\n  call central_diff_dv(u, seed, approx, compressed, h, n, m)\n\n  ! Write out the compressed result to file\n  if (output) then\n    open(unit=10, file=\"compressed_jacobian.dat\")\n    do i = 1, m\n      write(unit=10, fmt=100) compressed(i,:)\n    end do\n    100 format(10(f4.1,\",\"))\n    close(unit=10)\n  end if\n\n  ! Decompress rows and insert them into the Jacobian\n  do i = 1, m\n    j = i\n    jacobian(i::m,:) = 0.0\n    do while (j &lt;= n)\n      if (j == 1) then\n        jacobian(j,n) = compressed(i,n)\n      else\n        jacobian(j,j-1) = compressed(i,j-1)\n      end if\n      jacobian(j,j) = compressed(i,j)\n      if (j == n) then\n        jacobian(j,1) = compressed(i,1)\n      else\n        jacobian(j,j+1) = compressed(i,j+1)\n      end if\n      j = j + m\n    end do\n  end do\n\n  ! Write out the result to file\n  if (output) then\n    open(unit=11, file=\"decompressed_jacobian.dat\")\n    do i = 1, n\n      write(unit=11, fmt=100) jacobian(i,:)\n    end do\n    close(unit=11)\n  end if\n\nend program compressed_jacobian\n  \n\nDetails on running the code\n\nCompile with\n$ make compressed_jacobian\nand then run with\n$ ./compressed_jacobian\n\n\nCompiling and running produces files compressed_jacobian.dat and decompressed_jacobian.dat, which we can again read and view in the notebook. This time we only need to compute three JVPs, rather than \\(n\\)!\n\nExercise\nExperiment with increasing the number of gridpoints to see the impact on the overall cost of the dense approach versus the compressed approach.\nHint: Using the time command line tool should give some very basic timing information.\nSolution\n\nMake the changes\ndiff --git a/session1/exercises/sparse/compressed_jacobian.f90 b/session1/exercises/sparse/compressed_jacobian.f90\nindex 1c49ebd..86c05db 100644\n--- a/session1/exercises/sparse/compressed_jacobian.f90\n+++ b/session1/exercises/sparse/compressed_jacobian.f90\n@@ -5,8 +5,8 @@ program compressed_jacobian\n \n   implicit none\n \n-  logical, parameter :: output = .true. ! Flag for whether to write output to file\n-  integer, parameter :: n = 10          ! Number of grid points\n+  logical, parameter :: output = .false. ! Flag for whether to write output to file\n+  integer, parameter :: n = 10000          ! Number of grid points\n   real, parameter :: h = 1.0            ! Uniform grid spacing\n   real, dimension(n) :: u               ! Input vector\n   real, dimension(n) :: approx          ! Central difference approximation\ndiff --git a/session1/exercises/sparse/dense_jacobian.f90 b/session1/exercises/sparse/dense_jacobian.f90\nindex a61111d..e04a9aa 100644\n--- a/session1/exercises/sparse/dense_jacobian.f90\n+++ b/session1/exercises/sparse/dense_jacobian.f90\n@@ -5,8 +5,8 @@ program dense_jacobian\n \n   implicit none\n \n-  logical, parameter :: output = .true. ! Flag for whether to write output to file\n-  integer, parameter :: n = 10          ! Number of grid points\n+  logical, parameter :: output = .false. ! Flag for whether to write output to file\n+  integer, parameter :: n = 10000          ! Number of grid points\n   real, parameter :: h = 1.0            ! Uniform grid spacing\n   real, dimension(n) :: u               ! Input vector\n   real, dimension(n) :: approx          ! Central difference approximation\ndiff --git a/session1/exercises/sparse/diffsizes_dense.f90 b/session1/exercises/sparse/diffsizes_dense.f90\nindex 0fef76e..ffc198a 100644\n--- a/session1/exercises/sparse/diffsizes_dense.f90\n+++ b/session1/exercises/sparse/diffsizes_dense.f90\n@@ -5,5 +5,5 @@ module diffsizes\n   private\n   public :: nbdirsmax\n \n-  integer, parameter :: nbdirsmax = 10\n+  integer, parameter :: nbdirsmax = 10000\n end module diffsizes\nrecompile and then run\n$ time ./compressed_jacobian && time ./dense_jacobian\nYou should get output something like\nreal    0m0.309s\nuser    0m0.140s\nsys     0m0.168s\n\nreal    0m0.883s\nuser    0m0.505s\nsys     0m0.377s\nindicating that the compressed approach is about twice as fast at this resolution. We could likely get a much better speedup by optimising the decompression strategy.\n\n\n\nWarning! Make sure you turn off the output flag and recompile before conducting any timing experiments.\n\n\nprint_matrix(\"exercises/sparse/compressed_jacobian.dat\")\n\n\nprint_matrix(\"exercises/sparse/decompressed_jacobian.dat\")"
  },
  {
    "objectID": "session1/notebook.html#sparse-ad-in-practice",
    "href": "session1/notebook.html#sparse-ad-in-practice",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Sparse AD: in practice",
    "text": "Sparse AD: in practice\nIt’s worth noting that we used knowledge about the tridiagonal structure of the Jacobian to choose the seed vectors… so that we could compute the Jacobian. For Jacobians in the wild (e.g., those related to networks) we don’t necessarily have such intuition. In practice, we need to perform an initial step to establish the sparsity pattern before we can do the colouring. This functionality is available in Tapenade, but currently only on a branch."
  },
  {
    "objectID": "session1/notebook.html#sparse-ad-overall-performance-comparison",
    "href": "session1/notebook.html#sparse-ad-overall-performance-comparison",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Sparse AD: overall performance comparison",
    "text": "Sparse AD: overall performance comparison\nWe have considered three approaches for sparse AD:\n\n‘Dense approach’, which computes a JVP for each canonical unit vector.\n‘Compressed approach’, which computes JVPs for sums over sets of linearly independent columns.\n‘Matrix-free approach’, which applies a single JVP. (Only useful if the problem involves JVPs.)\n\nIn the following, we walk through a performance comparison of these different approaches applied to the solution of the nonlinear Gray-Scott equation \\[\n    \\frac{\\partial u}{\\partial t}=D_1\\Delta u-uv^2+\\gamma(1-u),\n    \\quad\\frac{\\partial v}{\\partial t}=D_2\\Delta v-uv^2=(\\gamma+\\kappa)v,\n    \\quad u,v:[0,2.5]^2\\rightarrow\\mathbb{R}\n\\] (subject to some initial conditions) using a finite difference approach with Crank-Nicolson timestepping (\\(\\theta=0.5\\)). This discretisation corresponds to a 5-point stencil and it turns out the Jacobian can be coloured with five colours. As such, the compressed approach involves five JVPs per Jacobian computation.\n\n Figure 6: Performance comparison for different Jacobian computation approaches for solving the Gray-Scott equation, taken from (Wallwork et al, 2019). PETSc was used for numerical solvers and ADOL-C for AD. The \\(N\\\\times N\\) notation in the subcaptions refers to the grid dimension.\n\n\n\nHere ‘analytic’ refers to the hand-derived Jacobian, which we can’t realistically expect to beat."
  },
  {
    "objectID": "session1/notebook.html#sparse-ad-convergence-comparison",
    "href": "session1/notebook.html#sparse-ad-convergence-comparison",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Sparse AD: convergence comparison",
    "text": "Sparse AD: convergence comparison\nMatrix-free works well for small dimensions. However, it slows down at higher resolution because Jacobian assembly is done at each iteration of the nonlinear solver, whereas the JVP is calculated at each iteration of the linear solver. It’s possible that a different choice of linear solver or preconditioner would reduce the cost of the matrix-free approach.\n\n Figure 7: Linear solver iterations as a function of grid resolution and number of processors for the Gray-Scott problem. (Numbers of nonlinear solver iterations are much lower.) Taken from (Wallwork et al, 2019)."
  },
  {
    "objectID": "session1/notebook.html#summary-and-outlook",
    "href": "session1/notebook.html#summary-and-outlook",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "Summary and outlook",
    "text": "Summary and outlook\nIn today’s session we:\n\nGot a brief history of automatic differentiation.\nLearnt about forward mode and the source transformation approach.\nTried out the Tapenade source transformation AD tool applied to some test problems.\nVerified the code generated by Tapenade both manually and using the Taylor test.\nLearnt about compressed and matrix-free approaches for sparse problems.\n\nIn tomorrow’s session (at the same time) we will:\n\nLearn about reverse mode and the operator overloading approach, comparing them with forward mode and source transformation, respectively.\nVerify the consistency of code generated by Tapenade under forward and reverse mode using the dot product test.\nCalculate higher order derivatives using Tapenade.\nTry out the Pyadjoint operator overloading AD tool underpinnning the Firedrake finite element library and see showcases of more advanced AD usage.\nLearn about checkpointing and using AD to compute higher order derivatives."
  },
  {
    "objectID": "session1/notebook.html#references",
    "href": "session1/notebook.html#references",
    "title": "Session 1: Forward mode differentiation and source transformation",
    "section": "References",
    "text": "References\n\nR. E. Wengert. A simple automatic derivative evaluation program (1964). Communications of the ACM, 7(8):463–464, doi.org:10.1145/355586.364791.\nA. Griewank. Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods & Software, 1:35–54, 1992, doi:10.1080/10556789208805505.\nA. H. Gebremedhin, et al. What color is your Jacobian? Graph coloring for computing derivatives (2005). SIAM review, 47(4), pp.629-705, doi:10.1137/S0036144504444711.\nJ. G. Wallwork, et al. Computing derivatives for petsc adjoint solvers using algorithmic differentiation (2019), arXiv preprint doi:10.48550/arXiv.1909.02836."
  }
]