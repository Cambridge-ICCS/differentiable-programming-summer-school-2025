{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaadad0-da7a-4895-b1e9-e38e30a99eae",
   "metadata": {},
   "source": [
    "# Session 1: Forward mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454c496-2b6f-4cbe-b87e-d3dbb78a8e19",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Terminology\n",
    "\n",
    "This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "Similarly, for $m\\in\\mathbb{N}$ dimensional, differentiable, vector-valued function $\\mathbf{f}:A\\rightarrow\\mathbb{R}^m$ with scalar input, we have derivative notations $\\mathbf{f}'(x)$ and $\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}$.\n",
    "\n",
    "For a differentiable function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is written as $f=f(x,y)$ then we have the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ with respect to first and second components, respectively. We use\n",
    "$$\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)$$\n",
    "to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\n",
    "\n",
    "When it comes to derivatives in code, we use the `_d` notation, which is standard in the AD literature. Its meaning will be described in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Origins of AD in 1950s.\n",
    "* However, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\n",
    "* Forward mode (the subject of this session) was discovered by Wengert in 1964.\n",
    "* Further developed by Griewank in the late 1980s.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: images?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "The idea of AD is to **treat a model as a sequence of elementary instructions** (e.g., addition, multiplication, exponentiation). Here a *model* could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the *chain rule*.\n",
    "\n",
    "#### Recap on A-level maths: the Chain Rule\n",
    "\n",
    "Consider two composable, differentiable (mathematical) functions, $f$ and $g$, with composition $h=f\\circ g$. By definition, this means\n",
    "$$h(x)=(f\\circ g)(x)=g(f(x)).$$\n",
    "\n",
    "Then the *chain rule* states that the derivative of $h$ may be computed in terms of the derivatives of $f$ and $g$ using the formula\n",
    "$$h'(x)=(f\\circ g)'(x)=(f\\circ g')(x)\\,f'(x)=g'(f(x))\\,f'(x).$$\n",
    "\n",
    "Equivalently, in Leibniz notation:\n",
    "$$\\frac{\\mathrm{d}h}{\\mathrm{d}x}=\\frac{\\mathrm{d}g}{\\mathrm{d}f}\\frac{\\mathrm{d}f}{\\mathrm{d}x}.$$\n",
    "\n",
    "For variables with multiple arguments, the result is equivalent for each partial derivative, e.g.,\n",
    "$$\\frac{\\partial h}{\\partial x}=\\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example\n",
    "\n",
    "Consider two functions acting on real numbers:\n",
    "$$f(x,y)=xy$$\n",
    "and\n",
    "$$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "Here $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ takes two inputs and returns a single output, while $g:\\mathbb{R}\\rightarrow\\mathbb{R}^2$ takes a single input and returns two outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Convince yourself that it is well defined for these functions may be composed in either order. (Although they won't necessarily give the same value!)\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "The image of $f$ is the set of all real numbers, so its image is the same as the domain of $g$ (i.e., $\\text{im}(f)=\\mathbb{R}=\\text{dom}(g)$).\n",
    "\n",
    "The image of $g$ is $[-1,1]^2=[-1,1]\\times[-1,1]$ because $\\sin$ and $\\cos$ give values between -1 and 1. Since this is a subset of $\\mathbb{R}^2$, the image of $g$ is a subset of the domain of $f$ (i.e., $\\text{im}(g)\\subset\\text{dom}(f)$).\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example (continued)\n",
    "\n",
    "Consider the composition $h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$. Then we have\n",
    "$$h(x,y)=(f\\circ g)(x,y)=g(f(x,y))=g(xy)=(\\sin(xy),\\cos(xy)).$$\n",
    "\n",
    "For the derivative of each component,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}=y,\n",
    "\\quad\\frac{\\partial f}{\\partial y}=x,\n",
    "\\quad\\frac{\\partial g}{\\partial z}=(\\cos(z),-\\sin(z)).\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs, i.e.,\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x},\n",
    "\\quad\\frac{\\partial h_1}{\\partial y},\n",
    "\\quad\\frac{\\partial h_2}{\\partial x},\n",
    "\\quad\\frac{\\partial h_2}{\\partial y},\n",
    "$$\n",
    "where $h(x,y)=(h_1(x,y),h_2(x,y))$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x}=\\cos(z),y=y\\cos(xy),\n",
    "\\quad\\frac{\\partial h_1}{\\partial y}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial y}=\\cos(z)x=x\\cos(xy),\n",
    "$$\n",
    "where $z=f(x,y)$ and\n",
    "$$\n",
    "\\quad\\frac{\\partial h_2}{\\partial x}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x}=\\sin(z)y=y\\sin(xy),\n",
    "\\quad\\frac{\\partial h_2}{\\partial y}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial y}=\\sin(z)x=x\\sin(xy).\n",
    "$$  \n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5886a-e982-4995-872c-ffbc8375208e",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Directed Acyclic Graph\n",
    "\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  Recall\n",
    "  $$f(x,y)=xy$$\n",
    "  and\n",
    "  $$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "\n",
    "  We can visualise the composition in terms of a DAG. Here $\\dot{x}$ and $\\dot{y}$ denote *seeds* for the derivatives of the input variables $x$ and $y$, respectively. Think of these as inputs from outside of the part of the program that we are differentiating.\n",
    "</div>\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "  <img src=\"images/forward_AD_as_DAG.png\" style=\"width: 50%;\">\n",
    "  <div><strong>Figure 1:</strong> Directed Acyclic Graph (DAG) for the mathematical functions in Example 1.</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODOs: 1. switch notation to have $x$ and $y$ as inputs and then $h_1$ and $h_2$ as outputs. 2. why aren't the columns working?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    "\n",
    "Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    "\n",
    "Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Approach 1: Source transformation\n",
    "\n",
    "High level idea: Given some (code) function `f(x)` and a seed vector `x_d`, generate the code for the function `f_d(x, x_d)` for its (directional) derivative.\n",
    "\n",
    "#### Evaluation of the source transformation approach:\n",
    "\n",
    "* Clearly demonstrates the differentiation process.\n",
    "* Static analysis, done ahead of time.\n",
    "* Does not require a 'tape' (see later).\n",
    "* Often the difficult part is hooking the differentiated code into the wider model/build system.\n",
    "* Limited number of tools.\n",
    "\n",
    "#### Source transformation tools:\n",
    "\n",
    "* [Tapenade](https://tapenade.gitlabpages.inria.fr/userdoc/build/html/index.html) (C, Fortran, Julia*)\n",
    "* [OpenAD](https://www.mcs.anl.gov/OpenAD) (Fortran)\n",
    "* [TAF](http://fastopt.com/products/taf) (Fortran) [commerical]\n",
    "* [PSyAD](https://psyclone-adjoint.readthedocs.io/en/stable)* (domain-specific)\n",
    "\n",
    "*\\*Work in progress*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: source transformation\n",
    "\n",
    "Below we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at `session1/exercises/fg/f.f90` and `session1/exercises/fg/g.f90`, respectively.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Either [install Tapenade](https://tapenade.gitlabpages.inria.fr/tapenade/distrib/README.html)* or visit the [Tapenade web interface](http://tapenade.inria.fr:8080/tapenade/index.jsp).\n",
    "2. Run `tapenade -h` to see all the options.\n",
    "3. Apply Tapenade to each of these subroutines using its default setting, which will apply forward mode to compute the JVP for some seed vector.\n",
    "4. Inspect the output files `f_d.f90` and `g_d.f90` and check they are as you expect.\n",
    "5. Inspect the message files `f_d.msg` and `g_d.msg`.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade f.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./f_d.f90 \n",
    "@@ Created ./f_d.msg\n",
    "$ cat f_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in forward (tangent) mode:\n",
    "!   variations   of useful results: z\n",
    "!   with respect to varying inputs: x y\n",
    "!   RW status of diff variables: x:in y:in z:out\n",
    "SUBROUTINE F_D(x, xd, y, yd, z, zd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL, INTENT(IN) :: xd, yd\n",
    "  REAL, INTENT(OUT) :: z\n",
    "  REAL, INTENT(OUT) :: zd\n",
    "  zd = y*xd + x*yd\n",
    "  z = x*y\n",
    "END SUBROUTINE F_D\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade g.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./g_d.f90 \n",
    "@@ Created ./g_d.msg\n",
    "$ cat g_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in forward (tangent) mode:\n",
    "!   variations   of useful results: v\n",
    "!   with respect to varying inputs: z\n",
    "!   RW status of diff variables: v:out z:in\n",
    "SUBROUTINE G_D(z, zd, v, vd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL, INTENT(IN) :: zd\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: v\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: vd\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  vd = (/COS(z)*zd, -(SIN(z)*zd)/)\n",
    "  v = (/SIN(z), COS(z)/)\n",
    "END SUBROUTINE G_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0a0d-61eb-419c-baa5-f8eefb52954d",
   "metadata": {},
   "source": [
    "## Example: ODE-constrained optimisation\n",
    "\n",
    "Consider the scalar ordinary differential equation (ODE)\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=f(u),\\quad u(0)=u_0,\n",
    "$$\n",
    "where $t\\in[0,T]$ is the time variable, $T>0$ is the end time, and $u_0\\in\\mathbb{R}$ is the initial condition. Given some $f:A\\rightarrow\\mathbb{R}$ with $A\\subseteq\\mathbb{R}$, we seek to solve the ODE for $u:[0,T]\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For simplicity, let's consider the ODE\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=u,\\quad u(0)=1,\n",
    "$$\n",
    "i.e., $f(u)=u$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "Convince yourself that the analytical solution of the ODE is $u=\\mathrm{e}^t$.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Plugging $u(t)=\\mathrm{e}^t$ into the LHS gives $\\frac{\\mathrm{d}u}{\\mathrm{d}t}=\\mathrm{e}^t=u$, which satisfies the ODE. Checking the initial condition, we have $u(0)=\\mathrm{e}^0=1$, which also satisfies.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468e1e9-1ea8-4e3b-b13d-162975a79088",
   "metadata": {},
   "source": [
    "## ODE example: forward vs backward Euler\n",
    "\n",
    "You've probably aware of the simplest example of an explicit timestepping method to approximate the solution of the ODE is *forward Euler* (a.k.a. explicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_{k-1}),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "You're probably also aware of the simplest example of an implicit timestepping method to approximate the solution of the ODE is *backward Euler* (a.k.a. implicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_k),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "These are special cases of the first-order *theta-method*\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)f(u_{k-1})+\\theta f(u_k),\n",
    "$$\n",
    "where $\\theta\\in[0,1]$.\n",
    "\n",
    "The forward and backward Euler approaches have been implemented for the case $f(u)=u$ in the `exercises/ode` subdirectory.\n",
    "```sh\n",
    "$ cd exercises/ode\n",
    "$ ls\n",
    "backward_euler.f90 forward_euler.f90 theta.f90\n",
    "```\n",
    "Here `forward_euler.f90` and `backward_euler.f90` contain programs to solve the ODE, making use of common code from `theta.f90`, but with values $\\theta=0$ and $\\theta=1$, respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran forward_euler.f90 -o forward\n",
    "$ gfortran backward_euler.f90 -o backward\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./forward\n",
    "$ ./backward\n",
    "$ ls\n",
    "backward      backward_euler.f90  forward.csv        theta.f90\n",
    "backward.csv  forward             forward_euler.f90\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797eba8c-286c-41c3-80c4-38d862ccda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "df_forward = pandas.read_csv(\"exercises/ode/forward.csv\")\n",
    "df_backward = pandas.read_csv(\"exercises/ode/backward.csv\")\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "times = np.linspace(0, 1, 101)\n",
    "\n",
    "axes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\n",
    "axes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\n",
    "axes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\n",
    "axes.legend()\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bda285-5a62-440f-b50e-aff4923795d6",
   "metadata": {},
   "source": [
    "As we see from the plot above, the forward Euler method tends to underestimate the solution, whereas the backward Euler method tends to overestimate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abe6ea-0f51-43d5-9c87-3120db0124d7",
   "metadata": {},
   "source": [
    "## ODE example: source transformation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "Single scalar input -> suitable for forward mode. Perhaps optimisation of theta-method timestepping scheme.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "Navigate to `exercises/ode` and apply forward mode AD to `theta.f90` with Tapenade, specifying the `theta` argument as independent and the output `residual` as dependent.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -head \"theta_method(theta)\\(residual)\" theta.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./theta_d.f90 \n",
    "@@ Created ./theta_d.msg\n",
    "$ cat theta_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of theta_method in forward (tangent) mode:\n",
    "!   variations   of useful results: residual\n",
    "!   with respect to varying inputs: theta\n",
    "!   RW status of diff variables: theta:in residual:out\n",
    "SUBROUTINE THETA_METHOD_D(u, u_, f, f_, dt, theta, thetad, residual, &\n",
    "& residuald)\n",
    "  IMPLICIT NONE\n",
    "! solution at current timestep\n",
    "  REAL, INTENT(IN) :: u\n",
    "! solution at previous timestep\n",
    "  REAL, INTENT(IN) :: u_\n",
    "! f(u)\n",
    "  REAL, INTENT(IN) :: f\n",
    "! f(u_)\n",
    "  REAL, INTENT(IN) :: f_\n",
    "! Timestep length\n",
    "  REAL, INTENT(IN) :: dt\n",
    "! Theta parameter\n",
    "  REAL, INTENT(IN) :: theta\n",
    "  REAL, INTENT(IN) :: thetad\n",
    "! Residual for current iteration\n",
    "  REAL, INTENT(OUT) :: residual\n",
    "  REAL, INTENT(OUT) :: residuald\n",
    "  residuald = -(dt*(f-f_)*thetad)\n",
    "  residual = u - u_ - dt*(theta*f+(1-theta)*f_)\n",
    "END SUBROUTINE THETA_METHOD_D\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e419ca-aaf2-438e-8cc1-b0656655e390",
   "metadata": {},
   "source": [
    "## Validation: the Taylor test\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "</div>\n",
    "\n",
    "#### Operator overloading tools:\n",
    "\n",
    "* LLVM\n",
    "    * [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "* C/C++\n",
    "    * About 2 dozen AD tools!\n",
    "    * e.g., [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial]\n",
    "* Fortran\n",
    "    * [Differentia](https://github.com/Nicholaswogan/Differentia), [lots of abandonware...]\n",
    "* Python\n",
    "    * [PyADOL-C](https://github.com/b45ch1/pyadolc), [Jax](https://github.com/jax-ml/jax), [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
    "* Julia\n",
    "    * About 2 dozen AD tools! https://juliadiff.org/\n",
    "    * e.g., Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable)\n",
    "    * [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface)\n",
    "* Domain-specific\n",
    "    * [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "* And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Calculating the *full* Jacobian\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question</b>\n",
    "\n",
    "Given a map $\\mathbf{f}$, some input $\\mathbf{x}$, and some seed $\\dot{\\mathbf{x}}$, we have the Jacobian vector product\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "*How can we use this to compute the full Jacobian matrix $\\nabla\\mathbf{f}(\\mathbf{x})$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "$$\\nabla F(x)=\\nabla F(x)I_n=\\nabla F(x)\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "Apply JVP to the $n$ canonical unit vectors.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Exercise in notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Sparse AD\n",
    "\n",
    "We can compute the full Jacobian with\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\nabla\\mathbf{f}(\\mathbf{x})I_n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "* But what about when $n$ gets very large?\n",
    "* And what about when the Jacobian is sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6671-c78f-4442-be33-0a1c04b6786f",
   "metadata": {},
   "source": [
    "#### Simplest case: diagonal Jacobian\n",
    "\n",
    "Suppose $\\nabla\\mathbf{f}(\\mathbf{x})$ is diagonal, say\n",
    "\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}.$$\n",
    "\n",
    "Then, for a seed vector $\\dot{\\mathbf{x}}=\\begin{bmatrix}\\dot{x}_1 & \\dot{x}_2 & \\dots & \\dot{x}_n\\end{bmatrix}^T$, we have\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}\n",
    "\\begin{bmatrix}\\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}f_1\\dot{x}_1 \\\\ f_2\\dot{x}_2 \\\\ \\vdots \\\\ f_n\\dot{x}_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can then back out the full Jacobian by putting each entry on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcb353-5df8-40e0-808e-8b23ca15b3c4",
   "metadata": {},
   "source": [
    "#### What colour is your Jacobian?\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "<ul>\n",
    "    <li>Demonstrate simplest case where the matrix is diagonal.</li>\n",
    "    <li>Orthogonal columns simple demo</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038997c-18ab-4a78-91ea-aeb2a702efe5",
   "metadata": {},
   "source": [
    "<img src=\"images/colours.png\" width=600 />\n",
    "\n",
    "*Jacobian colouring diagram taken from Gebremedhin, et al (2005).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eebe8-e8aa-4fa9-9ae5-348ccab005a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Exercise in notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e383c-201b-4867-af13-ecbb3f4349e3",
   "metadata": {},
   "source": [
    "<img src=\"images/runtimes.png\" width=600 />\n",
    "\n",
    "*Performance comparison for different Jacobian computation approaches taken from Wallwork et al. (2019).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c5dd0-bb4e-4593-a1f4-410413be9f75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "More diagrams from ADOL-C/PETSc preprint comparing runtimes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "* Low-level: elementary operators, e.g., Tapenade, ADIC, ADOL-C.\n",
    "* Medium-level: API calls, e.g., AD in PETSc.\n",
    "* High-level: high-level maths, e.g., Pyadjoint/dolfin-adjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Example code snippets of the above\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* R. E. Wengert. *A simple automatic derivative evaluation program*. Communications\n",
    "of the ACM, 7(8):463–464, 1964.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992.\n",
    "* D. Cortild, et al. *A Brief Review of Automatic Differentiation.* (2023).\n",
    "* A. H. Gebremedhin, et al. *What color is your Jacobian? Graph coloring for computing derivatives* (2005). SIAM review, 47(4), pp.629-705.\n",
    "* J. G. Wallwork, P. Hovland, H. Zhang, and O. Marin, *Computing derivatives for petsc adjoint solvers using algorithmic differentiation* (2019), arXiv preprint arXiv:1909.02836."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
