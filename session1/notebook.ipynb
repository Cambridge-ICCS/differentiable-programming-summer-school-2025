{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaadad0-da7a-4895-b1e9-e38e30a99eae",
   "metadata": {},
   "source": [
    "# Session 1: Forward mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454c496-2b6f-4cbe-b87e-d3dbb78a8e19",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "#### Terminology\n",
    "\n",
    "This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "Similarly, for $m\\in\\mathbb{N}$ dimensional, differentiable, vector-valued function $\\mathbf{f}:A\\rightarrow\\mathbb{R}^m$ with scalar input, we have derivative notations $\\mathbf{f}'(x)$ and $\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}$.\n",
    "\n",
    "For a differentiable function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is written as $f=f(x,y)$ then we have the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ with respect to first and second components, respectively. We use\n",
    "$$\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)$$\n",
    "to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\n",
    "\n",
    "When it comes to derivatives in code, we use the `_d` notation, which is standard in the AD literature. Its meaning will be described in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Origins of AD in 1950s.\n",
    "* However, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\n",
    "* Forward mode (the subject of this session) was discovered by Wengert in 1964.\n",
    "* Further developed by Griewank in the late 1980s.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: images?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "The idea of AD is to **treat a model as a sequence of elementary instructions** (e.g., addition, multiplication, exponentiation). Here a *model* could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the *chain rule*.\n",
    "\n",
    "#### Recap on A-level maths: the Chain Rule\n",
    "\n",
    "Consider two composable, differentiable (mathematical) functions, $f$ and $g$, with composition $h=f\\circ g$. By definition, this means\n",
    "$$h(x)=(f\\circ g)(x)=g(f(x)).$$\n",
    "\n",
    "Then the *chain rule* states that the derivative of $h$ may be computed in terms of the derivatives of $f$ and $g$ using the formula\n",
    "$$h'(x)=(f\\circ g)'(x)=(f\\circ g')(x)\\,f'(x)=g'(f(x))\\,f'(x).$$\n",
    "\n",
    "Equivalently, in Leibniz notation:\n",
    "$$\\frac{\\mathrm{d}h}{\\mathrm{d}x}=\\frac{\\mathrm{d}g}{\\mathrm{d}f}\\frac{\\mathrm{d}f}{\\mathrm{d}x}.$$\n",
    "\n",
    "For variables with multiple arguments, the result is equivalent for each partial derivative, e.g.,\n",
    "$$\\frac{\\partial h}{\\partial x}=\\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "Consider two functions acting on real numbers:\n",
    "$$f(x,y)=xy$$\n",
    "and\n",
    "$$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "Here $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ takes two inputs and returns a single output, while $g:\\mathbb{R}\\rightarrow\\mathbb{R}^2$ takes a single input and returns two outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Convince yourself that it is well defined for these functions may be composed in either order. (Although they won't necessarily give the same value!)\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "The image of $f$ is the set of all real numbers, so its image is the same as the domain of $g$ (i.e., $\\text{im}(f)=\\mathbb{R}=\\text{dom}(g)$).\n",
    "\n",
    "The image of $g$ is $[-1,1]^2=[-1,1]\\times[-1,1]$ because $\\sin$ and $\\cos$ give values between -1 and 1. Since this is a subset of $\\mathbb{R}^2$, the image of $g$ is a subset of the domain of $f$ (i.e., $\\text{im}(g)\\subset\\text{dom}(f)$).\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Example 1 (continued)\n",
    "\n",
    "Consider the composition $h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$. Then we have\n",
    "$$h(x,y)=(f\\circ g)(x,y)=g(f(x,y))=g(xy)=(\\sin(xy),\\cos(xy)).$$\n",
    "\n",
    "For the derivative of each component,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}=y,\n",
    "\\quad\\frac{\\partial f}{\\partial y}=x,\n",
    "\\quad\\frac{\\partial g}{\\partial z}=(\\cos(z),-\\sin(z)).\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs, i.e.,\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x},\n",
    "\\quad\\frac{\\partial h_1}{\\partial y},\n",
    "\\quad\\frac{\\partial h_2}{\\partial x},\n",
    "\\quad\\frac{\\partial h_2}{\\partial y},\n",
    "$$\n",
    "where $h(x,y)=(h_1(x,y),h_2(x,y))$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x}=\\cos(z),y=y\\cos(xy),\n",
    "\\quad\\frac{\\partial h_1}{\\partial y}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial y}=\\cos(z)x=x\\cos(xy),\n",
    "$$\n",
    "where $z=f(x,y)$ and\n",
    "$$\n",
    "\\quad\\frac{\\partial h_2}{\\partial x}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x}=\\sin(z)y=y\\sin(xy),\n",
    "\\quad\\frac{\\partial h_2}{\\partial y}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial y}=\\sin(z)x=x\\sin(xy).\n",
    "$$  \n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5886a-e982-4995-872c-ffbc8375208e",
   "metadata": {},
   "source": [
    "## Example 1: Directed Acyclic Graph\n",
    "\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  Recall\n",
    "  $$f(x,y)=xy$$\n",
    "  and\n",
    "  $$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "</div>\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "  <img src=\"images/forward_AD_as_DAG.png\" style=\"width: 50%;\">\n",
    "  <div><strong>Figure 1:</strong> Directed Acyclic Graph (DAG) for the mathematical functions in Example 1.</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODOs: 1. switch notation to have $x$ and $y$ as inputs and then $h_1$ and $h_2$ as outputs. 2. why aren't the columns working?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    "\n",
    "Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Approach 1: Source transformation\n",
    "\n",
    "High level idea: Given some (code) function `f(x)`, generate the code for the function `f_d(x, x_d)` for its (directional) derivative, where `x_d` is the seed vector.\n",
    "\n",
    "Often the difficult part is then hooking the differentiated code into the wider model/build system.\n",
    "\n",
    "#### Source transformation tools:\n",
    "\n",
    "* [Tapenade](https://tapenade.gitlabpages.inria.fr/userdoc/build/html/index.html) (C, Fortran, Julia*)\n",
    "* [TAF](http://fastopt.com/products/taf) (Fortran) [commerical]\n",
    "* [PSyAD](https://psyclone-adjoint.readthedocs.io/en/stable) (domain-specific)\n",
    "\n",
    "*\\*Work in progress*\n",
    "\n",
    "Below we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at `session1/exercise1/subroutine1.f90` and `session1/exercise1/subroutine2.f90`, respectively.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Either [install Tapenade](https://tapenade.gitlabpages.inria.fr/tapenade/distrib/README.html)* or visit the [Tapenade web interface](http://tapenade.inria.fr:8080/tapenade/index.jsp).\n",
    "2. Apply Tapenade to each of these subroutines using its default setting, which will apply forward mode to compute the JVP for some seed vector.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "cd session1/exercise1\n",
    "tapenade subroutine1.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in forward (tangent) mode:\n",
    "!   variations   of useful results: z\n",
    "!   with respect to varying inputs: x y\n",
    "!   RW status of diff variables: x:in y:in z:out\n",
    "SUBROUTINE F_D(x, xd, y, yd, z, zd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL, INTENT(IN) :: xd, yd\n",
    "  REAL, INTENT(OUT) :: z\n",
    "  REAL, INTENT(OUT) :: zd\n",
    "  zd = y*xd + x*yd\n",
    "  z = x*y\n",
    "END SUBROUTINE F_D\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "cd session1/exercise1\n",
    "tapenade subroutine2.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in forward (tangent) mode:\n",
    "!   variations   of useful results: v\n",
    "!   with respect to varying inputs: z\n",
    "!   RW status of diff variables: v:out z:in\n",
    "SUBROUTINE G_D(z, zd, v, vd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL, INTENT(IN) :: zd\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: v\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: vd\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  vd = (/COS(z)*zd, -(SIN(z)*zd)/)\n",
    "  v = (/SIN(z), COS(z)/)\n",
    "END SUBROUTINE G_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "</div>\n",
    "\n",
    "#### Operator overloading tools:\n",
    "\n",
    "* LLVM\n",
    "    * [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "* C/C++\n",
    "    * About 2 dozen AD tools!\n",
    "    * e.g., [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial]\n",
    "* Fortran\n",
    "    * [Differentia](https://github.com/Nicholaswogan/Differentia), [lots of abandonware...]\n",
    "* Python\n",
    "    * [PyADOL-C](https://github.com/b45ch1/pyadolc), [Jax](https://github.com/jax-ml/jax), [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
    "* Julia\n",
    "    * About 2 dozen AD tools! https://juliadiff.org/\n",
    "    * e.g., Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable)\n",
    "    * [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface)\n",
    "* Domain-specific\n",
    "    * [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "* And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Exercise: ODE-constrained optimisation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "Single scalar input -> suitable for forward mode. Perhaps optimisation of theta-method timestepping scheme.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Validation: the Taylor test\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Calculating the *full* Jacobian\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question</b>\n",
    "\n",
    "Given a map $\\mathbf{f}$, some input $\\mathbf{x}$, and some seed $\\dot{\\mathbf{x}}$, we have the Jacobian vector product\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "*How can we use this to compute the full Jacobian matrix $\\nabla\\mathbf{f}(\\mathbf{x})$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "$$\\nabla F(x)=\\nabla F(x)I_n=\\nabla F(x)\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "Apply JVP to the $n$ canonical unit vectors.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Exercise in notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Sparse AD\n",
    "\n",
    "We can compute the full Jacobian with\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\nabla\\mathbf{f}(\\mathbf{x})I_n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "* But what about when $n$ gets very large?\n",
    "* And what about when the Jacobian is sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6671-c78f-4442-be33-0a1c04b6786f",
   "metadata": {},
   "source": [
    "#### Simplest case: diagonal Jacobian\n",
    "\n",
    "Suppose $\\nabla\\mathbf{f}(\\mathbf{x})$ is diagonal, say\n",
    "\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}.$$\n",
    "\n",
    "Then, for a seed vector $\\dot{\\mathbf{x}}=\\begin{bmatrix}\\dot{x}_1 & \\dot{x}_2 & \\dots & \\dot{x}_n\\end{bmatrix}^T$, we have\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}\n",
    "\\begin{bmatrix}\\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}f_1\\dot{x}_1 \\\\ f_2\\dot{x}_2 \\\\ \\vdots \\\\ f_n\\dot{x}_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can then back out the full Jacobian by putting each entry on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbcb353-5df8-40e0-808e-8b23ca15b3c4",
   "metadata": {},
   "source": [
    "#### What colour is your Jacobian?\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "<ul>\n",
    "    <li>Demonstrate simplest case where the matrix is diagonal.</li>\n",
    "    <li>Orthogonal columns simple demo</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038997c-18ab-4a78-91ea-aeb2a702efe5",
   "metadata": {},
   "source": [
    "<img src=\"images/colours.png\" width=600 />\n",
    "\n",
    "*Jacobian colouring diagram taken from Gebremedhin, et al (2005).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eebe8-e8aa-4fa9-9ae5-348ccab005a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Exercise in notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e383c-201b-4867-af13-ecbb3f4349e3",
   "metadata": {},
   "source": [
    "<img src=\"images/runtimes.png\" width=600 />\n",
    "\n",
    "*Performance comparison for different Jacobian computation approaches taken from Wallwork et al. (2019).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c5dd0-bb4e-4593-a1f4-410413be9f75",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "More diagrams from ADOL-C/PETSc preprint comparing runtimes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "* Low-level: elementary operators, e.g., Tapenade, ADIC, ADOL-C.\n",
    "* Medium-level: API calls, e.g., AD in PETSc.\n",
    "* High-level: high-level maths, e.g., Pyadjoint/dolfin-adjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Example code snippets of the above\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* R. E. Wengert. *A simple automatic derivative evaluation program*. Communications\n",
    "of the ACM, 7(8):463–464, 1964.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992.\n",
    "* D. Cortild, et al. *A Brief Review of Automatic Differentiation.* (2023).\n",
    "* A. H. Gebremedhin, et al. *What color is your Jacobian? Graph coloring for computing derivatives* (2005). SIAM review, 47(4), pp.629-705.\n",
    "* J. G. Wallwork, P. Hovland, H. Zhang, and O. Marin, *Computing derivatives for petsc adjoint solvers using algorithmic differentiation* (2019), arXiv preprint arXiv:1909.02836."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
