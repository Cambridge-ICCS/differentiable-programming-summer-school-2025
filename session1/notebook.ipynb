{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Session 1: Forward mode differentiation\n",
    "\n",
    "This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "For a mathematical function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "Similarly, for $m\\in\\mathbb{N}$ dimensional vector-valued function $\\mathbf{f}:A\\rightarrow\\mathbb{R}^m$ with scalar input, we have derivative notations $\\mathbf{f}'(x)$ and $\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}$.\n",
    "\n",
    "For a function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is written as $f=f(x,y)$ then we have the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ with respect to first and second components, respectively. We use\n",
    "$$\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)$$\n",
    "to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\n",
    "\n",
    "When it comes to derivatives of code, we use the `_d` notation, which is standard in the AD literature. Its meaning will be described in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Origins of AD in 1950s.\n",
    "* However, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\n",
    "* Forward mode (the subject of this session) was discovered by Wengert in 1964.\n",
    "* Further developed by Griewank in the late 1980s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "The idea of AD is to **treat a model as a sequence of elementary instructions** (e.g., addition, multiplication, exponentiation). Here a *model* could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the *chain rule*.\n",
    "\n",
    "#### Recap on A-level maths: the Chain Rule\n",
    "\n",
    "Consider two composable, differentiable (mathematical) functions, $f$ and $g$, with composition $h=f\\circ g$. By definition, this means\n",
    "$$h(x)=(f\\circ g)(x)=g(f(x)).$$\n",
    "\n",
    "Then the *chain rule* states that the derivative of $h$ may be computed in terms of the derivatives of $f$ and $g$ using the formula\n",
    "$$h'(x)=(f\\circ g)'(x)=(f'\\circ g)(x)\\,g'(x)=g(f'(x))\\,g'(x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Consider two functions acting on real numbers, $f(x,y)=xy$ and $g(z)=(\\sin(z),\\cos(z))$. Here $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ takes two inputs and returns a single output, while $g:\\mathbb{R}\\rightarrow\\mathbb{R}^2$ takes a single input and returns two outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Convince yourself that it is well defined for these functions may be composed in either order. (Although they won't necessarily give the same value!)\n",
    "</div>\n",
    "\n",
    "Consider the composition $h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$. Then we have\n",
    "$$h(x,y)=(f\\circ g)(x,y)=g(f(x,y))=(\\sin(xy),\\cos(xy)).$$\n",
    "\n",
    "For the derivative of each component,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}=y,\n",
    "\\quad\\frac{\\partial f}{\\partial y}=x,\n",
    "\\quad\\frac{\\partial g}{\\partial z}=(\\cos(z),-\\sin(z)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs, i.e.,\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x},\n",
    "\\quad\\frac{\\partial h_1}{\\partial y},\n",
    "\\quad\\frac{\\partial h_2}{\\partial x},\n",
    "\\quad\\frac{\\partial h_2}{\\partial y},\n",
    "$$\n",
    "where $h(x,y)=(h_1(x,y),h_2(x,y))$.\n",
    "\n",
    "<details>\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x}=y\\cos(z)=y\\cos(xy),\n",
    "\\quad\\frac{\\partial h_1}{\\partial y}=x\\cos(z)=x\\cos(xy),\n",
    "\\quad\\frac{\\partial h_2}{\\partial x}=y\\sin(z)=y\\sin(xy),\n",
    "\\quad\\frac{\\partial h_2}{\\partial y}=x\\sin(z)=x\\sin(xy),\n",
    "$$  \n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    "\n",
    "Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "DAG interpretation diagram\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Approach 1: Source transformation\n",
    "\n",
    "High level idea: Given some (code) function `f(x)`, generate the code for the function `f_d(x, x_d)` for its (directional) derivative, where `x_d` is the seed vector.\n",
    "\n",
    "Often the difficult part is then hooking the differentiated code into the wider model/build system.\n",
    "\n",
    "#### Source transformation tools:\n",
    "\n",
    "* [Tapenade](https://tapenade.gitlabpages.inria.fr/userdoc/build/html/index.html) (C, Fortran, Julia*)\n",
    "* [TAF](http://fastopt.com/products/taf) (Fortran) [commerical]\n",
    "* [PSyAD](https://psyclone-adjoint.readthedocs.io/en/stable) (domain-specific)\n",
    "\n",
    "*\\*Work in progress*\n",
    "\n",
    "Below we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at `session1/exercise1/subroutine1.f90` and `session1/exercise1/subroutine2.f90`, respectively.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Either [install Tapenade](https://tapenade.gitlabpages.inria.fr/tapenade/docs/html/distrib/README-install.html)* or visit the [Tapenade web interface](http://tapenade.inria.fr:8080/tapenade/index.jsp).\n",
    "2. Apply Tapenade to each of these subroutines using its default setting, which will apply forward mode to compute the JVP for some seed vector.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "cd session1/exercise1\n",
    "tapenade subroutine1.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in forward (tangent) mode:\n",
    "!   variations   of useful results: z\n",
    "!   with respect to varying inputs: x y\n",
    "!   RW status of diff variables: x:in y:in z:out\n",
    "SUBROUTINE F_D(x, xd, y, yd, z, zd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL, INTENT(IN) :: xd, yd\n",
    "  REAL, INTENT(OUT) :: z\n",
    "  REAL, INTENT(OUT) :: zd\n",
    "  zd = y*xd + x*yd\n",
    "  z = x*y\n",
    "END SUBROUTINE F_D\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "cd session1/exercise1\n",
    "tapenade subroutine2.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in forward (tangent) mode:\n",
    "!   variations   of useful results: v\n",
    "!   with respect to varying inputs: z\n",
    "!   RW status of diff variables: v:out z:in\n",
    "SUBROUTINE G_D(z, zd, v, vd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL, INTENT(IN) :: zd\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: v\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: vd\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  vd = (/COS(z)*zd, -(SIN(z)*zd)/)\n",
    "  v = (/SIN(z), COS(z)/)\n",
    "END SUBROUTINE G_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "</div>\n",
    "\n",
    "#### Operator overloading tools:\n",
    "\n",
    "* LLVM\n",
    "    * [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "* C/C++\n",
    "    * About 2 dozen AD tools!\n",
    "    * e.g., [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial]\n",
    "* Fortran\n",
    "    * [Differentia](https://github.com/Nicholaswogan/Differentia), [lots of abandonware...]\n",
    "* Python\n",
    "    * [PyADOL-C](https://github.com/b45ch1/pyadolc), [Jax](https://github.com/jax-ml/jax), [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
    "* Julia\n",
    "    * About 2 dozen AD tools! https://juliadiff.org/\n",
    "    * e.g., Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable)\n",
    "    * [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface)\n",
    "* Domain-specific\n",
    "    * [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "* And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Exercise: ODE-constrained optimisation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "Single scalar input -> suitable for forward mode. Perhaps optimisation of theta-method timestepping scheme.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Validation: the Taylor test\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Calculating the *full* Jacobian\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question</b>\n",
    "\n",
    "Given a map $F$, some input $x$, and some seed $\\dot{x}$, we have the Jacobian vector product\n",
    "$$\\nabla F(x)\\dot{x}.$$\n",
    "\n",
    "*How can we use this to compute the full Jacobian matrix $\\nabla F(x)$?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "$$\\nabla F(x)=\\nabla F(x)I_n=\\nabla F(x)\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "Apply JVP to the $n$ canonical unit vectors.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Exercise in notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Sparse AD\n",
    "\n",
    "We can compute the full Jacobian with\n",
    "$$\\nabla F(x)=\\nabla F(x)I_n=\\nabla F(x)\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "* But what about when $n$ gets very large?\n",
    "* And what about when the Jacobian is sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### What colour is your Jacobian?\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "<ul>\n",
    "    <li>Demonstrate simplest case where the matrix is diagonal.</li>\n",
    "    <li>Orthogonal columns simple demo</li>\n",
    "    <li>Colouring, nice image from paper, citing (A. H. Gebremedhin, et al (2025))</li>\n",
    "    <li>Exercise?</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Diagram from ADOL-C/PETSc preprint comparing runtimes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "* Low-level: elementary operators, e.g., Tapenade, ADIC, ADOL-C.\n",
    "* Medium-level: API calls, e.g., AD in PETSc.\n",
    "* High-level: high-level maths, e.g., Pyadjoint/dolfin-adjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO</b>\n",
    "Example code snippets of the above\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* R. E. Wengert. *A simple automatic derivative evaluation program*. Communications\n",
    "of the ACM, 7(8):463–464, 1964.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992.\n",
    "* D. Cortild, et al. *A Brief Review of Automatic Differentiation.* (2023).\n",
    "* A. H. Gebremedhin, et al. *What color is your Jacobian? Graph coloring for computing derivatives* (2005). SIAM review, 47(4), pp.629-705."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
