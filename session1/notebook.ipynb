{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c6d35-ddb0-48bf-aa9f-a449f59ecab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpltools import annotation\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaadad0-da7a-4895-b1e9-e38e30a99eae",
   "metadata": {},
   "source": [
    "# Session 1: Forward mode differentiation and source transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5de901-6499-43e0-91da-d407101c3408",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "From the abstract for the session:\n",
    "\n",
    "> *Derivatives are at the heart of scientific programming. From the Jacobian matrices used to solve nonlinear systems to the gradient vectors used for optimisation methods, from the backpropagation operation in machine learning to the data assimilation methods used in weather forecasting, all of these techniques rely on derivative information. Differentiable programming (also known as automatic/algorithmic differentiation (AD)) provides a suite of tools for users to compute derivatives of quantities in their code without any manual encoding.*\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: elaborate, e.g., some examples</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a77216-01d1-4bbd-b5e4-36154a814f86",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In today's session we will:\n",
    "\n",
    "* Get a brief history of automatic differentiation.\n",
    "* Learn about *forward mode* and the *source transformation* approach.\n",
    "* Try out the *Tapenade* source transformation AD tool applied to some test problems.\n",
    "* Verify the code generated by Tapenade both manually and using the *Taylor test*.\n",
    "* Learn about compressed and matrix-free approaches for sparse problems.\n",
    "* Consider how different levels of abstraction can be appropriate for different problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "Similarly, for $m\\in\\mathbb{N}$ dimensional, differentiable, vector-valued function $\\mathbf{f}:A\\rightarrow\\mathbb{R}^m$ with scalar input, we have derivative notations $\\mathbf{f}'(x)$ and $\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}$.\n",
    "\n",
    "For a differentiable function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is written as $f=f(x,y)$ then we have the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ with respect to first and second components, respectively. We use\n",
    "$$\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)$$\n",
    "to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\n",
    "\n",
    "When it comes to derivatives in code, we use the `_d` notation, which is standard in the AD literature. Its meaning will be described in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Origins of AD in 1950s.\n",
    "* However, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\n",
    "* Forward mode (the subject of this session) was discovered by Wengert in 1964.\n",
    "* Further developed by Griewank in the late 1980s.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: images?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "The idea of AD is to **treat a model as a sequence of elementary instructions** (e.g., addition, multiplication, exponentiation). Here a *model* could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the *chain rule*.\n",
    "\n",
    "#### Recap on A-level maths: the Chain Rule\n",
    "\n",
    "Consider two composable, differentiable (mathematical) functions, $f$ and $g$, with composition $h=f\\circ g$. By definition, this means\n",
    "$$h(x)=(f\\circ g)(x)=g(f(x)).$$\n",
    "\n",
    "Then the *chain rule* states that the derivative of $h$ may be computed in terms of the derivatives of $f$ and $g$ using the formula\n",
    "$$h'(x)=(f\\circ g)'(x)=(f\\circ g')(x)\\,f'(x)=g'(f(x))\\,f'(x).$$\n",
    "\n",
    "Equivalently, in Leibniz notation:\n",
    "$$\\frac{\\mathrm{d}h}{\\mathrm{d}x}=\\frac{\\mathrm{d}g}{\\mathrm{d}f}\\frac{\\mathrm{d}f}{\\mathrm{d}x}.$$\n",
    "\n",
    "For variables with multiple arguments, the result is equivalent for each partial derivative, e.g.,\n",
    "$$\\frac{\\partial h}{\\partial x}=\\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example\n",
    "\n",
    "Consider two functions acting on real numbers:\n",
    "$$f(x,y)=xy$$\n",
    "and\n",
    "$$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "Here $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ takes two inputs and returns a single output, while $g:\\mathbb{R}\\rightarrow\\mathbb{R}^2$ takes a single input and returns two outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that it is well defined for these functions may be composed in either order. (Although they won't necessarily give the same value!)\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "The image of $f$ is the set of all real numbers, so its image is the same as the domain of $g$ (i.e., $\\text{im}(f)=\\mathbb{R}=\\text{dom}(g)$).\n",
    "\n",
    "The image of $g$ is $[-1,1]^2=[-1,1]\\times[-1,1]$ because $\\sin$ and $\\cos$ give values between -1 and 1. Since this is a subset of $\\mathbb{R}^2$, the image of $g$ is a subset of the domain of $f$ (i.e., $\\text{im}(g)\\subset\\text{dom}(f)$).\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example (continued)\n",
    "\n",
    "Consider the composition $h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$. Then we have\n",
    "$$h(x,y)=(f\\circ g)(x,y)=g(f(x,y))=g(xy)=(\\sin(xy),\\cos(xy)).$$\n",
    "\n",
    "For the derivative of each component,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}=y,\n",
    "\\quad\\frac{\\partial f}{\\partial y}=x,\n",
    "\\quad\\frac{\\partial g}{\\partial z}=(\\cos(z),-\\sin(z)).\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs, i.e.,\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x},\n",
    "\\quad\\frac{\\partial h_1}{\\partial y},\n",
    "\\quad\\frac{\\partial h_2}{\\partial x},\n",
    "\\quad\\frac{\\partial h_2}{\\partial y},\n",
    "$$\n",
    "where $h(x,y)=(h_1(x,y),h_2(x,y))$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x}=\\cos(z)y=y\\cos(xy),\n",
    "\\quad\\frac{\\partial h_1}{\\partial y}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial y}=\\cos(z)x=x\\cos(xy),\n",
    "$$\n",
    "where $z=f(x,y)$ and\n",
    "$$\n",
    "\\quad\\frac{\\partial h_2}{\\partial x}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x}=\\sin(z)y=y\\sin(xy),\n",
    "\\quad\\frac{\\partial h_2}{\\partial y}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial y}=\\sin(z)x=x\\sin(xy).\n",
    "$$  \n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5886a-e982-4995-872c-ffbc8375208e",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Directed Acyclic Graph\n",
    "\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  Recall\n",
    "  $$f(x,y)=xy$$\n",
    "  and\n",
    "  $$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "\n",
    "  We can visualise the composition in terms of a DAG. Here $\\dot{x}$ and $\\dot{y}$ denote *seeds* for the derivatives of the input variables $x$ and $y$, respectively. Think of these as inputs from outside of the part of the program that we are differentiating.\n",
    "</div>\n",
    "<div class=\"column\" style=\"width: 50%;\">\n",
    "  <div style=\"text-align: center;\">\n",
    "  <img src=\"images/forward_AD_as_DAG.png\" style=\"width: 50%;\">\n",
    "  <div><strong>Figure 1:</strong> Directed Acyclic Graph (DAG) for the mathematical functions in Example 1.</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODOs: 1. switch notation to have $x$ and $y$ as inputs and then $h_1$ and $h_2$ as outputs. 2. why aren't the columns working?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    "\n",
    "Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    "\n",
    "Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Approach 1: Source transformation\n",
    "\n",
    "High level idea: Given some (code) function `f(x)` and a seed vector `x_d`, generate the code for the function `f_d(x, x_d)` for its (directional) derivative.\n",
    "\n",
    "#### Evaluation of the source transformation approach:\n",
    "\n",
    "* Clearly demonstrates the differentiation process.\n",
    "* Static analysis, done ahead of time.\n",
    "* Does not require a 'tape' (see later).\n",
    "* Often the difficult part is hooking the differentiated code into the wider model/build system.\n",
    "* Limited number of tools.\n",
    "\n",
    "#### Source transformation tools:\n",
    "\n",
    "* [Tapenade](https://tapenade.gitlabpages.inria.fr/userdoc/build/html/index.html) (C, Fortran, Julia*)\n",
    "* [OpenAD](https://www.mcs.anl.gov/OpenAD) (Fortran)\n",
    "* [TAF](http://fastopt.com/products/taf) (Fortran) [commerical]\n",
    "* [PSyAD](https://psyclone-adjoint.readthedocs.io/en/stable)* (domain-specific)\n",
    "\n",
    "*\\*Work in progress*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: source transformation\n",
    "\n",
    "Below we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at `session1/exercises/fg/f.f90` and `session1/exercises/fg/g.f90`, respectively.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Either [install Tapenade](https://tapenade.gitlabpages.inria.fr/tapenade/distrib/README.html)* or visit the [Tapenade web interface](http://tapenade.inria.fr:8080/tapenade/index.jsp).\n",
    "2. Run `tapenade -h` to see all the options.\n",
    "3. Apply Tapenade to each of these subroutines using its default setting, which will apply forward mode to compute the JVP for some seed vector.\n",
    "4. Inspect the output files `f_d.f90` and `g_d.f90` and check they are as you expect.\n",
    "5. Inspect the message files `f_d.msg` and `g_d.msg`.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "<b>Solutions 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solutions 3 and 4</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade f.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./f_d.f90 \n",
    "@@ Created ./f_d.msg\n",
    "$ cat f_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in forward (tangent) mode:\n",
    "!   variations   of useful results: z\n",
    "!   with respect to varying inputs: x y\n",
    "!   RW status of diff variables: x:in y:in z:out\n",
    "SUBROUTINE F_D(x, xd, y, yd, z, zd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL, INTENT(IN) :: xd, yd\n",
    "  REAL, INTENT(OUT) :: z\n",
    "  REAL, INTENT(OUT) :: zd\n",
    "  zd = y*xd + x*yd\n",
    "  z = x*y\n",
    "END SUBROUTINE F_D\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade g.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./g_d.f90 \n",
    "@@ Created ./g_d.msg\n",
    "$ cat g_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in forward (tangent) mode:\n",
    "!   variations   of useful results: v\n",
    "!   with respect to varying inputs: z\n",
    "!   RW status of diff variables: v:out z:in\n",
    "SUBROUTINE G_D(z, zd, v, vd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL, INTENT(IN) :: zd\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: v\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: vd\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  vd = (/COS(z)*zd, -(SIN(z)*zd)/)\n",
    "  v = (/SIN(z), COS(z)/)\n",
    "END SUBROUTINE G_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440754c-a4d9-48e6-b1ed-9637fe509606",
   "metadata": {},
   "source": [
    "## Verification: the Taylor test\n",
    "\n",
    "Recall the Taylor expansion of a differentiable function $f:A\\rightarrow\\mathbb{R}$, for $A\\subseteq\\mathbb{R}$:\n",
    "$$\n",
    "    f(x+h)=f(x)+hf'(x) + \\frac{1}{2!}h^2f''(x) + \\dots + \\frac{1}{n!}h^nf^{(n)}(x) + \\dots,\n",
    "$$\n",
    "for some $h\\in\\mathbb{R}$, i.e.,\n",
    "$$\n",
    "    f(x+h)=f(x)+hf'(x) + \\mathcal{O}(h^2).\n",
    "$$\n",
    "This gives rise to the first-order forward difference approximation of the first derivative as follows:\n",
    "$$\n",
    "    f'(x) \\approx \\frac{f(x+h)-f(x)}{h}.\n",
    "$$\n",
    "\n",
    "This is a rather crude approximation to a derivative, but it can do the job, given an appropriate choice of $h$.\n",
    "\n",
    "The idea of the Taylor test is to take smaller and smaller spacing values $h$ from a given input value and to check that the difference between the forward difference and the AD-generated result converges quadratically. That is, we verify that\n",
    "$$\n",
    "    \\|f(x+h)-f(x)-h\\:\\texttt{f\\_d}(x)\\|=\\mathcal{O}(h^2),\n",
    "$$\n",
    "where $\\texttt{f\\_d}$ is the AD-generated result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bf4ab-4d9f-4f90-b0de-582292a59cb3",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Taylor test\n",
    "\n",
    "We return to the previous example and double-check that we are happy with the output of the AD tool.\n",
    "\n",
    "For simplicity of demonstration, we translate `g` and `g_d` into Python and conduct the Taylor test here.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Could we use f2py here?</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622987f4-a91d-4009-823b-69d983d64d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(z):\n",
    "    \"\"\"\n",
    "    Python translation of the `g` subroutine from `session1/exercises/fg/g.f90` using NumPy.\n",
    "    \"\"\"\n",
    "    return np.array([np.sin(z), np.cos(z)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559c39a-7849-4a92-88c4-899c0aa5d61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(z, zd):\n",
    "    \"\"\"\n",
    "    Python translation of the `g_d` subroutine generated by Tapenade in `session1/exercises/fg/g_d.f90` using NumPy.\n",
    "    \"\"\"\n",
    "    vd = np.array([np.cos(z) * zd, -np.sin(z) * zd])\n",
    "    v = np.array([np.sin(z), np.cos(z)])\n",
    "    return v, vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445f6e9-2fb0-41e4-b779-199d7445e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose arbitrary input for g\n",
    "z = 1.0\n",
    "\n",
    "# Choose unit seed\n",
    "zd = 1.0\n",
    "\n",
    "# Compute the derivative using forward mode AD\n",
    "_, gd_ad = gd(z, zd)\n",
    "\n",
    "# Run the Taylor test over several spacing values\n",
    "spacings = [1.0, 0.1, 0.01, 0.001]\n",
    "errors = []\n",
    "for h in spacings:\n",
    "    errors.append(np.linalg.norm(g(z + h) - g(z) - h * gd_ad))\n",
    "\n",
    "# Plot the solution, demonstrating that the expected quadratic convergence is achieved\n",
    "fig, axes = plt.subplots()\n",
    "axes.loglog(spacings, errors, \"--x\")\n",
    "annotation.slope_marker((1e-2, 1e-4), 2, ax=axes, invert=True)\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b32c7-1e9a-4a0a-8f1f-420a94a3ecd4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "    \n",
    "Perform the Taylor test to double-check you are happy with the AD-generated derivative of $f$, too. Note that the input will have two dimensions so you will have to consider perturbations in both directions separately.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc0a0d-61eb-419c-baa5-f8eefb52954d",
   "metadata": {},
   "source": [
    "## Example: ODE-constrained optimisation\n",
    "\n",
    "Consider the scalar ordinary differential equation (ODE)\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=f(u),\\quad u(0)=u_0,\n",
    "$$\n",
    "where $t\\in[0,T]$ is the time variable, $T>0$ is the end time, and $u_0\\in\\mathbb{R}$ is the initial condition. Given some $f:A\\rightarrow\\mathbb{R}$ with $A\\subseteq\\mathbb{R}$, we seek to solve the ODE for $u:[0,T]\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For simplicity, let's consider the ODE\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=u,\\quad u(0)=1,\n",
    "$$\n",
    "i.e., $f(u)=u$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "    \n",
    "Convince yourself that the analytical solution of the ODE is $u=\\mathrm{e}^t$.\n",
    "\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Plugging $u(t)=\\mathrm{e}^t$ into the LHS gives $\\frac{\\mathrm{d}u}{\\mathrm{d}t}=\\mathrm{e}^t=u$, which satisfies the ODE. Checking the initial condition, we have $u(0)=\\mathrm{e}^0=1$, which also satisfies.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1468e1e9-1ea8-4e3b-b13d-162975a79088",
   "metadata": {},
   "source": [
    "## ODE example: forward vs backward Euler\n",
    "\n",
    "You've probably aware of the simplest example of an explicit timestepping method to approximate the solution of the ODE is *forward Euler* (a.k.a. explicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_{k-1}),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "You're probably also aware of the simplest example of an implicit timestepping method to approximate the solution of the ODE is *backward Euler* (a.k.a. implicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_k),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "These are special cases of the first-order *theta-method*\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)f(u_{k-1})+\\theta f(u_k),\n",
    "$$\n",
    "where $\\theta\\in[0,1]$.\n",
    "\n",
    "The forward and backward Euler approaches have been implemented for the case $f(u)=u$ in the `session1/exercises/ode` subdirectory.\n",
    "```sh\n",
    "$ cd exercises/ode\n",
    "$ ls\n",
    "backward_euler.f90 forward_euler.f90 theta.f90\n",
    "```\n",
    "Here `forward_euler.f90` and `backward_euler.f90` contain programs to solve the ODE, making use of common code from `theta.f90`, but with values $\\theta=0$ and $\\theta=1$, respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran forward_euler.f90 -o forward\n",
    "$ gfortran backward_euler.f90 -o backward\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./forward\n",
    "$ ./backward\n",
    "$ ls\n",
    "backward      backward_euler.f90  forward.csv        theta.f90\n",
    "backward.csv  forward             forward_euler.f90\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797eba8c-286c-41c3-80c4-38d862ccda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forward = pandas.read_csv(\"exercises/ode/forward.csv\")\n",
    "df_backward = pandas.read_csv(\"exercises/ode/backward.csv\")\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "times = np.linspace(0, 1, 101)\n",
    "\n",
    "axes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\n",
    "axes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\n",
    "axes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\n",
    "axes.legend()\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abe6ea-0f51-43d5-9c87-3120db0124d7",
   "metadata": {},
   "source": [
    "## ODE example: source transformation\n",
    "\n",
    "As we see from the plot above, the forward Euler method tends to underestimate the solution, whereas the backward Euler method tends to overestimate it. Let's try to optimise the value of $\\theta$ to best match the solution using a gradient-based optimisation method. To do that, we first need the gradient.\n",
    "\n",
    "The optimisation problem we seek to solve is to minimise some error measure $J$ for the approximation of $u$ by varying $\\theta$. That is,\n",
    "$$\n",
    "    \\min_{\\theta\\in[0,1]}J(u;\\theta).\n",
    "$$\n",
    "where the notation $J(u;\\theta)$ refers to the implicit dependence of the solution approximation $u$ on $\\theta$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "Navigate to `session1/exercises/ode` and apply forward mode AD to the `theta_method` subroutine in `theta.f90` with Tapenade, specifying the `theta` argument as independent and the output `u` as dependent.\n",
    "\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -head \"theta_method(theta)\\(u)\" theta.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./theta_d.f90 \n",
    "@@ Created ./theta_d.msg\n",
    "$ cat theta_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of theta_method in forward (tangent) mode:\n",
    "!   variations   of useful results: u\n",
    "!   with respect to varying inputs: theta\n",
    "!   RW status of diff variables: u:out theta:in\n",
    "SUBROUTINE THETA_METHOD_D(theta, thetad, filename, u, ud)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: theta\n",
    "  REAL, INTENT(IN) :: thetad\n",
    "  CHARACTER(len=*), INTENT(IN) :: filename\n",
    "  REAL, INTENT(OUT) :: u\n",
    "  REAL, INTENT(OUT) :: ud\n",
    "  REAL, PARAMETER :: dt=0.1\n",
    "  REAL, PARAMETER :: end_time=1.0\n",
    "  REAL :: t\n",
    "  REAL :: u_\n",
    "  REAL :: u_d\n",
    "! Create a CSV file for output\n",
    "  OPEN(unit=10, file=filename) \n",
    "  WRITE(unit=10, fmt='(''t,u'')') \n",
    "! Initialisation\n",
    "  t = 0.0\n",
    "  CALL INITIAL_CONDITION(u_)\n",
    "  WRITE(unit=10, fmt=100) t, u_\n",
    "  ud = 0.0\n",
    "  u_d = 0.0\n",
    "! Timestepping loop\n",
    "  DO WHILE (t .LT. end_time - 1e-05)\n",
    "    CALL THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n",
    "    u_d = ud\n",
    "    u_ = u\n",
    "    t = t + dt\n",
    "    WRITE(unit=10, fmt=100) t, u\n",
    "  END DO\n",
    "  CLOSE(unit=10) \n",
    " 100 FORMAT(f4.2,',',f4.2)\n",
    "END SUBROUTINE THETA_METHOD_D\n",
    "\n",
    "!  Differentiation of theta_step in forward (tangent) mode:\n",
    "!   variations   of useful results: u\n",
    "!   with respect to varying inputs: u_ theta\n",
    "SUBROUTINE THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n",
    "  IMPLICIT NONE\n",
    "! solution at current timestep\n",
    "  REAL, INTENT(OUT) :: u\n",
    "  REAL, INTENT(OUT) :: ud\n",
    "! solution at previous timestep\n",
    "  REAL, INTENT(IN) :: u_\n",
    "  REAL, INTENT(IN) :: u_d\n",
    "! Timestep length\n",
    "  REAL, INTENT(IN) :: dt\n",
    "! Theta parameter\n",
    "  REAL, INTENT(IN) :: theta\n",
    "  REAL, INTENT(IN) :: thetad\n",
    "  REAL :: temp\n",
    "  temp = u_/(-(dt*theta)+1)\n",
    "  ud = (dt*(1-theta)+1)*(u_d+temp*dt*thetad)/(1-dt*theta) - temp*dt*&\n",
    "&   thetad\n",
    "  u = (dt*(1-theta)+1)*temp\n",
    "END SUBROUTINE THETA_STEP_D\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ccf71-3555-4fd1-a923-01f2b124104c",
   "metadata": {},
   "source": [
    "## ODE example: optimisation with gradient descent\n",
    "\n",
    "Let's solve this ODE problem with one of the simplest gradient-based optimisation approaches: gradient descent. This amounts to an initial guess $\\theta_0$, followed by iterative updates\n",
    "$$\n",
    "    \\theta_{k+1}=\\theta_k+\\alpha_k\\:p_k,\n",
    "$$\n",
    "where $\\alpha_k$ is the step length at iteration $k$ and $p_k$ is the descent direction. For gradient descent, we simply take\n",
    "$$\n",
    "    p_k=-\\frac{\\mathrm{d}J_k}{\\mathrm{d}\\theta_k}.\n",
    "$$\n",
    "\n",
    "Since we know the analytical solution for this problem, we may make an 'artifical' choice of cost function such as\n",
    "$$\n",
    "    J(u;\\theta)=(u(1)-\\mathrm{e}^1)^2,\n",
    "$$\n",
    "where here $\\mathrm{e}^1$ is the analytical solution at the end time $t=1$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "The gradient descent algorithm is implemented for our ODE problem in `session1/exercises/ode/gradient_descent.f90`. However, there is another missing piece: you will also need to differentiate the cost function. Convince yourself you are satisfied with the output.\n",
    "\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade cost_function.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine cost_function as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./cost_function_d.f90 \n",
    "@@ Created ./cost_function_d.msg\n",
    "cat cost_function_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of cost_function in forward (tangent) mode:\n",
    "!   variations   of useful results: j\n",
    "!   with respect to varying inputs: u\n",
    "!   RW status of diff variables: j:out u:in\n",
    "! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\n",
    "SUBROUTINE COST_FUNCTION_D(u, ud, j, jd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: u\n",
    "  REAL, INTENT(IN) :: ud\n",
    "  REAL, INTENT(OUT) :: j\n",
    "  REAL, INTENT(OUT) :: jd\n",
    "  INTRINSIC EXP\n",
    "  REAL, PARAMETER :: e=EXP(1.0)\n",
    "  jd = 2*(u-e)*ud\n",
    "  j = (u-e)**2\n",
    "END SUBROUTINE COST_FUNCTION_D\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran gradient_descent.f90 -o gradient_descent\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./gradient_descent\n",
    "$ ls\n",
    "backward.csv        forward.csv           optimised.csv\n",
    "backward_euler.f90  forward_euler.f90     theta.f90\n",
    "cost_function.f90   gradient_descent.f90  theta_progress.csv\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4240c-9870-4fee-82bc-1ec369d32f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt = pandas.read_csv(\"exercises/ode/optimisation_progress.csv\")\n",
    "\n",
    "costs = np.array(df_opt[\"J\"])\n",
    "costs[0] = np.nan  # Remove the first entry because it's not actually a cost function value\n",
    "controls = np.array(df_opt[\"theta\"])\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "axes[0].loglog(df_opt[\"it\"], costs, \"--\", label=\"Cost function value\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "axes[1].plot(df_opt[\"it\"], df_opt[\"theta\"], \"--\", label=\"Control value\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5cdec-67ca-4ce9-ba3d-9526e9d30829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimised = pandas.read_csv(\"exercises/ode/optimised.csv\")\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\n",
    "axes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\n",
    "axes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\n",
    "axes.plot(df_optimised[\"t\"], df_optimised[\"u\"], \"-.^\", label=rf\"Optimised ($\\theta={controls[-1]:.4f}$)\")\n",
    "axes.legend()\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577d470-a294-415f-b9fd-f5a9fe6c8aa0",
   "metadata": {},
   "source": [
    "As we might hope, the optimised value of $\\theta$ gives a much better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Calculating the *full* Jacobian\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question</b>\n",
    "\n",
    "Given a map $\\mathbf{f}$, some input $\\mathbf{x}$, and some seed $\\dot{\\mathbf{x}}$, we have the Jacobian vector product\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "*How can we use this to compute the full Jacobian matrix $\\nabla\\mathbf{f}(\\mathbf{x})$?*\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "$$\\nabla F(x)=\\nabla F(x)I_n=\\nabla F(x)\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "Apply JVP to the $n$ canonical unit vectors.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35297e48-0c7d-4ac1-862f-af2005602d93",
   "metadata": {},
   "source": [
    "## Example: tridiagonal matrix\n",
    "\n",
    "Consider the classic central difference approximation to the second derivative (yes, more derivatives!):\n",
    "$$\n",
    "    \\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\approx\\frac{u(x-h)-2u(x)+u(x+h)}{h^2},\n",
    "$$\n",
    "for some uniform spacing $h>0$. For a uniform discretisation $\\{x_k\\}_{k=1}^n$ of an interval, this can be written as\n",
    "$$\n",
    "    \\left.\\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\right|_{x=x_k}\\approx\\frac{u_{k-1}-2u_k+u_{k+1}}{h^2},\n",
    "$$\n",
    "where $u_k$ is an approximation of $u(x_k)$. This is implemented in Fortran in `session1/exercises/sparse/central_diff.f90` for the case of a periodic 1D function.\n",
    "\n",
    "```fortran\n",
    "! Central difference approximation for the second derivative of a periodic 1D function\n",
    "subroutine central_diff(u, f, h, n)\n",
    "  implicit none\n",
    "\n",
    "  integer, intent(in) :: n\n",
    "  real, intent(in), dimension(n) :: u\n",
    "  real, intent(out), dimension(n) :: f\n",
    "  real, intent(in) :: h\n",
    "\n",
    "  integer :: i\n",
    "\n",
    "  if (size(u, 1) /= n) then\n",
    "    print *, \"Invalid input array size\"\n",
    "    stop 1\n",
    "  end if\n",
    "  if (size(f, 1) /= n) then\n",
    "    print *, \"Invalid output array size\"\n",
    "    stop 1\n",
    "  end if\n",
    "\n",
    "  do i = 1, n\n",
    "    if (i == 1) then\n",
    "      f(i) = (u(n) - 2 * u(i) + u(i+1)) / h ** 2\n",
    "    else if (i == n) then\n",
    "      f(i) = (u(i-1) - 2 * u(i) + u(1)) / h ** 2\n",
    "    else\n",
    "      f(i) = (u(i-1) - 2 * u(i) + u(i+1)) / h ** 2\n",
    "    end if\n",
    "  end do\n",
    "end subroutine central_diff\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Apply Tapenade to compute the derivative of the output array `f` with respect to the input array `u`.\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "    \n",
    "Running\n",
    "```sh\n",
    "$ tapenade -head \"central_diff(f)/(u)\" central_diff.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./central_diff_d.f90 \n",
    "@@ Created ./central_diff_d.msg\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of central_diff in forward (tangent) mode:\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: u\n",
    "!   RW status of diff variables: f:out u:in\n",
    "! Central difference approximation for the second derivative of a periodic 1D function\n",
    "SUBROUTINE CENTRAL_DIFF_D(u, ud, f, fd, h, n)\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: u\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: ud\n",
    "  REAL, DIMENSION(n), INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(n), INTENT(OUT) :: fd\n",
    "  REAL, INTENT(IN) :: h\n",
    "  INTEGER :: i\n",
    "  INTRINSIC SIZE\n",
    "  IF (SIZE(u, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid input array size'\n",
    "    STOP\n",
    "  ELSE IF (SIZE(f, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid output array size'\n",
    "    STOP\n",
    "  ELSE\n",
    "    fd = 0.0\n",
    "    DO i=1,n\n",
    "      IF (i .EQ. 1) THEN\n",
    "        fd(i) = (ud(n)-2*ud(i)+ud(i+1))/h**2\n",
    "        f(i) = (u(n)-2*u(i)+u(i+1))/h**2\n",
    "      ELSE IF (i .EQ. n) THEN\n",
    "        fd(i) = (ud(i-1)-2*ud(i)+ud(1))/h**2\n",
    "        f(i) = (u(i-1)-2*u(i)+u(1))/h**2\n",
    "      ELSE\n",
    "        fd(i) = (ud(i-1)-2*ud(i)+ud(i+1))/h**2\n",
    "        f(i) = (u(i-1)-2*u(i)+u(i+1))/h**2\n",
    "      END IF\n",
    "    END DO\n",
    "  END IF\n",
    "END SUBROUTINE CENTRAL_DIFF_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8480e3-08b8-4d3f-b421-afbd80db64e6",
   "metadata": {},
   "source": [
    "## Tridiagonal example: compute dense Jacobian\n",
    "\n",
    "In `session1/exercises/sparse/dense_jacobian.f90` we provide a program for computing the Jacobian of the centred difference function over the uniform partition of the interval.\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/sparse\n",
    "$ gfortran dense_jacobian.f90 -o dense_jacobian\n",
    "$ ./dense_jacobian\n",
    "```\n",
    "produces a file `dense_jacobian.dat`, which we can read and view in the notebook. Note that the computation here required $n$ applications of forward mode AD, where $n$ is the number of gridpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4437fb92-614a-4e83-a24b-1c25449d8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"exercises/sparse/dense_jacobian.dat\").readlines()\n",
    "print(np.array([[float(entry) for entry in line[:-2].split(\",\")] for line in lines]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Sparse AD\n",
    "\n",
    "We can compute the full Jacobian with\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\nabla\\mathbf{f}(\\mathbf{x})I_n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}e_1,e_2,\\dots,e_n\\end{bmatrix}.$$\n",
    "\n",
    "* But what about when $n$ gets very large?\n",
    "* And what about when the Jacobian is sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f6671-c78f-4442-be33-0a1c04b6786f",
   "metadata": {},
   "source": [
    "## Diagonal Jacobian example\n",
    "\n",
    "Suppose $\\nabla\\mathbf{f}(\\mathbf{x})$ is diagonal, say\n",
    "\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}.$$\n",
    "\n",
    "Then, for a seed vector $\\dot{\\mathbf{x}}=\\begin{bmatrix}\\dot{x}_1 & \\dot{x}_2 & \\dots & \\dot{x}_n\\end{bmatrix}^T$, we have\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{f}(\\mathbf{x})\\:\\dot{\\mathbf{x}}\n",
    "=\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}\n",
    "\\begin{bmatrix}\\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}f_1\\dot{x}_1 \\\\ f_2\\dot{x}_2 \\\\ \\vdots \\\\ f_n\\dot{x}_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Making the clever choice of seed vector $\\dot{\\mathbf{x}}:=\\mathbf{e}=\\begin{bmatrix}1 & 1 & \\dots & 1\\end{bmatrix}^T$, we can apply **a single JVP** and then back out the full Jacobian by putting each entry on the diagonal:\n",
    "$$\n",
    "    \\mathrm{diag}(\\nabla\\mathbf{f}(\\mathbf{x})\\:\\mathbf{e})\n",
    "    =\\mathrm{diag}\\left(\\begin{bmatrix}f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_n \\end{bmatrix}\\right)\n",
    "    =\\begin{bmatrix}f_1\\\\& f_2\\\\ & & \\ddots\\\\ & & & f_n\\end{bmatrix}\n",
    "    =\\nabla\\mathbf{f}(\\mathbf{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afeb89-f362-4672-a236-7d287e7e06a0",
   "metadata": {},
   "source": [
    "## Sparse AD: what colour is your Jacobian?\n",
    "\n",
    "The example above is the simplest case of what is known as *colouring*. The idea is to group columns of the Jacobian such that the columns in each group are linearly independent. This gives rise to compression approaches for sparse AD.\n",
    "\n",
    "<img src=\"images/colours.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div><strong>Figure 2:</strong> Jacobian colouring diagram taken from Gebremedhin, et al (2005).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b03dde-0fca-4244-a96a-ca421deb939d",
   "metadata": {},
   "source": [
    "## Tridiagonal example: compute compressed Jacobian\n",
    "\n",
    "In `session1/exercises/sparse/compressed_jacobian.f90` we provide a program for computing the same Jacobian of the centred difference function but this time using a clever choice of seed vectors:\n",
    "$$\n",
    "    \\begin{bmatrix}1\\\\0\\\\0\\\\1\\\\0\\\\0\\\\1\\\\0\\\\0\\\\1\\end{bmatrix},\n",
    "    \\quad\\begin{bmatrix}0\\\\1\\\\0\\\\0\\\\1\\\\0\\\\0\\\\1\\\\0\\\\0\\end{bmatrix},\n",
    "    \\quad\\begin{bmatrix}0\\\\0\\\\1\\\\0\\\\0\\\\1\\\\0\\\\0\\\\1\\\\0\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "```fortran\n",
    "include \"central_diff_d.f90\"\n",
    "\n",
    "! Program for computing a tridiagonal Jacobian using compression\n",
    "program compressed_jacobian\n",
    "  implicit none\n",
    "\n",
    "  integer, parameter :: n = 10\n",
    "  real, parameter :: h = 1.0\n",
    "\n",
    "  real, dimension(n) :: u, ud\n",
    "  real, dimension(n) :: f, fd\n",
    "  real, dimension(n,n) :: jacobian\n",
    "  integer :: i, j\n",
    "\n",
    "  ! Specify some arbitrary input\n",
    "  u(:) = 1.0\n",
    "\n",
    "  ! Set the seed vector to zero initially\n",
    "  ud(:) = 0.0\n",
    "\n",
    "  ! Loop over three offsets\n",
    "  do i = 1, 3\n",
    "\n",
    "    ! Apply the JVP to seed vectors based on the offsets\n",
    "    ud(i::3) = 1.0\n",
    "    call central_diff_d(u, ud, f, fd, h, n)\n",
    "    ud(i::3) = 0.0\n",
    "\n",
    "    ! Decompress rows and insert them into the Jacobian\n",
    "    j = i\n",
    "    jacobian(i::3,:) = 0.0\n",
    "    do while (j <= n)\n",
    "      if (j == 1) then\n",
    "        jacobian(j,n) = fd(n)\n",
    "      else\n",
    "        jacobian(j,j-1) = fd(j-1)\n",
    "      end if\n",
    "      jacobian(j,j) = fd(j)\n",
    "      if (j == n) then\n",
    "        jacobian(j,1) = fd(1)\n",
    "      else\n",
    "        jacobian(j,j+1) = fd(j+1)\n",
    "      end if\n",
    "      j = j + 3\n",
    "    end do\n",
    "  end do\n",
    "\n",
    "  ! Write out the result\n",
    "  open(unit=10, file=\"compressed_jacobian.dat\")\n",
    "  do i = 1, n\n",
    "    write(unit=10, fmt=100) jacobian(i,:)\n",
    "  end do\n",
    "  100 format(10(f4.1,\",\"))\n",
    "  close(unit=10)\n",
    "\n",
    "end program compressed_jacobian\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/sparse\n",
    "$ gfortran compressed_jacobian.f90 -o compressed_jacobian\n",
    "$ ./compressed_jacobian\n",
    "```\n",
    "produces a file `compressed_jacobian.dat`, which we can again read and view in the notebook. This time we only need to compute **three** JVPs, rather than $n$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7824e9-d8e8-40c7-a147-e238e5f15e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"exercises/sparse/compressed_jacobian.dat\").readlines()\n",
    "print(np.array([[float(entry) for entry in line[:-2].split(\",\")] for line in lines]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c4ef3a-6a5b-4cfd-a59b-1e6eed5aae6f",
   "metadata": {},
   "source": [
    "## Sparse AD: further details\n",
    "\n",
    "#### In practice\n",
    "\n",
    "It's worth noting that we used knowledge about the tridiagonal structure of the Jacobian to choose the seed vectors... so that we could compute the Jacobian. For Jacobians in the wild (e.g., those related to networks) we don't necessarily have such intuition. In practice, we need to perform an initial step to establish the sparsity pattern before we can do the colouring.\n",
    "\n",
    "#### Vector mode\n",
    "\n",
    "Above, we applied the JVP to three different seed vectors. Most AD tools support a 'vector' mode, which allows this to be done in a single call. This is the *vector-Jacobian product (VJP)*:\n",
    "\n",
    "$$\\text{VJP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{X}}):=\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_k\\end{bmatrix},$$\n",
    "\n",
    "for a *seed matrix* $\\dot{\\mathbf{X}}:=\\begin{bmatrix}\\dot{\\mathbf{x}}_1,\\dot{\\mathbf{x}}_2,\\dots,\\dot{\\mathbf{x}}_k\\end{bmatrix}\\in\\mathbb{R}^{k\\times n}$, $k\\in\\mathbb{N}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "1. Apply Tapenade to the `central_diff` subroutine.\n",
    "2. Rework `session1/exercises/sparse/compressed_jacobian.f90` to use vector mode.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "    \n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/sparse\n",
    "$ tapenade -head \"central_diff(f)/(u)\" -vector central_diff.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./central_diff_dv.f90 \n",
    "@@ Created ./central_diff_dv.msg\n",
    "$ cat central_diff_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of central_diff in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: u\n",
    "!   RW status of diff variables: f:out u:in\n",
    "! Central difference approximation for the second derivative of a periodic 1D function\n",
    "SUBROUTINE CENTRAL_DIFF_DV(u, ud, f, fd, h, n, nbdirs)\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: u\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: ud\n",
    "  REAL, DIMENSION(n), INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(OUT) :: fd\n",
    "  REAL, INTENT(IN) :: h\n",
    "  INTEGER :: i\n",
    "  INTRINSIC SIZE\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  IF (SIZE(u, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid input array size'\n",
    "    STOP\n",
    "  ELSE IF (SIZE(f, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid output array size'\n",
    "    STOP\n",
    "  ELSE\n",
    "    fd = 0.0\n",
    "    DO i=1,n\n",
    "      IF (i .EQ. 1) THEN\n",
    "        DO nd=1,nbdirs\n",
    "          fd(nd, i) = (ud(nd, n)-2*ud(nd, i)+ud(nd, i+1))/h**2\n",
    "        END DO\n",
    "        f(i) = (u(n)-2*u(i)+u(i+1))/h**2\n",
    "      ELSE IF (i .EQ. n) THEN\n",
    "        DO nd=1,nbdirs\n",
    "          fd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, 1))/h**2\n",
    "        END DO\n",
    "        f(i) = (u(i-1)-2*u(i)+u(1))/h**2\n",
    "      ELSE\n",
    "        DO nd=1,nbdirs\n",
    "          fd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, i+1))/h**2\n",
    "        END DO\n",
    "        f(i) = (u(i-1)-2*u(i)+u(i+1))/h**2\n",
    "      END IF\n",
    "    END DO\n",
    "  END IF\n",
    "END SUBROUTINE CENTRAL_DIFF_DV\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```fortran\n",
    "! Module containing the nbdirsmax variable required by central_diff_dv\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 3\n",
    "end module diffsizes\n",
    "\n",
    "! Include the vector forward mode derivative generated by Tapenade\n",
    "include \"central_diff_dv.f90\"\n",
    "\n",
    "! Program for computing a tridiagonal Jacobian using compression - vector version\n",
    "program compressed_jacobian_vector\n",
    "  implicit none\n",
    "\n",
    "  integer, parameter :: n = 10\n",
    "  real, parameter :: h = 1.0\n",
    "\n",
    "  real, dimension(n) :: u, f\n",
    "  real, dimension(3,n) :: udv, fdv\n",
    "  real, dimension(n,n) :: jacobian\n",
    "  integer :: i, j\n",
    "\n",
    "  ! Specify some arbitrary input\n",
    "  u(:) = 1.0\n",
    "\n",
    "  ! Set the seed matrix\n",
    "  udv(:,:) = 0.0\n",
    "  do i = 1, 3\n",
    "    udv(i,i::3) = 1.0\n",
    "  end do\n",
    "\n",
    "  ! Apply the VJP\n",
    "  call central_diff_dv(u, udv, f, fdv, h, n, 3)\n",
    "\n",
    "  ! Decompress rows and insert them into the Jacobian\n",
    "  do i = 1, 3\n",
    "    j = i\n",
    "    jacobian(i::3,:) = 0.0\n",
    "    do while (j <= n)\n",
    "      if (j == 1) then\n",
    "        jacobian(j,n) = fdv(i,n)\n",
    "      else\n",
    "        jacobian(j,j-1) = fdv(i,j-1)\n",
    "      end if\n",
    "      jacobian(j,j) = fdv(i,j)\n",
    "      if (j == n) then\n",
    "        jacobian(j,1) = fdv(i,1)\n",
    "      else\n",
    "        jacobian(j,j+1) = fdv(i,j+1)\n",
    "      end if\n",
    "      j = j + 3\n",
    "    end do\n",
    "  end do\n",
    "\n",
    "  ! Write out the result\n",
    "  open(unit=10, file=\"compressed_jacobian_vector.dat\")\n",
    "  do i = 1, n\n",
    "    write(unit=10, fmt=100) jacobian(i,:)\n",
    "  end do\n",
    "  100 format(10(f4.1,\",\"))\n",
    "  close(unit=10)\n",
    "\n",
    "end program compressed_jacobian_vector\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e383c-201b-4867-af13-ecbb3f4349e3",
   "metadata": {},
   "source": [
    "## Sparse AD: overall performance comparison\n",
    "\n",
    "We have considered three approaches for sparse AD:\n",
    "1. 'Dense approach', which computes a JVP for each canonical unit vector.\n",
    "2. 'Compressed approach', which computes JVPs for sums over sets of linearly independent columns.\n",
    "3. 'Matrix-free approach', which applies a single JVP. (Only useful if the problem involves JVPs.)\n",
    "\n",
    "In the following, we walk through a performance comparison of these different approaches applied to the solution of the nonlinear Gray-Scott equation\n",
    "$$\n",
    "    \\frac{\\partial u}{\\partial t}=D_1\\Delta u-uv^2+\\gamma(1-u),\n",
    "    \\quad\\frac{\\partial v}{\\partial t}=D_2\\Delta v-uv^2=(\\gamma+\\kappa)v,\n",
    "    \\quad u,v:[0,2.5]^2\\rightarrow\\mathbb{R}\n",
    "$$\n",
    "(subject to some initial conditions) using a finite difference approach with Crank-Nicolson timestepping ($\\theta=0.5$). This discretisation corresponds to a 5-point stencil and it turns out the Jacobian can be coloured with five colours. As such, the compressed approach involves five JVPs per Jacobian computation.\n",
    "\n",
    "<img src=\"images/runtimes.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div><strong>Figure 3:</strong> Performance comparison for different Jacobian computation approaches for solving the Gray-Scott equation, taken from Wallwork et al. (2019). PETSc was used for numerical solvers and ADOL-C for AD. The $N\\times N$ notation in the subcaptions refers to the grid dimension.</div>\n",
    "\n",
    "<!-- Run on a Knights Landing node at Argonne National Laboratory -->\n",
    "\n",
    "<br>\n",
    "\n",
    "Here 'analytic' refers to the hand-derived Jacobian, which we can't realistically expect to beat.\n",
    "\n",
    "<!-- Notes\n",
    "\n",
    "* Compressed approach exhibits strong scalability.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f922e8-ab13-4396-9abd-5005db6d45ba",
   "metadata": {},
   "source": [
    "## Sparse AD: convergence comparison\n",
    "\n",
    "Matrix-free works well for small dimensions. However, it slows down at higher resolution because Jacobian assembly is done at each iteration of the *nonlinear* solver, whereas the JVP is calculated at each iteration of the *linear* solver. It's possible that a different choice of linear solver or preconditioner would reduce the cost of the matrix-free approach.\n",
    "\n",
    "<img src=\"images/linear_solver_table.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div><strong>Figure 4:</strong> Linear solver iterations as a function of grid resolution and number of processors for the Gray-Scott problem. Taken from Wallwork et al. (2019).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2cfa1-89a8-4d44-ab4f-32d867ac0c17",
   "metadata": {},
   "source": [
    "## Sparse AD: performance comparison between steps\n",
    "\n",
    "As it turns out, the cost of the sparsity detection and colouring are negligible for this particular setup. The cost is dominated by the decompression (recovery), with the propagation of seed vectors the next-most expensive step.\n",
    "\n",
    "<img src=\"images/sparse_ad_proportions.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div><strong>Figure 5:</strong> Comparative runtimes of components of the \"compressed\" sparse AD approach for the Gray-Scott problem. Taken from Wallwork et al. (2019).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6add3be-640f-41a1-a7e2-393c4229fd02",
   "metadata": {},
   "source": [
    "## Summary and outlook\n",
    "\n",
    "In today's session we:\n",
    "* Got a brief history of automatic differentiation.\n",
    "* Learnt about *forward mode* and the *source transformation* approach.\n",
    "* Tried out the *Tapenade* source transformation AD tool applied to some test problems.\n",
    "* Verified the code generated by Tapenade both manually and using the *Taylor test*.\n",
    "* Learnt about compressed and matrix-free approaches for sparse problems.\n",
    "* Considered how different levels of abstraction can be appropriate for different problems.\n",
    "\n",
    "In tomorrow's session (at the same time) we will:\n",
    "* Learn about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verify the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Try out the *autograd* operator overloading AD tool underpinnning PyTorch (as well as libtorch (C++) and FTorch (Fortran)).\n",
    "* Learn about *checkpointing* and using AD to compute higher order derivatives.\n",
    "* See a showcase of some more advanced use cases of AD using Firedrake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* R. E. Wengert. *A simple automatic derivative evaluation program*. Communications\n",
    "of the ACM, 7(8):463464, 1964.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:3554, 1992.\n",
    "* D. Cortild, et al. *A Brief Review of Automatic Differentiation.* (2023).\n",
    "* A. H. Gebremedhin, et al. *What color is your Jacobian? Graph coloring for computing derivatives* (2005). SIAM review, 47(4), pp.629-705.\n",
    "* J. G. Wallwork, et al. *Computing derivatives for petsc adjoint solvers using algorithmic differentiation* (2019), arXiv preprint arXiv:1909.02836."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163bb368-ddc4-4326-81f9-84ffd857d419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
