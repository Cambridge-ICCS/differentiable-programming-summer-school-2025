{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpltools import annotation\n",
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b1bdd-e79f-4315-b94b-eb10f900ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(filename):\n",
    "    \"\"\"Function for printing a matrix saved to a text file from Fortran.\"\"\"\n",
    "    print(np.array([[float(entry) for entry in line[:-2].split(\",\")] for line in open(filename).readlines()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Session 1: Forward mode differentiation and source transformation\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Set up codespace now. (It will take a while!)</b>\n",
    "\n",
    "We will show you where to find this notebook in the repo while you wait.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Differentiable programming is an enabling technology. Given scientific code for computing some quantity, it allows us to generate derivatives of quantities involved in the computation without any need for deriving or hand-coding the derivative expressions involved.\n",
    "\n",
    "Some motivating examples:\n",
    "* Computing the **Jacobian** for a nonlinear system.\n",
    "* Computing the **gradient** required for an ODE- or PDE-constrained optimisation method.\n",
    "* The **backpropagation** operation used for training machine learning models.\n",
    "* Computing **Hessians** for uncertainty quantification methods.\n",
    "* Solving the **adjoint** problems involved in data assimilation methods commonly used for weather forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In today's session we will:\n",
    "\n",
    "* Get a brief history of automatic differentiation.\n",
    "* Learn about *forward mode* and the *source transformation* approach.\n",
    "* Try out the *Tapenade* source transformation AD tool applied to some test problems.\n",
    "* Verify the code generated by Tapenade both manually and using the *Taylor test*.\n",
    "* Learn about compressed and matrix-free approaches for sparse problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "<b>Caution</b> with the physics notation for derivatives $\\dot{x}$. It won't always mean what you expect! (See later.)\n",
    "</div>\n",
    "\n",
    "Similarly, for $m\\in\\mathbb{N}$ dimensional, differentiable, vector-valued function $\\mathbf{f}:A\\rightarrow\\mathbb{R}^m$ with scalar input, we have derivative notations $\\mathbf{f}'(x)$ and $\\frac{\\mathrm{d}\\mathbf{f}}{\\mathrm{d}x}$.\n",
    "\n",
    "For a differentiable function with vector input (i.e., multiple inputs), we use partial derivative notation. For example, if $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ is written as $f=f(x,y)$ then we have the partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ with respect to first and second components, respectively. We use\n",
    "$$\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1},\\dots,\\frac{\\partial f}{\\partial x_m}\\right)$$\n",
    "to denote the vector of all such partial derivatives. Similarly for vector-valued functions with multiple inputs.\n",
    "\n",
    "When it comes to derivatives in code, we use the `_d` notation (for \"derivative\" or \"dot\"), which is standard in the AD literature. Its meaning will be described in due course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Origins of AD in 1950s.\n",
    "* However, it found a wider audience in the 1980s, when it became more relevant thanks to advances in both computer power and modern programming languages.\n",
    "* Forward mode (the subject of this session) was discovered by Wengert in 1964.\n",
    "* Further developed by Griewank in the late 1980s.\n",
    "\n",
    "<img src=\"images/Wengert.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 1:</strong> Header of R. E. Wengert, <em>A simple automatic derivative evaluation program</em> (1964).\n",
    "Communications of the ACM 7(8):463â€“4.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "The idea of AD is to **treat a model as a sequence of elementary instructions** (e.g., addition, multiplication, exponentiation). Here a *model* could be a function or subroutine, code block, or a whole program. Elementary operations are well-understood and their derivatives are known. As such, the derivative of the whole model may be computed by composing the derivatives of each operation using the *chain rule*.\n",
    "\n",
    "#### Recap on A-level maths: the Chain Rule\n",
    "\n",
    "Consider two composable, differentiable (mathematical) functions, $f$ and $g$, with composition $h=f\\circ g$. By definition, this means\n",
    "$$h(x)=(f\\circ g)(x)=g(f(x)).$$\n",
    "\n",
    "Then the *chain rule* states that the derivative of $h$ may be computed in terms of the derivatives of $f$ and $g$ using the formula\n",
    "$$h'(x)=(f\\circ g)'(x)=(f\\circ g')(x)\\,f'(x)=g'(f(x))\\,f'(x).$$\n",
    "\n",
    "Equivalently, in Leibniz notation:\n",
    "$$\\frac{\\mathrm{d}h}{\\mathrm{d}x}=\\frac{\\mathrm{d}g}{\\mathrm{d}f}\\frac{\\mathrm{d}f}{\\mathrm{d}x}.$$\n",
    "\n",
    "For variables with multiple arguments, the result is equivalent for each partial derivative, e.g.,\n",
    "$$\\frac{\\partial h}{\\partial x}=\\frac{\\partial g}{\\partial f}\\frac{\\partial f}{\\partial x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example\n",
    "\n",
    "Consider two functions acting on real numbers:\n",
    "$$f(x,y)=xy$$\n",
    "and\n",
    "$$g(z)=(\\sin(z),\\cos(z)).$$\n",
    "Here $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ takes two inputs and returns a single output, while $g:\\mathbb{R}\\rightarrow\\mathbb{R}^2$ takes a single input and returns two outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "    \n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that it is well defined for these functions may be composed in either order. (Although they won't necessarily give the same value!)\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "        \n",
    "The image of $f$ is the set of all real numbers, so its image is the same as the domain of $g$ (i.e., $\\text{im}(f)=\\mathbb{R}=\\text{dom}(g)$).\n",
    "\n",
    "The image of $g$ is $[-1,1]^2=[-1,1]\\times[-1,1]$ because $\\sin$ and $\\cos$ give values between -1 and 1. Since this is a subset of $\\mathbb{R}^2$, the image of $g$ is a subset of the domain of $f$ (i.e., $\\text{im}(g)\\subset\\text{dom}(f)$).\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Directed Acyclic Graph\n",
    "\n",
    "We can visualise the functions in terms of DAGs.\n",
    "\n",
    "Recalling that\n",
    "$$f(x_1,x_2)=x_1x_2$$\n",
    "and\n",
    "$$g(y)=(\\sin(y),\\cos(y)),$$\n",
    "we have\n",
    "\n",
    "<img src=\"images/f_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 2:</strong> Directed Acyclic Graph (DAG) for the $f$ function in the $f$ & $g$ example.</div>\n",
    "\n",
    "<img src=\"images/g_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 3:</strong> Directed Acyclic Graph (DAG) for the $g$ function in the $f$ & $g$ example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db5aa3-b635-46b1-b875-b53f20dd4509",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: composition\n",
    "\n",
    "Consider the composition $h=f\\circ g:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$, which is given by\n",
    "$$h(x_1,x_2)=(f\\circ g)(x_1,x_2)=g(f(x_1,x_2))=g(x_1x_2)=(\\sin(x_1x_2),\\cos(x_1x_2)).$$\n",
    "\n",
    "For the derivative of each component,\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_1}=\\frac{\\partial}{\\partial x_1}x_1x_2=x_2,\n",
    "\\quad\\frac{\\partial f}{\\partial x_2}=\\frac{\\partial}{\\partial x_2}x_1x_2=x_1,\n",
    "\\quad\\frac{\\partial g}{\\partial y}=\\frac{\\partial}{\\partial y}(\\sin(y),\\cos(y))=(\\cos(y),-\\sin(y)).\n",
    "$$\n",
    "\n",
    "Introduce the notation $g(y)=(g_1(y),g_2(y))$ so that\n",
    "$$\n",
    "\\frac{\\partial g_1}{\\partial y}=\\cos(y),\n",
    "\\quad\\frac{\\partial g_2}{\\partial y}=-\\sin(y).\n",
    "$$\n",
    "Similarly $h(x_1,x_2)=(h_1(x_1,x_2),h_2(x_1,x_2))=(g_1(f(x_1,x_2)),g_2(f(x_1,x_2)))$.\n",
    "\n",
    "Let's use the chain rule to work out the derivatives of each of the outputs with respect to each of the inputs.\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial x_1}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x_1}=\\cos(y)x_2=x_2\\cos(x_1x_2),\n",
    "\\quad\\frac{\\partial h_1}{\\partial x_2}=\\frac{\\partial g_1}{\\partial f}\\frac{\\partial f}{\\partial x_2}=\\cos(y)x_1=x_1\\cos(x_1x_2),\n",
    "$$\n",
    "where $y=f(x_1,x_2)$ and\n",
    "$$\n",
    "\\quad\\frac{\\partial h_2}{\\partial x_1}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x_1}=-\\sin(y)x_2=-x_2\\sin(x_1x_2),\n",
    "\\quad\\frac{\\partial h_2}{\\partial x_2}=\\frac{\\partial g_2}{\\partial f}\\frac{\\partial f}{\\partial x_2}=-\\sin(y)x_1=-x_1\\sin(x_1x_2).\n",
    "$$\n",
    "\n",
    "We will come back to these formulae to verify the correctness of our AD computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    "\n",
    "Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "You will also hear the related term *tangent linear model (TLM)*.\n",
    "\n",
    "Think of the seed vector as the direction in which we want to compute the derivative.\n",
    "In practice, the seed vector is often a derivative of some upstream code from outside of the part of the program being differentiated.\n",
    "That is, the upstream code is *passive*, whereas the part we are interested in is *active*, as far as AD is concerned.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need to assemble the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74e68c-b6dd-4258-8c3d-c4d41cc8888c",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Seed vectors\n",
    "\n",
    "Let's revisit the DAG interpretation and consider how the derivatives work.\n",
    "\n",
    "<img src=\"images/forward_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 4:</strong> Directed Acyclic Graph (DAG) for the composition of the functions in the $f$ & $g$ example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Approach 1: Source transformation\n",
    "\n",
    "High level idea: Given some (code) function `f(x)` and a seed vector `x_d`, generate the code for the function `f_d(x, x_d)` for its (directional) derivative.\n",
    "\n",
    "#### Notes on the source transformation approach:\n",
    "\n",
    "* Static analysis, done ahead of time.\n",
    "* Often the difficult part is hooking the differentiated code into the wider model/build system.\n",
    "* Limited number of tools.\n",
    "\n",
    "#### Source transformation tools:\n",
    "\n",
    "* [Tapenade](https://tapenade.gitlabpages.inria.fr/userdoc/build/html/index.html) (C, Fortran, Julia*)\n",
    "* [OpenAD](https://www.mcs.anl.gov/OpenAD) (Fortran)\n",
    "* [TAF](http://fastopt.com/products/taf) (Fortran) [commerical]\n",
    "* [PSyAD](https://psyclone-adjoint.readthedocs.io/en/stable)* (domain-specific)\n",
    "\n",
    "*\\*Work in progress*\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Disclaimer</b> We will be using some rather out-dated Fortran practices (e.g., `include`-ing Fortran source), for simplicity of working with Tapenade.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: source transformation\n",
    "\n",
    "Below we have the Fortran code for the example functions above, written as subroutines. You can find this in the repository at `session1/exercises/fg/f.f90` and `session1/exercises/fg/g.f90`, respectively.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y)\n",
    "  implicit none\n",
    "  real, dimension(2), intent(in)  :: x\n",
    "  real, intent(out) :: y\n",
    "  y = x(1) * x(2)\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(y, z)\n",
    "  implicit none\n",
    "  real, intent(in) :: y\n",
    "  real, intent(out), dimension(2) :: z\n",
    "  z = [sin(y), cos(y)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Run `tapenade -h` to see all the options.\n",
    "2. Apply Tapenade to each of these subroutines using the `-head` argument.* Inspect the output files `f_d.f90` and `g_d.f90` and check they are as you expect.\n",
    "3. (Optional) Inspect the message files `f_d.msg` and `g_d.msg`.\n",
    "\n",
    "*The syntax is `-head procedure_name(dependent_variables)\\(independent_variables)`, where here `procedure_name`, `dependent_variables`, and `independent_variables` all need substituting as appropriate. If multiple variables are used then they should be comma-separated.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session1/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade -head \"f(y)/(x)\" f.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./f_d.f90 \n",
    "@@ Created ./f_d.msg\n",
    "$ cat f_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in forward (tangent) mode:\n",
    "!   variations   of useful results: y\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: x:in y:out\n",
    "SUBROUTINE F_D(x, xd, y, yd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, DIMENSION(2), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(2), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: y\n",
    "  REAL, INTENT(OUT) :: yd\n",
    "  yd = x(2)*xd(1) + x(1)*xd(2)\n",
    "  y = x(1)*x(2)\n",
    "END SUBROUTINE F_D\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade -head \"g(z)/(y)\" g.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./g_d.f90 \n",
    "@@ Created ./g_d.msg\n",
    "$ cat g_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in forward (tangent) mode:\n",
    "!   variations   of useful results: z\n",
    "!   with respect to varying inputs: y\n",
    "!   RW status of diff variables: y:in z:out\n",
    "SUBROUTINE G_D(y, yd, z, zd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: y\n",
    "  REAL, INTENT(IN) :: yd\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: z\n",
    "  REAL, DIMENSION(2), INTENT(OUT) :: zd\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  zd = (/COS(y)*yd, -(SIN(y)*yd)/)\n",
    "  z = (/SIN(y), COS(y)/)\n",
    "END SUBROUTINE G_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Verification: the Taylor test\n",
    "\n",
    "Recall the Taylor expansion of a differentiable function $f:A\\rightarrow\\mathbb{R}$, for $A\\subseteq\\mathbb{R}$:\n",
    "$$\n",
    "    f(x+\\epsilon)=f(x)+\\epsilon f'(x) + \\frac{1}{2!}\\epsilon^2f''(x) + \\dots + \\frac{1}{n!}\\epsilon^nf^{(n)}(x) + \\dots,\n",
    "$$\n",
    "for some $\\epsilon\\in\\mathbb{R}$, i.e.,\n",
    "$$\n",
    "    f(x+\\epsilon)=f(x)+\\epsilon f'(x) + \\mathcal{O}(\\epsilon^2).\n",
    "$$\n",
    "This gives rise to the first-order forward difference approximation of the first derivative as follows:\n",
    "$$\n",
    "    f'(x) \\approx \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}.\n",
    "$$\n",
    "\n",
    "This is a rather crude approximation to a derivative, but it can do the job, given an appropriate choice of $\\epsilon$.\n",
    "\n",
    "The idea of the Taylor test is to take smaller and smaller spacing values $\\epsilon$ from a given input value and to check that the difference between the forward difference and the AD-generated result converges quadratically. That is, we verify that\n",
    "$$\n",
    "    \\|f(x+\\epsilon)-f(x)-\\epsilon\\:\\texttt{f\\_d}(x)\\|=\\mathcal{O}(\\epsilon^2),\n",
    "$$\n",
    "where $\\texttt{f\\_d}$ is the AD-generated result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Taylor test\n",
    "\n",
    "We return to the previous example and double-check that we are happy with the output of the AD tool.\n",
    "\n",
    "For simplicity of demonstration, we translate `f`, `g`, `f_d`, and `g_d` into Python and conduct the Taylor test here.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Could we use f2py here?</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Python translation of the `f` subroutine from `session1/exercises/fg/f.f90` using NumPy.\"\"\"\n",
    "    return x[0] * x[1]\n",
    "\n",
    "def g(y):\n",
    "    \"\"\"Python translation of the `g` subroutine from `session1/exercises/fg/g.f90` using NumPy.\"\"\"\n",
    "    return np.array([np.sin(y), np.cos(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_d(x, xd):\n",
    "    \"\"\"Python translation of the `f_d` subroutine generated by Tapenade in `session1/exercises/fg/f_d.f90` using NumPy.\"\"\"\n",
    "    yd = x[1] * xd[0] + x[0] * xd[1]\n",
    "    y = x[0] * x[1]\n",
    "    return y, yd\n",
    "\n",
    "def g_d(y, yd):\n",
    "    \"\"\"Python translation of the `g_d` subroutine generated by Tapenade in `session1/exercises/fg/g_d.f90` using NumPy.\"\"\"\n",
    "    zd = np.array([np.cos(y) * yd, -np.sin(y) * yd])\n",
    "    z = np.array([np.sin(y), np.cos(y)])\n",
    "    return z, zd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose arbitrary inputs for the composition\n",
    "x = np.array([1.0, 1.0])\n",
    "\n",
    "# Choose seed vector\n",
    "xd = np.array([1.0, 0.0])\n",
    "\n",
    "# Compute the derivative using forward mode AD\n",
    "y, yd = f_d(x, xd)\n",
    "_, derivative_ad = g_d(y, yd)\n",
    "\n",
    "# Run the Taylor test over several spacing values\n",
    "spacings = [1.0, 0.1, 0.01, 0.001]\n",
    "errors = []\n",
    "for spacing in spacings:\n",
    "\n",
    "    # Compute the perturbation in the seed vector direction\n",
    "    epsilon = spacing * xd\n",
    "\n",
    "    # Compute the discrepancy\n",
    "    errors.append(np.linalg.norm(g(f(x + epsilon)) - g(f(x)) - spacing * derivative_ad))\n",
    "\n",
    "# Plot the solution, demonstrating that the expected quadratic convergence is achieved\n",
    "fig, axes = plt.subplots()\n",
    "axes.loglog(spacings, errors, \"--x\")\n",
    "axes.set_xlabel(r\"$\\epsilon$ spacing\")\n",
    "axes.set_ylabel(r\"$\\ell_2$ error\")\n",
    "annotation.slope_marker((1e-2, 1e-4), 2, ax=axes, invert=True)\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Above, we computed the derivative of $h=f\\circ g$ with respect to the first component, $x_1$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Optional exercise</b>\n",
    "    \n",
    "Perform the Taylor test to double-check you are happy with the AD-generated derivative with respect to the second component, $x_2$, too.\n",
    "\n",
    "<em>Hint:</em> You only need to change one line.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Example: ODE-constrained optimisation\n",
    "\n",
    "Consider the scalar ordinary differential equation (ODE)\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=f(u),\\quad u(0)=u_0,\n",
    "$$\n",
    "where $t\\in[0,T]$ is the time variable, $T>0$ is the end time, and $u_0\\in\\mathbb{R}$ is the initial condition. Given some $f:A\\rightarrow\\mathbb{R}$ with $A\\subseteq\\mathbb{R}$, we seek to solve the ODE for $u:[0,T]\\rightarrow\\mathbb{R}$.\n",
    "\n",
    "For simplicity, let's consider the ODE\n",
    "$$\n",
    "    \\frac{\\mathrm{d}u}{\\mathrm{d}t}=u,\\quad u(0)=1,\n",
    "$$\n",
    "i.e., $f(u)=u$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Optional exercise</b>\n",
    "    \n",
    "Convince yourself that the analytical solution of the ODE is $u(t)=\\mathrm{e}^t$.\n",
    "\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Plugging $u(t)=\\mathrm{e}^t$ into the LHS gives $\\frac{\\mathrm{d}u}{\\mathrm{d}t}=\\mathrm{e}^t=u$, which satisfies the ODE. Checking the initial condition, we have $u(0)=\\mathrm{e}^0=1$, which also satisfies.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## ODE example: forward vs backward Euler\n",
    "\n",
    "You've probably aware of the simplest example of an explicit timestepping method to approximate the solution of the ODE is *forward Euler* (a.k.a. explicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_{k-1}),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "You're probably also aware of the simplest example of an implicit timestepping method to approximate the solution of the ODE is *backward Euler* (a.k.a. implicit Euler):\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=f(u_k),\n",
    "$$\n",
    "for $k\\in\\mathbb{N}$ and some timestep $\\Delta t>0$.\n",
    "\n",
    "These are special cases of a *theta-method*,\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)f(u_{k-1})+\\theta f(u_k),\n",
    "$$\n",
    "where $\\theta\\in[0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f9aab-0fd2-40c9-8c22-2b73beb5f6c4",
   "metadata": {},
   "source": [
    "## ODE example: applying forward and backward Euler\n",
    "\n",
    "For our problem, we have\n",
    "$$\n",
    "    \\frac{u_{k}-u_{k-1}}{\\Delta t}=(1-\\theta)u_{k-1}+\\theta u_k,\n",
    "$$\n",
    "which can be rearranged to give\n",
    "$$\n",
    "    u_{k}=\\frac{1+\\Delta t(1-\\theta)}{1-\\Delta t\\theta}u_{k-1}.\n",
    "$$\n",
    "\n",
    "The forward and backward Euler approaches have been implemented for the case $f(u)=u$ in the `session1/exercises/ode` subdirectory.\n",
    "```sh\n",
    "$ cd exercises/ode\n",
    "$ ls\n",
    "backward_euler.f90  forward_euler.f90     Makefile\n",
    "cost_function.f90   gradient_descent.f90  theta.f90\n",
    "```\n",
    "where `theta.f90` contains several subroutines, including\n",
    "```fortran\n",
    "! Single iteration of a theta method for solving the ODE initial value problem\n",
    "!   du/dt = u, u(0) = 1\n",
    "subroutine theta_step(u, u_, dt, theta)\n",
    "  implicit none\n",
    "  real, intent(out) :: u    ! Solution at current timestep\n",
    "  real, intent(in) :: u_    ! Solution at previous timestep\n",
    "  real, intent(in) :: dt    ! Timestep length\n",
    "  real, intent(in) :: theta ! Theta parameter\n",
    "\n",
    "  u = u_ * (1 + dt * (1 - theta)) / (1 - dt * theta)\n",
    "end subroutine theta_step\n",
    "```\n",
    "\n",
    "Here `forward_euler.f90` and `backward_euler.f90` contain programs to solve the ODE, making use of common code from `theta.f90`, but with values $\\theta=0$ and $\\theta=1$, respectively. In the former case, we have\n",
    "```fortran\n",
    "include \"theta.f90\"\n",
    "\n",
    "! Program for running the forward Euler method to solve the ODE initial value problem\n",
    "!   du/dt = u, u(0) = 1\n",
    "program forward_euler\n",
    "  implicit none\n",
    "\n",
    "  real, parameter :: theta = 0.0                       ! Forward Euler corresponds to theta = 0.0\n",
    "  real :: u                                            ! Solution variable\n",
    "  char(len=100), parameter :: filename = \"forward.csv\" ! Filename to save results to\n",
    "\n",
    "  call theta_method(theta, u, filename)\n",
    "end program forward_euler\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran forward_euler.f90 -o forward_euler\n",
    "$ gfortran backward_euler.f90 -o backward_euler\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./forward_euler\n",
    "$ ./backward_euler\n",
    "$ ls\n",
    "backward.csv    backward_euler.f90  forward.csv    forward_euler.f90     Makefile\n",
    "backward_euler  cost_function.f90   forward_euler  gradient_descent.f90  theta.f90\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forward = pandas.read_csv(\"exercises/ode/forward.csv\")\n",
    "df_backward = pandas.read_csv(\"exercises/ode/backward.csv\")\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "times = np.linspace(0, 1, 101)\n",
    "\n",
    "axes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\n",
    "axes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\n",
    "axes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\n",
    "axes.legend()\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## ODE example: source transformation\n",
    "\n",
    "As we see from the plot above, the forward Euler method tends to underestimate the solution, whereas the backward Euler method tends to overestimate it. Let's try to optimise the value of $\\theta$ to best match the solution using a gradient-based optimisation method. To do that, we first need the gradient. **AD enables us to do this automatically.**\n",
    "\n",
    "The optimisation problem we seek to solve is to minimise some error measure $J$ for the approximation of $u$ by varying $\\theta$. That is,\n",
    "$$\n",
    "    \\min_{\\theta\\in[0,1]}J(u;\\theta).\n",
    "$$\n",
    "where the notation $J(u;\\theta)$ refers to the implicit dependence of the solution approximation $u$ on $\\theta$.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Note</b> Forward and backward Euler are first-order accurate methods. By optimising the $\\theta$ parameter, we can arrive at a second-order accuate method.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "    \n",
    "<b>Exercise</b>\n",
    "    \n",
    "Navigate to `session1/exercises/ode` and apply forward mode AD to the `theta_method` subroutine in `theta.f90` with Tapenade, specifying the `theta` argument as independent and the output `u` as dependent.\n",
    "\n",
    "<b>Solutions</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -head \"theta_method(theta)\\(u)\" theta.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./theta_d.f90 \n",
    "@@ Created ./theta_d.msg\n",
    "$ cat theta_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of theta_method in forward (tangent) mode:\n",
    "!   variations   of useful results: u\n",
    "!   with respect to varying inputs: theta\n",
    "!   RW status of diff variables: u:out theta:in\n",
    "SUBROUTINE THETA_METHOD_D(theta, thetad, filename, u, ud)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: theta\n",
    "  REAL, INTENT(IN) :: thetad\n",
    "  CHARACTER(len=*), INTENT(IN) :: filename\n",
    "  REAL, INTENT(OUT) :: u\n",
    "  REAL, INTENT(OUT) :: ud\n",
    "  REAL, PARAMETER :: dt=0.1\n",
    "  REAL, PARAMETER :: end_time=1.0\n",
    "  REAL :: t\n",
    "  REAL :: u_\n",
    "  REAL :: u_d\n",
    "! Create a CSV file for output\n",
    "  OPEN(unit=10, file=filename) \n",
    "  WRITE(unit=10, fmt='(''t,u'')') \n",
    "! Initialisation\n",
    "  t = 0.0\n",
    "  CALL INITIAL_CONDITION(u_)\n",
    "  WRITE(unit=10, fmt=100) t, u_\n",
    "  ud = 0.0\n",
    "  u_d = 0.0\n",
    "! Timestepping loop\n",
    "  DO WHILE (t .LT. end_time - 1e-05)\n",
    "    CALL THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n",
    "    u_d = ud\n",
    "    u_ = u\n",
    "    t = t + dt\n",
    "    WRITE(unit=10, fmt=100) t, u\n",
    "  END DO\n",
    "  CLOSE(unit=10) \n",
    " 100 FORMAT(f4.2,',',f4.2)\n",
    "END SUBROUTINE THETA_METHOD_D\n",
    "\n",
    "!  Differentiation of theta_step in forward (tangent) mode:\n",
    "!   variations   of useful results: u\n",
    "!   with respect to varying inputs: u_ theta\n",
    "SUBROUTINE THETA_STEP_D(u, ud, u_, u_d, dt, theta, thetad)\n",
    "  IMPLICIT NONE\n",
    "! solution at current timestep\n",
    "  REAL, INTENT(OUT) :: u\n",
    "  REAL, INTENT(OUT) :: ud\n",
    "! solution at previous timestep\n",
    "  REAL, INTENT(IN) :: u_\n",
    "  REAL, INTENT(IN) :: u_d\n",
    "! Timestep length\n",
    "  REAL, INTENT(IN) :: dt\n",
    "! Theta parameter\n",
    "  REAL, INTENT(IN) :: theta\n",
    "  REAL, INTENT(IN) :: thetad\n",
    "  REAL :: temp\n",
    "  temp = u_/(-(dt*theta)+1)\n",
    "  ud = (dt*(1-theta)+1)*(u_d+temp*dt*thetad)/(1-dt*theta) - temp*dt*&\n",
    "&   thetad\n",
    "  u = (dt*(1-theta)+1)*temp\n",
    "END SUBROUTINE THETA_STEP_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## ODE example: optimisation with gradient descent\n",
    "\n",
    "Let's solve this ODE problem with one of the simplest gradient-based optimisation approaches: gradient descent. This amounts to an initial guess $\\theta_0$, followed by iterative updates\n",
    "$$\n",
    "    \\theta_{k+1}=\\theta_k+\\alpha\\:p_k,\n",
    "$$\n",
    "where $\\alpha>0$ is the step length and $p_k$ is the descent direction. For gradient descent, we simply take\n",
    "$$\n",
    "    p_k=-\\frac{\\mathrm{d}J_k}{\\mathrm{d}\\theta_k}.\n",
    "$$\n",
    "\n",
    "Since we know the analytical solution for this problem, we may make an 'artifical' choice of cost function such as\n",
    "$$\n",
    "    J(u;\\theta)=(u(1)-\\mathrm{e}^1)^2,\n",
    "$$\n",
    "where here $\\mathrm{e}^1$ is the analytical solution at the end time $t=1$. This is implemented in `cost_function.f90` as\n",
    "```fortran\n",
    "! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\n",
    "subroutine cost_function(u, J)\n",
    "  implicit none\n",
    "  real, intent(in) :: u           ! Numerical solution at the end time\n",
    "  real, intent(out) :: J          ! Cost function value\n",
    "  real, parameter :: e = exp(1.0) ! The exponential constant, e=2.71828...\n",
    "\n",
    "  J = (u - e) ** 2\n",
    "end subroutine cost_function\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "    \n",
    "<b>Exercise</b>\n",
    "    \n",
    "The gradient descent algorithm is implemented for our ODE problem in `session1/exercises/ode/gradient_descent.f90`.\n",
    "\n",
    "1. However, there is another missing piece: you will also need to differentiate the cost function. Convince yourself you are satisfied with the output.\n",
    "2. Run the gradient descent optimisation algorithm and execute the following two cells to visualise the results.\n",
    "3. Play around with the optimisation parameters defined in `gradient_descent.f90`. Can you achieve convergence in fewer iterations without changing the `gtol` convergence tolerance?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade cost_function.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine cost_function as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./cost_function_d.f90 \n",
    "@@ Created ./cost_function_d.msg\n",
    "cat cost_function_d.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of cost_function in forward (tangent) mode:\n",
    "!   variations   of useful results: j\n",
    "!   with respect to varying inputs: u\n",
    "!   RW status of diff variables: j:out u:in\n",
    "! Cost function evaluating the l2 error at the end time against the analytical solution u(t)=exp(t)\n",
    "SUBROUTINE COST_FUNCTION_D(u, ud, j, jd)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: u\n",
    "  REAL, INTENT(IN) :: ud\n",
    "  REAL, INTENT(OUT) :: j\n",
    "  REAL, INTENT(OUT) :: jd\n",
    "  INTRINSIC EXP\n",
    "  REAL, PARAMETER :: e=EXP(1.0)\n",
    "  jd = 2*(u-e)*ud\n",
    "  j = (u-e)**2\n",
    "END SUBROUTINE COST_FUNCTION_D\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran gradient_descent.f90 -o gradient_descent\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./gradient_descent\n",
    "```\n",
    "You should get convergence in around 755 iterations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 3</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Increasing the step length slighly to $\\alpha=1.99$ you should get convergence in around 378 iterations - about half. Interestingly, the choice $\\alpha=0.2$ fails to converge. Bonus exercise: plot the progress for $\\alpha=0.2$ in the cell below to see what went wrong.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_opt = pandas.read_csv(\"exercises/ode/optimisation_progress.csv\")\n",
    "\n",
    "costs = np.array(df_opt[\"J\"])\n",
    "costs[0] = np.nan  # Remove the first entry because it's uninitialised garbage\n",
    "controls = np.array(df_opt[\"theta\"])\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "axes[0].loglog(df_opt[\"it\"], costs, \"--\", label=\"Cost function value\")\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "axes[1].plot(df_opt[\"it\"], df_opt[\"theta\"], \"--\", label=\"Control value\")\n",
    "axes[1].legend()\n",
    "axes[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_optimised = pandas.read_csv(\"exercises/ode/optimised.csv\")\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(times, np.exp(times), \"-\", color=\"k\", label=\"Analytical solution\")\n",
    "axes.plot(df_forward[\"t\"], df_forward[\"u\"], \"--x\", label=\"Forward Euler\")\n",
    "axes.plot(df_backward[\"t\"], df_backward[\"u\"], \":o\", label=\"Backward Euler\")\n",
    "axes.plot(df_optimised[\"t\"], df_optimised[\"u\"], \"-.^\", label=rf\"Optimised ($\\theta={controls[-1]:.4f}$)\")\n",
    "axes.legend()\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "As we might hope, the optimised value of $\\theta$ gives a much better approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Calculating the *full* Jacobian\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Question</b>\n",
    "\n",
    "Given a map $\\mathbf{f}$, some input $\\mathbf{x}$, and some seed $\\dot{\\mathbf{x}}$, we have the Jacobian vector product\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}.$$\n",
    "\n",
    "*How can we use this to compute the full Jacobian matrix $\\nabla\\mathbf{f}(\\mathbf{x})$?*\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "    \n",
    "$$\n",
    "\\nabla\\mathbf{f}(\\mathbf{x})\n",
    "=\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{I}_n\n",
    "=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}\\mathbf{e}_1,\\mathbf{e}_2,\\dots,\\mathbf{e}_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Apply JVP to the $n$ canonical unit vectors.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8ee3e-79be-444f-add5-73a3f18555ff",
   "metadata": {},
   "source": [
    "## Vector mode\n",
    "\n",
    "The expression $\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{e}_n\\end{bmatrix}$ above is a concatenation of $n$ JVPs. Whilst it's possible to apply these in a loop, most AD tools support a 'vector' mode, which allows this to be done in a single call. This is the *vector-Jacobian product (VJP)*:\n",
    "\n",
    "$$\\text{VJP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{X}}):=\\begin{bmatrix}\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_1,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_2,\\dots,\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}_k\\end{bmatrix},$$\n",
    "\n",
    "for a *seed matrix* $\\dot{\\mathbf{X}}:=\\begin{bmatrix}\\dot{\\mathbf{x}}_1,\\dot{\\mathbf{x}}_2,\\dots,\\dot{\\mathbf{x}}_k\\end{bmatrix}\\in\\mathbb{R}^{k\\times n}$, $k\\in\\mathbb{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Example: tridiagonal matrix\n",
    "\n",
    "Consider the classic central difference approximation to the second derivative (yes, more derivatives!):\n",
    "$$\n",
    "    \\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\approx\\frac{u(x-h)-2u(x)+u(x+h)}{h^2},\n",
    "$$\n",
    "for some uniform spacing $h>0$. For a uniform discretisation $\\{x_k\\}_{k=1}^n$ of an interval, this can be written as\n",
    "$$\n",
    "    \\left.\\frac{\\mathrm{d}^2u}{\\mathrm{d}x^2}\\right|_{x=x_k}\\approx\\frac{u_{k-1}-2u_k+u_{k+1}}{h^2},\n",
    "$$\n",
    "where $u_k$ is an approximation of $u(x_k)$. This is implemented in Fortran in `session1/exercises/sparse/central_diff.f90` for the case of a periodic 1D function.\n",
    "\n",
    "```fortran\n",
    "! Central difference approximation for the second derivative of a periodic 1D function\n",
    "subroutine central_diff(u, approx, h, n)\n",
    "  implicit none\n",
    "\n",
    "  integer, intent(in) :: n                   ! Number of grid points\n",
    "  real, intent(in) :: h                      ! Uniform grid spacing\n",
    "  real, dimension(n), intent(in) :: u        ! Input vector\n",
    "  real, dimension(n), intent(out) :: approx  ! Central difference approximation\n",
    "  integer :: i                               ! Dummy index for looping\n",
    "\n",
    "  if (size(u, 1) /= n) then\n",
    "    print *, \"Invalid input array size\"\n",
    "    stop 1\n",
    "  end if\n",
    "  if (size(approx, 1) /= n) then\n",
    "    print *, \"Invalid output array size\"\n",
    "    stop 1\n",
    "  end if\n",
    "\n",
    "  do i = 1, n\n",
    "    if (i == 1) then\n",
    "      ! Periodic boundary on the left\n",
    "      approx(i) = (u(n) - 2 * u(i) + u(i+1)) / h ** 2\n",
    "    else if (i == n) then\n",
    "      ! Periodic boundary on the right\n",
    "      approx(i) = (u(i-1) - 2 * u(i) + u(1)) / h ** 2\n",
    "    else\n",
    "      ! Interior points\n",
    "      approx(i) = (u(i-1) - 2 * u(i) + u(i+1)) / h ** 2\n",
    "    end if\n",
    "  end do\n",
    "\n",
    "end subroutine central_diff\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Apply Tapenade in vector mode to compute the derivative of the output array `approx` with respect to the input array `u`.\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "    \n",
    "Running\n",
    "```sh\n",
    "$ tapenade -head \"central_diff(approx)/(u)\" -vector central_diff.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./central_diff_dv.f90 \n",
    "@@ Created ./central_diff_dv.msg\n",
    "$ cat central_diff_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of central_diff in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: approx\n",
    "!   with respect to varying inputs: u\n",
    "!   RW status of diff variables: approx:out u:in\n",
    "! Central difference approximation for the second derivative of a periodic 1D function\n",
    "SUBROUTINE CENTRAL_DIFF_DV(u, ud, approx, approxd, h, n, nbdirs)\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: u\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: ud\n",
    "  REAL, DIMENSION(n), INTENT(OUT) :: approx\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(OUT) :: approxd\n",
    "  REAL, INTENT(IN) :: h\n",
    "  INTEGER :: i\n",
    "  INTRINSIC SIZE\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  IF (SIZE(u, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid input array size'\n",
    "    STOP\n",
    "  ELSE IF (SIZE(approx, 1) .NE. n) THEN\n",
    "    PRINT*, 'Invalid output array size'\n",
    "    STOP\n",
    "  ELSE\n",
    "    approxd = 0.0\n",
    "    DO i=1,n\n",
    "      IF (i .EQ. 1) THEN\n",
    "        DO nd=1,nbdirs\n",
    "          approxd(nd, i) = (ud(nd, n)-2*ud(nd, i)+ud(nd, i+1))/h**2\n",
    "        END DO\n",
    "        approx(i) = (u(n)-2*u(i)+u(i+1))/h**2\n",
    "      ELSE IF (i .EQ. n) THEN\n",
    "        DO nd=1,nbdirs\n",
    "          approxd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, 1))/h**2\n",
    "        END DO\n",
    "        approx(i) = (u(i-1)-2*u(i)+u(1))/h**2\n",
    "      ELSE\n",
    "        DO nd=1,nbdirs\n",
    "          approxd(nd, i) = (ud(nd, i-1)-2*ud(nd, i)+ud(nd, i+1))/h**2\n",
    "        END DO\n",
    "        approx(i) = (u(i-1)-2*u(i)+u(i+1))/h**2\n",
    "      END IF\n",
    "    END DO\n",
    "  END IF\n",
    "END SUBROUTINE CENTRAL_DIFF_DV\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Tridiagonal example: compute dense Jacobian\n",
    "\n",
    "In `session1/exercises/sparse/dense_jacobian.f90` we provide a program for computing the Jacobian of the centred difference function over the uniform partition of the interval.\n",
    "\n",
    "```fortran\n",
    "! Module containing the nbdirsmax variable required by central_diff_dv\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 10\n",
    "end module diffsizes\n",
    "\n",
    "! Include the vector forward mode derivative generated by Tapenade\n",
    "include \"central_diff_dv.f90\"\n",
    "\n",
    "! Naive program for computing a tridiagonal Jacobian\n",
    "program dense_jacobian\n",
    "  use diffsizes, only: m => nbdirsmax  ! Number of seed vectors\n",
    "\n",
    "  implicit none\n",
    "\n",
    "  logical, parameter :: write = .true. ! Flag for whether to write output to file\n",
    "  integer, parameter :: n = 10         ! Number of grid points\n",
    "  real, parameter :: h = 1.0           ! Uniform grid spacing\n",
    "  real, dimension(n) :: u              ! Input vector\n",
    "  real, dimension(n) :: approx         ! Central difference approximation\n",
    "  real, dimension(m,n) :: seed         ! Seed matrix for the VJP\n",
    "  real, dimension(m,n) :: jacobian     ! Jacobian for the central difference calculation\n",
    "  integer :: i                         ! Dummy index for looping\n",
    "\n",
    "  if (m /= n) then\n",
    "    print *, \"Error: number of grid points must match number of seed vectors.\"\n",
    "    stop\n",
    "  end if\n",
    "\n",
    "  ! Specify some arbitrary input\n",
    "  u(:) = 1.0\n",
    "\n",
    "  ! Set up the seed matrix as the nxn identity\n",
    "  seed(:,:) = 0.0\n",
    "  do i = 1, n\n",
    "    seed(i,i) = 1.0\n",
    "  end do\n",
    "\n",
    "  ! Propagate the seed matrix through the VJP\n",
    "  call central_diff_dv(u, seed, approx, jacobian, h, n, n)\n",
    "\n",
    "  ! Write out the result to file\n",
    "  if (write) then\n",
    "    open(unit=10, file=\"dense_jacobian.dat\")\n",
    "    do i = 1, n\n",
    "      write(unit=10, fmt=100) jacobian(i,:)\n",
    "    end do\n",
    "    100 format(10(f4.1,\",\"))\n",
    "    close(unit=10)\n",
    "  end if\n",
    "\n",
    "end program dense_jacobian\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran dense_jacobian.f90 -o dense_jacobian\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./dense_jacobian\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "Compiling and running produces a file `dense_jacobian.dat`, which we can read and view in the notebook. Note that the computation here required $n$ applications of forward mode AD, where $n$ is the number of gridpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_matrix(\"exercises/sparse/dense_jacobian.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Sparse AD\n",
    "\n",
    "We can compute the full Jacobian with\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\nabla\\mathbf{f}(\\mathbf{x})\\mathbf{I}_n=\\nabla\\mathbf{f}(\\mathbf{x})\\begin{bmatrix}\\mathbf{e}_1,\\mathbf{e}_2,\\dots,\\mathbf{e}_n\\end{bmatrix}.$$\n",
    "\n",
    "* But what about when $n$ gets very large?\n",
    "* And what about when the Jacobian is sparse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Diagonal Jacobian example\n",
    "\n",
    "Suppose $\\nabla\\mathbf{f}(\\mathbf{x})$ is diagonal, say\n",
    "\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})=\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}.$$\n",
    "\n",
    "Then, for a seed vector $\\dot{\\mathbf{x}}=\\begin{bmatrix}\\dot{x}_1 & \\dot{x}_2 & \\dots & \\dot{x}_n\\end{bmatrix}^T$, we have\n",
    "\n",
    "$$\n",
    "\\nabla\\mathbf{f}(\\mathbf{x})\\:\\dot{\\mathbf{x}}\n",
    "=\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}\n",
    "\\begin{bmatrix}\\dot{x}_1 \\\\ \\dot{x}_2 \\\\ \\vdots \\\\ \\dot{x}_n\\end{bmatrix}\n",
    "=\\begin{bmatrix}J_1\\dot{x}_1 \\\\ J_2\\dot{x}_2 \\\\ \\vdots \\\\ J_n\\dot{x}_n\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Making the clever choice of seed vector $\\dot{\\mathbf{x}}:=\\mathbf{e}=\\begin{bmatrix}1 & 1 & \\dots & 1\\end{bmatrix}^T$, we can apply **a single JVP** and then back out the full Jacobian by putting each entry on the diagonal:\n",
    "$$\n",
    "    \\mathrm{diag}(\\nabla\\mathbf{f}(\\mathbf{x})\\:\\mathbf{e})\n",
    "    =\\mathrm{diag}\\left(\\begin{bmatrix}J_1 \\\\ J_2 \\\\ \\vdots \\\\ J_n \\end{bmatrix}\\right)\n",
    "    =\\begin{bmatrix}J_1\\\\& J_2\\\\ & & \\ddots\\\\ & & & J_n\\end{bmatrix}\n",
    "    =\\nabla\\mathbf{f}(\\mathbf{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Sparse AD: what colour is your Jacobian?\n",
    "\n",
    "The example above is the simplest case of what is known as *colouring*. The idea is to group columns of the Jacobian such that the columns in each group are linearly independent. This gives rise to compression approaches for sparse AD.\n",
    "\n",
    "<img src=\"images/colours.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 5:</strong> Jacobian colouring diagram taken from Gebremedhin, et al (2005).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Tridiagonal example: compute compressed Jacobian\n",
    "\n",
    "In `session1/exercises/sparse/compressed_jacobian.f90` we provide a program for computing the same Jacobian of the centred difference function but this time using a clever choice of seed matrix:\n",
    "$$\n",
    "    \\dot{\\mathbf{X}}:=\\begin{bmatrix}\n",
    "        1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1\\\\\n",
    "        0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0\\\\\n",
    "        0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "```fortran\n",
    "! Module containing the nbdirsmax variable required by central_diff_dv\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 3\n",
    "end module diffsizes\n",
    "\n",
    "! Include the vector forward mode derivative generated by Tapenade\n",
    "include \"central_diff_dv.f90\"\n",
    "\n",
    "! Program for computing a tridiagonal Jacobian using compression\n",
    "program compressed_jacobian\n",
    "  use diffsizes, only: m => nbdirsmax  ! Number of seed vectors\n",
    "\n",
    "  implicit none\n",
    "\n",
    "  logical, parameter :: write = .true. ! Flag for whether to write output to file\n",
    "  integer, parameter :: n = 10         ! Number of grid points\n",
    "  real, parameter :: h = 1.0           ! Uniform grid spacing\n",
    "  real, dimension(n) :: u              ! Input vector\n",
    "  real, dimension(n) :: approx         ! Central difference approximation\n",
    "  real, dimension(m,n) :: seed         ! Seed matrix for the VJP\n",
    "  real, dimension(m,n) :: compressed   ! Compressed Jacobian for the central difference calculation\n",
    "  real, dimension(n,n) :: jacobian     ! Jacobian for the central difference calculation\n",
    "  integer :: i, j                      ! Dummy indices for looping\n",
    "\n",
    "  ! Specify some arbitrary input\n",
    "  u(:) = 1.0\n",
    "\n",
    "  ! Set up the seed matrix as a 3xn array\n",
    "  seed(:,:) = 0.0\n",
    "  do i = 1, m\n",
    "    seed(i,i::m) = 1.0\n",
    "  end do\n",
    "\n",
    "  ! Apply the VJP\n",
    "  call central_diff_dv(u, seed, approx, compressed, h, n, m)\n",
    "\n",
    "  ! Write out the compressed result to file\n",
    "  if (write) then\n",
    "    open(unit=10, file=\"compressed_jacobian.dat\")\n",
    "    do i = 1, m\n",
    "      write(unit=10, fmt=100) compressed(i,:)\n",
    "    end do\n",
    "    100 format(10(f4.1,\",\"))\n",
    "    close(unit=10)\n",
    "  end if\n",
    "\n",
    "  ! Decompress rows and insert them into the Jacobian\n",
    "  do i = 1, m\n",
    "    j = i\n",
    "    jacobian(i::m,:) = 0.0\n",
    "    do while (j <= n)\n",
    "      if (j == 1) then\n",
    "        jacobian(j,n) = compressed(i,n)\n",
    "      else\n",
    "        jacobian(j,j-1) = compressed(i,j-1)\n",
    "      end if\n",
    "      jacobian(j,j) = compressed(i,j)\n",
    "      if (j == n) then\n",
    "        jacobian(j,1) = compressed(i,1)\n",
    "      else\n",
    "        jacobian(j,j+1) = compressed(i,j+1)\n",
    "      end if\n",
    "      j = j + m\n",
    "    end do\n",
    "  end do\n",
    "\n",
    "  ! Write out the result to file\n",
    "  if (write) then\n",
    "    open(unit=11, file=\"decompressed_jacobian.dat\")\n",
    "    do i = 1, n\n",
    "      write(unit=11, fmt=100) jacobian(i,:)\n",
    "    end do\n",
    "    close(unit=11)\n",
    "  end if\n",
    "\n",
    "end program compressed_jacobian\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Details on running the code</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Compile with\n",
    "```sh\n",
    "$ gfortran compressed_jacobian.f90 -o compressed_jacobian\n",
    "```\n",
    "and then run with\n",
    "```sh\n",
    "$ ./compressed_jacobian\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "Compiling and running produces files `compressed_jacobian.dat` and `decompressed_jacobian.dat`, which we can again read and view in the notebook. This time we only need to compute **three** JVPs, rather than $n$!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "<b>Exercise</b>\n",
    "\n",
    "Experiment with increasing the number of gridpoints to see the impact on the overall cost of the dense approach versus the compressed approach.\n",
    "\n",
    "Hint: Using the `time` command line tool should give some very basic timing information.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Make the changes\n",
    "```diff\n",
    "diff --git a/session1/exercises/sparse/compressed_jacobian.f90 b/session1/exercises/sparse/compressed_jacobian.f90\n",
    "index e12f353..c5ece7a 100644\n",
    "--- a/session1/exercises/sparse/compressed_jacobian.f90\n",
    "+++ b/session1/exercises/sparse/compressed_jacobian.f90\n",
    "@@ -13,8 +13,8 @@ program compressed_jacobian\n",
    " \n",
    "   implicit none\n",
    " \n",
    "-  logical, parameter :: write = .true. ! Flag for whether to write output to file\n",
    "-  integer, parameter :: n = 10         ! Number of grid points\n",
    "+  logical, parameter :: write = .false. ! Flag for whether to write output to file\n",
    "+  integer, parameter :: n = 10000      ! Number of grid points\n",
    "   real, parameter :: h = 1.0           ! Uniform grid spacing\n",
    "   real, dimension(n) :: u              ! Input vector\n",
    "   real, dimension(n) :: approx         ! Central difference approximation\n",
    "diff --git a/session1/exercises/sparse/dense_jacobian.f90 b/session1/exercises/sparse/dense_jacobian.f90\n",
    "index a088f30..1bd2227 100644\n",
    "--- a/session1/exercises/sparse/dense_jacobian.f90\n",
    "+++ b/session1/exercises/sparse/dense_jacobian.f90\n",
    "@@ -1,7 +1,7 @@\n",
    " ! Module containing the nbdirsmax variable required by central_diff_dv\n",
    " module diffsizes\n",
    "   implicit none\n",
    "-  integer, parameter :: nbdirsmax = 10\n",
    "+  integer, parameter :: nbdirsmax = 10000\n",
    " end module diffsizes\n",
    " \n",
    " ! Include the vector forward mode derivative generated by Tapenade\n",
    "@@ -13,8 +13,8 @@ program dense_jacobian\n",
    " \n",
    "   implicit none\n",
    " \n",
    "-  logical, parameter :: write = .true. ! Flag for whether to write output to file\n",
    "-  integer, parameter :: n = 10         ! Number of grid points\n",
    "+  logical, parameter :: write = .false. ! Flag for whether to write output to file\n",
    "+  integer, parameter :: n = 10000      ! Number of grid points\n",
    "   real, parameter :: h = 1.0           ! Uniform grid spacing\n",
    "   real, dimension(n) :: u              ! Input vector\n",
    "   real, dimension(n) :: approx         ! Central difference approximation\n",
    "```\n",
    "recompile and then run\n",
    "```sh\n",
    "$ time ./compressed && time ./dense\n",
    "```\n",
    "You should get output something like\n",
    "```default\n",
    "real    0m0.209s\n",
    "user    0m0.108s\n",
    "sys     0m0.101s\n",
    "\n",
    "real    0m0.522s\n",
    "user    0m0.295s\n",
    "sys     0m0.226s\n",
    "```\n",
    "indicating that the compressed approach is about twice as fast at this resolution. We could likely get a much better speedup by optimising the decompression strategy.\n",
    "    \n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Warning!</b> Make sure you turn off the `write` flag and recompile before conducting any timing experiments.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_matrix(\"exercises/sparse/compressed_jacobian.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32529b06-d72e-4b72-9aa7-74846f24e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_matrix(\"exercises/sparse/decompressed_jacobian.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Sparse AD: in practice\n",
    "\n",
    "It's worth noting that we used knowledge about the tridiagonal structure of the Jacobian to choose the seed vectors... so that we could compute the Jacobian. For Jacobians in the wild (e.g., those related to networks) we don't necessarily have such intuition. In practice, we need to perform an initial step to establish the sparsity pattern before we can do the colouring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Sparse AD: overall performance comparison\n",
    "\n",
    "We have considered three approaches for sparse AD:\n",
    "1. 'Dense approach', which computes a JVP for each canonical unit vector.\n",
    "2. 'Compressed approach', which computes JVPs for sums over sets of linearly independent columns.\n",
    "3. 'Matrix-free approach', which applies a single JVP. (Only useful if the problem involves JVPs.)\n",
    "\n",
    "In the following, we walk through a performance comparison of these different approaches applied to the solution of the nonlinear Gray-Scott equation\n",
    "$$\n",
    "    \\frac{\\partial u}{\\partial t}=D_1\\Delta u-uv^2+\\gamma(1-u),\n",
    "    \\quad\\frac{\\partial v}{\\partial t}=D_2\\Delta v-uv^2=(\\gamma+\\kappa)v,\n",
    "    \\quad u,v:[0,2.5]^2\\rightarrow\\mathbb{R}\n",
    "$$\n",
    "(subject to some initial conditions) using a finite difference approach with Crank-Nicolson timestepping ($\\theta=0.5$). This discretisation corresponds to a 5-point stencil and it turns out the Jacobian can be coloured with five colours. As such, the compressed approach involves five JVPs per Jacobian computation.\n",
    "\n",
    "<img src=\"images/runtimes.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 6:</strong> Performance comparison for different Jacobian computation approaches for solving the Gray-Scott equation, taken from Wallwork et al. (2019). PETSc was used for numerical solvers and ADOL-C for AD. The $N\\times N$ notation in the subcaptions refers to the grid dimension.</div>\n",
    "\n",
    "<!-- Run on a Knights Landing node at Argonne National Laboratory -->\n",
    "\n",
    "<br>\n",
    "\n",
    "Here 'analytic' refers to the hand-derived Jacobian, which we can't realistically expect to beat.\n",
    "\n",
    "<!-- Notes\n",
    "\n",
    "* Compressed approach exhibits strong scalability.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Sparse AD: convergence comparison\n",
    "\n",
    "Matrix-free works well for small dimensions. However, it slows down at higher resolution because Jacobian assembly is done at each iteration of the *nonlinear* solver, whereas the JVP is calculated at each iteration of the *linear* solver. It's possible that a different choice of linear solver or preconditioner would reduce the cost of the matrix-free approach.\n",
    "\n",
    "<img src=\"images/linear_solver_table.png\" width=600 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 7:</strong> <em>Linear</em> solver iterations as a function of grid resolution and number of processors for the Gray-Scott problem. (Numbers of <em>nonlinear</em> solver iterations are much lower.) Taken from Wallwork et al. (2019).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Summary and outlook\n",
    "\n",
    "In today's session we:\n",
    "* Got a brief history of automatic differentiation.\n",
    "* Learnt about *forward mode* and the *source transformation* approach.\n",
    "* Tried out the *Tapenade* source transformation AD tool applied to some test problems.\n",
    "* Verified the code generated by Tapenade both manually and using the *Taylor test*.\n",
    "* Learnt about compressed and matrix-free approaches for sparse problems.\n",
    "\n",
    "In tomorrow's session (at the same time) we will:\n",
    "* Learn about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verify the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Calculate higher order derivatives using Tapenade.\n",
    "* Try out the *Pyadjoint* operator overloading AD tool underpinnning the Firedrake finite element library and see showcases of more advanced AD usage.\n",
    "* Learn about *checkpointing* and using AD to compute higher order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* R. E. Wengert. *A simple automatic derivative evaluation program* (1964). Communications\n",
    "of the ACM, 7(8):463â€“464.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation* (1992). Optimization Methods & Software, 1:35â€“54.\n",
    "* D. Cortild, et al. *A Brief Review of Automatic Differentiation* (2023).\n",
    "* A. H. Gebremedhin, et al. *What color is your Jacobian? Graph coloring for computing derivatives* (2005). SIAM review, 47(4), pp.629-705.\n",
    "* J. G. Wallwork, et al. *Computing derivatives for petsc adjoint solvers using algorithmic differentiation* (2019), arXiv preprint arXiv:1909.02836."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
