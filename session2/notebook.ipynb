{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation and operator overloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: motivation on backprop; think about learning outcomes</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In today's session we will:\n",
    "* Learn about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verify the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Try out the *autograd* operator overloading AD tool underpinnning PyTorch (as well as libtorch (C++) and FTorch (Fortran)).\n",
    "* Learn about *checkpointing* and using AD to compute higher order derivatives.\n",
    "* See a showcase of some more advanced use cases of AD using Firedrake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "**Recall from session 1**: This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "**Recall from session 1**: For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "**Recall from session 1**: When it comes to *forward* derivatives in code, we use the `_d` notation, which is standard in the AD literature.\n",
    "\n",
    "When it comes to *reverse* mode derivatives in code, we use the `_b` notation (for \"backward\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Where did the history section go?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "> Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "> $$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    ">\n",
    "> Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    ">\n",
    "> Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Jacobian-transpose-vector product (JTVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb{R}^m.$$\n",
    "\n",
    "Given $\\mathbf{x}\\in A$ and a seed vector $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, reverse mode AD allows us to compute the *transpose action* (transposed matrix-vector product)\n",
    "$$\\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}.$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that the JTVP is well defined.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "We have $\\nabla\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{m\\times n}$, so $\\nabla\\mathbf{f}(\\mathbf{x})^T\\in\\mathbb{R}^{n\\times m}$. Since $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, the dimensions are appropriate to take the JTVP.\n",
    "</details>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "Again, the computation is <em>matrix-free</em>. We don't actually need the Jacobian or its transpose when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Forward mode vs. reverse mode\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "* Forward mode is more appropriate if $n\\ll m$, i.e., $\\#inputs\\ll\\#outputs$.\n",
    "  * e.g., sensitivity analysis or optimisation w.r.t. a small number of parameters.\n",
    "* Reverse mode is more appropriate if $n\\gg m$, i.e., $\\#inputs\\gg\\#outputs$.\n",
    "  * e.g., ODE/PDE-constrained optimisation (cost function), machine learning training (loss function), goal-oriented error estimation (quantity of interest).\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: reverse mode\n",
    "\n",
    "Recall the $f$ \\& $g$ example from session 1. The Fortran code for the two functions is copied in the repository at `session2/exercises/fg/f.f90` and `session2/exercises/fg/g.f90`.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Either [install Tapenade](https://tapenade.gitlabpages.inria.fr/tapenade/distrib/README.html)* or visit the [Tapenade web interface](http://tapenade.inria.fr:8080/tapenade/index.jsp).\n",
    "2. Run `tapenade -h` to review the options.\n",
    "3. Apply Tapenade to each of these subroutines **in reverse mode**, which will compute the JTVP for some seed vector.\n",
    "4. Inspect the output files `f_b.f90` and `g_b.f90` and check they are as you expect.\n",
    "5. Inspect the message files `f_b.msg` and `g_b.msg`.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "<b>Solutions 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solutions 3 and 4</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade -b f.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "@@ Created ./f_b.f90 \n",
    "@@ Created ./f_b.msg\n",
    "$ cat f_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: x y z\n",
    "!   with respect to varying inputs: x y z\n",
    "!   RW status of diff variables: x:incr y:incr z:in-zero\n",
    "SUBROUTINE F_B(x, xb, y, yb, z, zb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL :: xb, yb\n",
    "  REAL :: z\n",
    "  REAL :: zb\n",
    "  xb = xb + y*zb\n",
    "  yb = yb + x*zb\n",
    "  zb = 0.0\n",
    "END SUBROUTINE F_B\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade tapenade -b g.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "@@ Created ./g_b.f90 \n",
    "@@ Created ./g_b.msg\n",
    "$ cat g_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: v z\n",
    "!   with respect to varying inputs: v z\n",
    "!   RW status of diff variables: v:in-zero z:incr\n",
    "SUBROUTINE G_B(z, zb, v, vb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL :: zb\n",
    "  REAL, DIMENSION(2) :: v\n",
    "  REAL, DIMENSION(2) :: vb\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  zb = zb + COS(z)*vb(1) - SIN(z)*vb(2)\n",
    "  vb = 0.0\n",
    "END SUBROUTINE G_B\n",
    "```\n",
    "\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Verification: the dot product test\n",
    "\n",
    "We have approaches for computing derivatives with both forward mode and reverse mode. But how do we know the outputs are consistent? This can be verified using the so-called *dot product test*.\n",
    "\n",
    "In the dot product test, we define a quantity $\\psi:=\\bar{\\mathbf{y}}\\dot{\\mathbf{y}}$, where $\\dot{\\mathbf{y}}=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$ is the output of forward mode and check that $\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}$, where $\\bar{\\mathbf{x}}^T=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}$ is the output of reverse mode.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "Prove that we do expect $\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}$.\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "$$\n",
    "    \\psi:=\\bar{\\mathbf{y}}\\dot{\\mathbf{y}}\n",
    "    =\\bar{\\mathbf{y}}(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}})\n",
    "    =(\\bar{\\mathbf{y}}\\nabla\\mathbf{f}(\\mathbf{x}))\\dot{\\mathbf{x}}\n",
    "    =\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: dot product test\n",
    "\n",
    "We've already computed forward mode and reverse mode derivatives for the function $f$ using Tapenade. We can run a dot product test using the code in `session2/exercises/fg/dot_product_test.f90`.\n",
    "\n",
    "```fortran\n",
    "include \"f_d.f90\"\n",
    "include \"f_b.f90\"\n",
    "\n",
    "! Program for verifying the consistency of the forward mode and reverse mode derivatives of the\n",
    "! function f.\n",
    "program dot_product_test\n",
    "  implicit none\n",
    "\n",
    "  real :: x, xd, xb\n",
    "  real :: y, yd, yb\n",
    "  real :: z, zd, zb\n",
    "\n",
    "  real :: result1, result2\n",
    "  real, parameter :: atol = 1e-05\n",
    "\n",
    "  ! Set arbitrary primal input\n",
    "  x = 1.2\n",
    "  y = -2.3\n",
    "\n",
    "  ! Call forward mode with some arbitrary seeds\n",
    "  xd = 4.2\n",
    "  yd = -0.7\n",
    "  call f_d(x, xd, y, yd, z, zd)\n",
    "\n",
    "  ! Choose a seed for reverse mode and evaluate the first result\n",
    "  zb = 3.0\n",
    "  result1 = zd * zb\n",
    "\n",
    "  ! Call reverse mode and evaluate the second result\n",
    "  xb = 0.0\n",
    "  yb = 0.0\n",
    "  call f_b(x, xb, y, yb, z, zb)\n",
    "  result2 = xd * xb + yd * yb\n",
    "\n",
    "  ! Check the two results match within the prespecified tolerance\n",
    "  if (abs(result1 - result2) < atol) then\n",
    "    write(unit=6, fmt=\"('PASS')\")\n",
    "  else\n",
    "    write(unit=6, fmt=\"('FAIL with atol=',e10.4)\") atol\n",
    "  end if\n",
    "\n",
    "end program dot_product_test\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Build and run the `dot_product_test` program.\n",
    "2. Why does the test fail if the `result1` assignment is moved to after the call to `f_b`?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ cd exercises/fg\n",
    "$ gfortran dot_product_test.f90 -o dot_product_test\n",
    "$ ./dot_product_test\n",
    "PASS\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Because `zb` gets reset to zero by `f_b`. This is done by default because it's almost always the Right Thing to do.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Question</b>\n",
    "    \n",
    "What are two ways we can we use these to compute the Hessian of $f$?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply forward mode to compute the gradient of *this* (i.e., apply forward mode to the *forward mode derivative code*). Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply reverse mode to compute the gradient of *this* (i.e., apply reverse mode to the *forward mode derivative code*). That is, $(\\nabla(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}))^T\\bar{\\mathbf{y}}=\\dot{\\mathbf{x}}^T\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})\\bar{\\mathbf{y}}$. Here the Hessian $\\mathbf{H}(\\mathbf{f}):=\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})$ is symmetric and so the two applications give the Hessian-vector product with the seed. Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Speelpenning example\n",
    "\n",
    "We compute the Hessian for the classic test case introduced by Speelpenning, which amounts to a reduction of a vector using the product:\n",
    "$$\n",
    "    f(\\mathbf{x})=\\prod_{i=1}^nx_i.\n",
    "$$\n",
    "This test case is of interest because its Hessian is dense.\n",
    "\n",
    "The Speelpenning function is implemented as a Fortran subroutine in `session2/exercises/speelpenning/speelpenning.f90`:\n",
    "```fortran\n",
    "subroutine speelpenning(x, f, n)\n",
    "  implicit none\n",
    "  integer, intent(in) :: n\n",
    "  real, dimension(n), intent(in) :: x\n",
    "  real, intent(out) :: f\n",
    "  integer :: i\n",
    "  f = 1.0\n",
    "  do i = 1, n\n",
    "    f = f * x(i)\n",
    "  end do\n",
    "end subroutine speelpenning\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercises</b>\n",
    "\n",
    "1. Compute the Hessian of the `speelpenning` subroutine using Tapenade.*\n",
    "2. Build and run the `view_hessian` program in `session2/exercises/speelpenning/view_hessian.f90` and convince yourself that the output is as you expect.\n",
    "\n",
    "*Hint: One of the two approaches is much easier than the other!\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -vector speelpenning.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Command: Took subroutine speelpenning as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv.f90 \n",
    "@@ Created ./speelpenning_dv.msg\n",
    "$ cat speelpenning_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV\n",
    "```\n",
    "We need to define the `diffsizes` module. Create `diffsizes.f90` containing\n",
    "```fortran\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 7\n",
    "end module diffsizes\n",
    "```\n",
    "\n",
    "Inline this with the VJP code:\n",
    "```sh\n",
    "$ cat diffsizes.f90 speelpenning_dv.f90 > tmp.f90\n",
    "```\n",
    "\n",
    "Then we can apply vector forward mode:\n",
    "```sh\n",
    "$ tapenade -vector -head \"speelpenning_dv(fd)/(x)\" tmp.f90 -o speelpenning_dv\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv_dv.f90 \n",
    "@@ Created ./speelpenning_dv_dv.msg\n",
    "$ cat speelpenning_dv_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning_dv in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: fd\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: x:in fd:out\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV_DV(x, xd0, xd, f, fd, fdd, n, nbdirs, nbdirs0&\n",
    "&)\n",
    "  USE DIFFSIZES\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax0, n), INTENT(IN) :: xd0\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax0) :: fd0\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  REAL, DIMENSION(nbdirsmax0, nbdirsmax), INTENT(OUT) :: fdd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  INTEGER :: nd0\n",
    "  INTEGER :: nbdirs0\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  fd0 = 0.0\n",
    "  fdd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      DO nd0=1,nbdirs0\n",
    "        fdd(nd0, nd) = fd(nd)*xd0(nd0, i) + x(i)*fdd(nd0, nd) + xd(nd, i&\n",
    "&         )*fd0(nd0)\n",
    "      END DO\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    DO nd0=1,nbdirs0\n",
    "      fd0(nd0) = x(i)*fd0(nd0) + f*xd0(nd0, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV_DV\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/speelpenning\n",
    "$ gfortran view_hessian.f90 -o view_hessian\n",
    "$ ./view_hessian\n",
    "```\n",
    "should give the output\n",
    "```default\n",
    "   0.0 2520.0 1680.0 1260.0 1008.0  840.0  720.0\n",
    "2520.0    0.0  840.0  630.0  504.0  420.0  360.0\n",
    "1680.0  840.0    0.0  420.0  336.0  280.0  240.0\n",
    "1260.0  630.0  420.0    0.0  252.0  210.0  180.0\n",
    "1008.0  504.0  336.0  252.0    0.0  168.0  144.0\n",
    " 840.0  420.0  280.0  210.0  168.0    0.0  120.0\n",
    " 720.0  360.0  240.0  180.0  144.0  120.0    0.0\n",
    "```\n",
    "It makes sense that the diagonal entries are zero. The matrix is symmetric, as expected. The $(6,7)^{th}$ and $(7,6)^{th}$ entries are both \n",
    "$$120=5!=\\prod_{i=1}^5x_i=\\frac{\\partial^2}{\\partial x_6\\partial x_7}\\prod_{i=1}^7x_i.$$\n",
    "Further checks left as an exercise to the reader.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: gradient values, tape, overloaded operators, tape unrolling</b>\n",
    "</div>\n",
    "\n",
    "#### Operator overloading tools:\n",
    "\n",
    "* LLVM\n",
    "    * [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "* C/C++\n",
    "    * About 2 dozen AD tools!\n",
    "    * e.g., [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial]\n",
    "* Fortran\n",
    "    * [Differentia](https://github.com/Nicholaswogan/Differentia), [lots of abandonware...]\n",
    "* Python\n",
    "    * [PyADOL-C](https://github.com/b45ch1/pyadolc), [Jax](https://github.com/jax-ml/jax), [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
    "* Julia\n",
    "    * About 2 dozen AD tools! https://juliadiff.org/\n",
    "    * e.g., Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable)\n",
    "    * [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface)\n",
    "* Domain-specific\n",
    "    * [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "* And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode demo\n",
    "\n",
    "Builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * (a ** 3 - b * b / 3)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Remark on `grad_fn`</b>\n",
    "</div>\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an external gradient. This defaults to a tensor of ones of the appropriate shape, but can be customised to account for cases where gradient values are propagated from another model or subcomponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.grad}\")\n",
    "print(f\"b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "##  Mathematical background on adjoint methods\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: derivation; continuous vs. discrete </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: overview </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Exercise: Sensitivity analysis\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Neural network training with backpropagation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: torch example </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Further applications\n",
    "\n",
    "* Data assimilation. <!-- Uses gradients of cost functions involving mismatches against observations to assimilate the observations. -->\n",
    "* Uncertainty quantification. <!-- Uses Hessians -->\n",
    "* Online training in machine learning. <!-- Requires derivatives of code downstream from machine learning outputs -->\n",
    "* PDE-constrained optimisation.  <!-- Typically uses gradient-based methods -->\n",
    "* Goal-oriented error estimation and mesh adaptation. <!-- Uses adjoint solutions -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "So far, we have considered AD applied to elementary operations (`+`, `-`, `*`, `/`, `**`) in Fortran. Most of the early AD tools worked in this way, tracking operations applied to real numbers and chaining together their derivatives. However, code differentiated with a source transformation tool in this manner can quickly become difficult to read for large codes. And under the operator overloading approach, the memory footprint of the tape can become very large.\n",
    "\n",
    "By raising the level of abstraction, some of these difficulties can be avoided.\n",
    "\n",
    "#### Medium-level: API calls\n",
    "\n",
    "Examples:\n",
    "* AD in PETSc\n",
    "  * https://github.com/SciCompKL/adjoint-PETSc.\n",
    "  * (Note: [old draft implementation](https://gitlab.com/petsc/petsc/-/tree/main/src/ts/tutorials/autodiff) differentiated elementary operators.)\n",
    "\n",
    "#### High-level: operations on fields\n",
    "\n",
    "Examples:\n",
    "* AD in Firedrake using Pyadjoint/dolfin-adjoint\n",
    "  * https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/06-pde-constrained-optimisation.ipynb.\n",
    "  * https://nbviewer.org/github/firedrakeproject/firedrake/blob/master/docs/notebooks/11-extract-adjoint-solutions.ipynb.\n",
    "* AD in PSyclone using PSyAD.\n",
    "  * https://github.com/stfc/PSyclone/tree/master/examples/psyad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## PDE-constrained optimisation showcase\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Firedrake </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Goal-oriented mesh adaptation\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Firedrake </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In today's session we:\n",
    "* Learnt about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verified the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Tried out the *autograd* operator overloading AD tool underpinnning PyTorch (as well as libtorch (C++) and FTorch (Fortran)).\n",
    "* Learnt about *checkpointing* and using AD to compute higher order derivatives.\n",
    "* Saw a showcase of some more advanced use cases of AD using Firedrake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992.\n",
    "* J. Huckelheim, et al. *A taxonomy of automatic differentiation pitfalls*, WIREs Data Mining Knowl Discovery, 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
