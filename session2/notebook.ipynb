{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a29344e-4054-44d4-8ec3-cb5f38ead232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1ea3e-5481-4dae-98fc-ad2295a15a0a",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation\n",
    "\n",
    "[TODO: intro text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29a48d-f172-4bc3-8c14-f0896eda7d68",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Reverse mode (the subject of this session) was discovered by Linnainmaa in the 1970s.\n",
    "* Speelpenning introduced the modern formulation of reverse mode in the late 1980s.\n",
    "* Griewank improved the feasibility of reverse mode in 1992 by introducing checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a0bfa-7ddb-4930-9a78-56f71635615b",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode demo\n",
    "\n",
    "Builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855c7416-81cf-4993-b88a-56ccd96ec19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13edc874-2378-4287-8e74-480b59733c08",
   "metadata": {},
   "source": [
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2467c8f-2a60-4b36-9aec-341eca24c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4ffc2-5c6f-4932-931a-d1a107a80147",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54159cd2-7c00-4f1b-aa08-b3d749c8ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = tensor([-12.,  65.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3 * (a ** 3 - b * b / 3)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1cec49-27f3-421f-89ab-1e7b10897d2f",
   "metadata": {},
   "source": [
    "[TODO: Remark on `grad_fn`]\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an external gradient. This defaults to a tensor of ones of the appropriate shape, but can be customised to account for cases where gradient values are propagated from another model or subcomponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e651f4-057c-47a4-b264-e107412c3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c5d896-e160-4685-a92e-b07a025a78b7",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8517d86d-bcb5-4f24-bdff-83c5debb6842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([36., 81.])\n",
      "b = tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a = {a.grad}\")\n",
    "print(f\"b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb2832b-53db-42d8-a678-2b0c98b31a63",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d1bb5-c440-4bc3-9ae0-2e2777b492d4",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
