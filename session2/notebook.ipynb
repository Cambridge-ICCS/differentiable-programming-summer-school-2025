{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from mpltools import annotation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from firedrake.adjoint import pyadjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation and operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Set up codespace now. (It will take a while!)</b>\n",
    "\n",
    "We will show you where to find this notebook in the repo while you wait.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* Were you interested in yesterday's session but unclear how to make use of differentiable programming in practice?\n",
    "* We computed first-order derivatives for various programs. However, some advanced applications like uncertainty quantification requires Hessians (matrices of second derivatives). How do we calculate those?\n",
    "* Have you ever wondered how neural network training actually works under the hood?\n",
    "\n",
    "<!-- Discuss optimisation of parameters in the net -->\n",
    "\n",
    "<img src=\"images/nn.svg\" width=300 style=\"margin-left:auto; margin-right:auto; clip-path: inset(0 50% 0 50%);\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 1:</strong> Neural network schematic created with <a href=\"https://alexlenail.me/NN-SVG/index.html\">https://alexlenail.me/NN-SVG/index.html</a>.</div>\n",
    "\n",
    "<br>\n",
    "Machine learning (ML) models typically have large numbers of parameters to be tuned and small numbers of outputs.\n",
    "Forward mode doesn't seem like such a great fit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In today's session we will:\n",
    "\n",
    "* Learn about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verify the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Calculate higher order derivatives using Tapenade.\n",
    "* Try out the *Pyadjoint* operator overloading AD tool underpinnning the Firedrake finite element library and see showcases of more advanced AD usage.\n",
    "* Learn about *checkpointing* and using AD to compute higher order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "**Recall from session 1**: This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "**Recall from session 1**: For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "<b>Caution</b> with the physics notation for derivatives $\\dot{x}$. It can refer to both seeds for forward mode and forward mode derivatives.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "<b>Caution</b>\n",
    "For seed vectors in reverse mode we will use the \"bar\" notation $\\bar{y}$. This is not a mean value.\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**Recall from session 1**: When it comes to *forward* derivatives in code, we use the `_d` notation, which is standard in the AD literature.\n",
    "\n",
    "When it comes to *reverse* mode derivatives in code, we use the `_b` notation (for \"backward\" or \"bar\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Brief history of reverse mode\n",
    "\n",
    "* Reverse mode (a.k.a. backpropagation) was discovered by Linnainmaa in the 1970s.\n",
    "* The terminology 'back-propagating error correction' had already been introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this.\n",
    "* Speelpenning introduced the modern formulation of reverse mode in the late 1980s.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/jake.png\" width=\"500\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 2:</strong> Schematic from\n",
    "  <a href=\"https://doi.org/10.2172/5254402\">(Speelpenning, 1980)</a>.\n",
    "</div>\n",
    "\n",
    "* Griewank improved the feasibility of reverse mode in 1992 by introducing checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "> Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "> $$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    ">\n",
    "> Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    ">\n",
    "> Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Jacobian-transpose-vector product (JTVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb{R}^m.$$\n",
    "\n",
    "Given $\\mathbf{x}\\in A$ and a seed vector $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, reverse mode AD allows us to compute the *transpose action* (transposed matrix-vector product)\n",
    "$$\\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}.$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that the JTVP is well defined.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "We have $\\nabla\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{m\\times n}$, so $\\nabla\\mathbf{f}(\\mathbf{x})^T\\in\\mathbb{R}^{n\\times m}$. Since $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, the dimensions are appropriate to take the JTVP.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Note</b>\n",
    "Again, the computation is <em>matrix-free</em>. We don't actually need the Jacobian or its transpose when we compute this product.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Forward mode vs. reverse mode\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "* Forward mode is more appropriate if $n\\ll m$, i.e., $\\#inputs\\ll\\#outputs$.\n",
    "  * e.g., sensitivity analysis or optimisation w.r.t. a small number of parameters.\n",
    "* Reverse mode is more appropriate if $n\\gg m$, i.e., $\\#inputs\\gg\\#outputs$.\n",
    "  * e.g., ODE/PDE-constrained optimisation (cost function), machine learning training (loss function), goal-oriented error estimation (quantity of interest).\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Directed Acyclic Graph\n",
    "\n",
    "Recall the introductory example from yesterday and the DAG representations of the $f$ and $g$ functions.\n",
    "\n",
    "Recalling that\n",
    "$$f(x_1,x_2)=x_1x_2$$\n",
    "and\n",
    "$$g(y)=(\\sin(y),\\cos(y)),$$\n",
    "we have\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/f_dag.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 3:</strong> Directed Acyclic Graph (DAG) for the $f$ function in the $f$ & $g$ example. Generated using tikZ and $\\LaTeX$.\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/g_dag.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 4:</strong> Directed Acyclic Graph (DAG) for the $g$ function in the $f$ & $g$ example. Generated using tikZ and $\\LaTeX$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: reverse mode seed vectors\n",
    "\n",
    "In reverse mode, gradient information propagates in the opposite direction.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/reverse_dag.png\" width=\"400\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 5:</strong> Directed Acyclic Graph (DAG) for the composition of the functions in the $f$ & $g$ example. Generated using tikZ and $\\LaTeX$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: reverse mode\n",
    "\n",
    "The Fortran code for the two functions is copied in the repository at `session2/exercises/fg/f.f90` and `session2/exercises/fg/g.f90`.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y)\n",
    "  implicit none\n",
    "  real, dimension(2), intent(in)  :: x\n",
    "  real, intent(out) :: y\n",
    "  y = x(1) * x(2)\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(y, z)\n",
    "  implicit none\n",
    "  real, intent(in) :: y\n",
    "  real, intent(out), dimension(2) :: z\n",
    "  z = [sin(y), cos(y)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Run `tapenade -h` to review the options.\n",
    "2. Apply Tapenade to each of these subroutines **in reverse mode**, which will compute the JTVP for some seed vector. Inspect the output files `f_b.f90` and `g_b.f90` and check they are as you expect.\n",
    "3. (Optional) Inspect the message files `f_b.msg` and `g_b.msg`.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade -b f.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "@@ Created ./f_b.f90 \n",
    "@@ Created ./f_b.msg\n",
    "$ cat f_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: x y\n",
    "!   with respect to varying inputs: x y\n",
    "!   RW status of diff variables: x:incr y:in-zero\n",
    "SUBROUTINE F_B(x, xb, y, yb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, DIMENSION(2), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(2) :: xb\n",
    "  REAL :: y\n",
    "  REAL :: yb\n",
    "  xb(1) = xb(1) + x(2)*yb\n",
    "  xb(2) = xb(2) + x(1)*yb\n",
    "  yb = 0.0\n",
    "END SUBROUTINE F_B\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade tapenade -b g.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "@@ Created ./g_b.f90 \n",
    "@@ Created ./g_b.msg\n",
    "$ cat g_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: y z\n",
    "!   with respect to varying inputs: y z\n",
    "!   RW status of diff variables: y:incr z:in-zero\n",
    "SUBROUTINE G_B(y, yb, z, zb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: y\n",
    "  REAL :: yb\n",
    "  REAL, DIMENSION(2) :: z\n",
    "  REAL, DIMENSION(2) :: zb\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  yb = yb + COS(y)*zb(1) - SIN(y)*zb(2)\n",
    "  zb = 0.0\n",
    "END SUBROUTINE G_B\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Verification: the dot product test\n",
    "\n",
    "We have approaches for computing derivatives with both forward mode and reverse mode. But how do we know the outputs are consistent? This can be verified using the so-called *dot product test*.\n",
    "\n",
    "In the dot product test, we define the inner product $\\psi:=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}$ of $\\bar{\\mathbf{y}}$ and $\\dot{\\mathbf{y}}$, where $\\dot{\\mathbf{y}}=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$ is the output of forward mode for some seed $\\dot{\\mathbf{x}}$ and $\\bar{\\mathbf{y}}$ is a seed for reverse mode. We check that $\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}$, where $\\bar{\\mathbf{x}}^T=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}$ is the output of reverse mode.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Optional exercise</b>\n",
    "    \n",
    "Prove that we do indeed expect $\\psi=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}=\\bar{\\mathbf{x}}^T\\dot{\\mathbf{x}}$.\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "$$\n",
    "    \\psi:=\\bar{\\mathbf{y}}^T\\dot{\\mathbf{y}}\n",
    "    =\\bar{\\mathbf{y}}^T(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}})\n",
    "    =(\\bar{\\mathbf{y}}^T\\nabla\\mathbf{f}(\\mathbf{x}))\\dot{\\mathbf{x}}\n",
    "    =(\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}})^T\\dot{\\mathbf{x}}\n",
    "    =\\bar{\\mathbf{x}}^T\\dot{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: dot product test\n",
    "\n",
    "We've already computed forward mode and reverse mode derivatives for the function $f$ using Tapenade. We can run a dot product test using the code in `session2/exercises/fg/dot_product_test.f90`.\n",
    "\n",
    "```fortran\n",
    "include \"f_d.f90\"\n",
    "include \"f_b.f90\"\n",
    "\n",
    "! Program for verifying the consistency of the forward mode and reverse mode derivatives of the\n",
    "! function f.\n",
    "program dot_product_test\n",
    "  implicit none\n",
    "\n",
    "  real, dimension(2) :: x   ! Primal input\n",
    "  real, dimension(2) :: xd  ! Forward mode seed\n",
    "  real, dimension(2) :: xb  ! Reverse mode derivative\n",
    "  real :: y                 ! Primal output\n",
    "  real :: yd                ! Forward mode derivative\n",
    "  real :: yb                ! Reverse mode seed\n",
    "\n",
    "  real :: result1                 ! LHS of dot product test\n",
    "  real :: result2                 ! RHS of dot product test\n",
    "  real, parameter :: atol = 1e-05 ! Absolute tolerance for the dot product test\n",
    "\n",
    "  ! Set arbitrary primal input\n",
    "  x(:) = [1.2, -2.3]\n",
    "\n",
    "  ! Call forward mode with some arbitrary seeds\n",
    "  xd(:) = [4.2, -0.7]\n",
    "  call f_d(x, xd, y, yd)\n",
    "\n",
    "  ! Choose a seed for reverse mode and evaluate the first result\n",
    "  yb = 3.0\n",
    "  result1 = dot_product([yd], [yb])\n",
    "\n",
    "  ! Call reverse mode and evaluate the second result\n",
    "  xb(:) = [0.0, 0.0]\n",
    "  call f_b(x, xb, y, yb)\n",
    "  result2 = dot_product(xd, xb)\n",
    "\n",
    "  ! Check the two results match within the prespecified tolerance\n",
    "  if (abs(result1 - result2) < atol) then\n",
    "    write(unit=6, fmt=\"('PASS')\")\n",
    "  else\n",
    "    write(unit=6, fmt=\"('FAIL with atol=',e10.4)\") atol\n",
    "  end if\n",
    "\n",
    "end program dot_product_test\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Regenerate the forward mode derivative code for `f.f90` using Tapenade.\n",
    "2. Build and run the `dot_product_test` program.\n",
    "3. (Optional) Why does the test fail if the `result1` assignment is moved to after the call to `f_b`?\n",
    "\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ cd exercises/fg\n",
    "$ gfortran dot_product_test.f90 -o dot_product_test\n",
    "$ ./dot_product_test\n",
    "PASS\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 3</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Because `yb` gets reset to zero by `f_b`. This is done by default because it's almost always the Right Thing to do.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "    \n",
    "What are two ways we can we use these to compute the Hessian of $f$?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply forward mode to compute the gradient of *this* (i.e., apply forward mode to the *forward mode derivative code*). Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply reverse mode to compute the gradient of *this* (i.e., apply reverse mode to the *forward mode derivative code*). That is, $(\\nabla(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}))^T\\bar{\\mathbf{y}}=\\dot{\\mathbf{x}}^T\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})\\bar{\\mathbf{y}}$. Here the Hessian $\\mathbf{H}(\\mathbf{f}):=\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})$ is symmetric and so the two applications give the Hessian-vector product with the seed. Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Speelpenning example\n",
    "\n",
    "We compute the Hessian for the classic test case introduced by Speelpenning, which amounts to a reduction of a vector using the product:\n",
    "$$\n",
    "    f(\\mathbf{x})=\\prod_{i=1}^nx_i.\n",
    "$$\n",
    "This test case is of interest because its Hessian is dense.\n",
    "\n",
    "The Speelpenning function is implemented as a Fortran subroutine in `session2/exercises/speelpenning/speelpenning.f90`:\n",
    "```fortran\n",
    "! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n",
    "subroutine speelpenning(x, f, n)\n",
    "  implicit none\n",
    "\n",
    "  integer, intent(in) :: n            ! Size of the input vector\n",
    "  real, dimension(n), intent(in) :: x ! Input vector\n",
    "  real, intent(out) :: f              ! Output value, product of all entries\n",
    "  integer :: i                        ! Dummy loop index\n",
    "\n",
    "  f = 1.0\n",
    "  do i = 1, n\n",
    "    f = f * x(i)\n",
    "  end do\n",
    "end subroutine speelpenning\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercises</b>\n",
    "\n",
    "1. Compute the Hessian of the `speelpenning` subroutine using Tapenade using two applications of forward mode.*\n",
    "2. Build and run the `view_hessian` program in `session2/exercises/speelpenning/view_hessian.f90`.\n",
    "3. (Optional) Derive the expected Hessian and convince yourself that the output is as you expect.\n",
    "4. (Optional) Compute the Hessian of the `speelpenning` subroutine using Tapenade using forward mode followed by reverse mode.*\n",
    "\n",
    "*Hint: You will need to pass the `diffsizes.f90` file as an additional input file for the second derivative calculation.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Forward-then-forward approach.\n",
    "\n",
    "```sh\n",
    "$ tapenade -vector speelpenning.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Command: Took subroutine speelpenning as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv.f90 \n",
    "@@ Created ./speelpenning_dv.msg\n",
    "$ cat speelpenning_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV\n",
    "```\n",
    "\n",
    "As mentioned in the hint, we need to make use of the `diffsizes` module defined in `diffsizes.f90`:\n",
    "```fortran\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 7\n",
    "end module diffsizes\n",
    "```\n",
    "\n",
    "We can apply vector forward mode again, passing this in as an additional input:\n",
    "```sh\n",
    "$ tapenade -vector -head \"speelpenning_dv(fd)/(x)\" diffsizes.f90 speelpenning_dv.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv_dv.f90 \n",
    "@@ Created ./speelpenning_dv_dv.msg\n",
    "$ cat speelpenning_dv_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning_dv in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: fd\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: x:in fd:out\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV_DV(x, xd0, xd, f, fd, fdd, n, nbdirs, nbdirs0&\n",
    "&)\n",
    "  USE DIFFSIZES\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax0, n), INTENT(IN) :: xd0\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax0) :: fd0\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  REAL, DIMENSION(nbdirsmax0, nbdirsmax), INTENT(OUT) :: fdd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  INTEGER :: nd0\n",
    "  INTEGER :: nbdirs0\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  fd0 = 0.0\n",
    "  fdd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      DO nd0=1,nbdirs0\n",
    "        fdd(nd0, nd) = fd(nd)*xd0(nd0, i) + x(i)*fdd(nd0, nd) + xd(nd, i&\n",
    "&         )*fd0(nd0)\n",
    "      END DO\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    DO nd0=1,nbdirs0\n",
    "      fd0(nd0) = x(i)*fd0(nd0) + f*xd0(nd0, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV_DV\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/speelpenning\n",
    "$ gfortran view_hessian.f90 -o view_hessian\n",
    "$ ./view_hessian\n",
    "```\n",
    "should give the output\n",
    "```default\n",
    "   0.0 2520.0 1680.0 1260.0 1008.0  840.0  720.0\n",
    "2520.0    0.0  840.0  630.0  504.0  420.0  360.0\n",
    "1680.0  840.0    0.0  420.0  336.0  280.0  240.0\n",
    "1260.0  630.0  420.0    0.0  252.0  210.0  180.0\n",
    "1008.0  504.0  336.0  252.0    0.0  168.0  144.0\n",
    " 840.0  420.0  280.0  210.0  168.0    0.0  120.0\n",
    " 720.0  360.0  240.0  180.0  144.0  120.0    0.0\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 3</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "It makes sense that the diagonal entries are zero. The matrix is symmetric, as expected.\n",
    "The $(i,j)^{th}$ entry of the Hessian is given by\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x_i\\partial x_j}\\prod_{k=1}^nx_k\n",
    "=\\frac{\\partial}{\\partial x_i}\\prod_{k=1,\\,j\\neq k}^nx_k\n",
    "=\\prod_{k=1,\\,j\\neq k,\\,j\\neq i}^nx_k\n",
    "$$\n",
    "As such, the $(6,7)^{th}$ and $(7,6)^{th}$ entries are both \n",
    "$$120=5!=\\prod_{i=1}^5x_i=\\frac{\\partial^2}{\\partial x_6\\partial x_7}\\prod_{i=1}^7x_i.$$\n",
    "Further checks left as an exercise to the reader.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 4</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Forward-then-reverse approach.\n",
    "```sh\n",
    "$ tapenade -b -vector -head \"speelpenning_dv(fd)/(x)\" diffsizes.f90 speelpenning_dv.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "@@ Created ./speelpenning_dv_bv.f90 \n",
    "@@ Created ./diffsizes_bv.msg\n",
    "$ cat speelpenning_dv_bv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning_dv in reverse (adjoint) mode (with options multiDirectional):\n",
    "!   gradient     of useful results: fd\n",
    "!   with respect to varying inputs: x fd\n",
    "!   RW status of diff variables: x:out fd:in-zero\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n",
    "SUBROUTINE SPEELPENNING_DV_BV(x, xb, xd, f, fd, fdb, n, nbdirs, nbdirs0)\n",
    "  USE DIFFSIZES\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "! Size of the input vector\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "! Input vector\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax0, n) :: xb\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "! Output value, product of all entries\n",
    "  REAL :: f\n",
    "  REAL, DIMENSION(nbdirsmax0) :: fb\n",
    "  REAL, DIMENSION(nbdirsmax) :: fd\n",
    "  REAL, DIMENSION(nbdirsmax0, nbdirsmax) :: fdb\n",
    "! Dummy loop index\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  INTEGER :: nd0\n",
    "  INTEGER :: nbdirs0\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      CALL PUSHREAL4(fd(nd))\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    CALL PUSHREAL4(f)\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "  fb = 0.0\n",
    "  xb = 0.0\n",
    "  DO i=n,1,-1\n",
    "    CALL POPREAL4(f)\n",
    "    DO nd0=1,nbdirs0\n",
    "      xb(nd0, i) = xb(nd0, i) + f*fb(nd0)\n",
    "      fb(nd0) = x(i)*fb(nd0)\n",
    "    END DO\n",
    "    DO nd=nbdirs,1,-1\n",
    "      CALL POPREAL4(fd(nd))\n",
    "      DO nd0=1,nbdirs0\n",
    "        xb(nd0, i) = xb(nd0, i) + fd(nd)*fdb(nd0, nd)\n",
    "        fb(nd0) = fb(nd0) + xd(nd, i)*fdb(nd0, nd)\n",
    "        fdb(nd0, nd) = x(i)*fdb(nd0, nd)\n",
    "      END DO\n",
    "    END DO\n",
    "  END DO\n",
    "  fdb = 0.0\n",
    "END SUBROUTINE SPEELPENNING_DV_BV\n",
    "```\n",
    "Note that running the code requires either `PUSHREAL4` and `POPREAL4` to be imported from Tapenade's 'ADFirstAidKit', or for these subroutines for pushing to and popping from a stack to be implemented manually.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "The *operator overloading* approach is fundamentally different to source transformation. It does not generate additional source files and instead generates derivatives *at runtime*.\n",
    "\n",
    "The general idea is to record all operations applied to variables involved in the computation so that derivatives can be computed by applying the chain rule to the derivatives of those operations.\n",
    "\n",
    "Depending on the AD tool, the user experience is something like the following:\n",
    "\n",
    "1. Mark (input) variables to compute derivatives with respect to as *independent*.\n",
    "2. Mark (output) variables to compute derivatives of as *dependent*.\n",
    "3. Call driver methods supported by the AD tool to compute derivatives.\n",
    "4. Run the program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Operator overloading\n",
    "\n",
    "The approach fits well with another 'OO': *object orientation*. In classic implementations such as [ADOL-C](https://github.com/coin-or/ADOL-C), basic types are enriched by introducing corresponding structs (derived types in Fortran) that hold both the standard (forward) value and also the gradient. For example,\n",
    "```fortran\n",
    "type adouble\n",
    "  double precision :: val\n",
    "  double precision :: grad\n",
    "end type adouble\n",
    "```\n",
    "\n",
    "In order to compute derivatives of expressions involving `adouble`s, we need to *overload* the operators that act on `double`s so that they operate on the `val` attribute and also provide code for the derivative(s). That is, we extend the definition of such operators (`+`, `-`, `*`, `/`, `**`, and others) so that they may be applied to `adoubles`s.\n",
    "\n",
    "Since forward mode evaluates eagerly (i.e., computes the derivative at the same time as the function evaluation), the `adouble` approach facilitates this. This is sometimes referred to as the *dual number approach*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Operator overloading: tape\n",
    "\n",
    "In the case of reverse mode, the dual number approach is insufficient. As mentioned previously, we need to keep track of operations from the primal code, as well as primal values when executing reverse mode.\n",
    "\n",
    "A fundamental concept related to the operator overloading AD approach is *tape*. This is the name used for the record of all the operations that have been executed. Consider the following lines of Fortran code for an example evaluation of the $f$ function considered previously:\n",
    "```fortran\n",
    "type(adouble) :: x1\n",
    "type(adouble) :: x2\n",
    "type(adouble) :: y\n",
    "\n",
    "x1 = 2.0\n",
    "x2 = 3.0\n",
    "y = x1 * x2\n",
    "```\n",
    "Conceptually, the tape would be something like:\n",
    "\n",
    "| Index | Operation      | Dependencies | Outputs |\n",
    "| ----- | -------------- | ------------ | ------- |\n",
    "| 0     | Assignment     | 2.0          | x1      |\n",
    "| 1     | Assignment     | 3.0          | x2      |\n",
    "| 2     | Multiplication | x1, x2       | y       |\n",
    "\n",
    "That is, it records each operation in order, as well as the relevant dependencies and outputs. In practice, tapes are much more sophisticated than this, but hopefully the idea is clear. The tape can also be visualised as a DAG, as illustrated in subsequent cells.\n",
    "\n",
    "Given that we know the derivatives of addition, exponentiation, and multiplication, we can *unroll* the tape to compute the desired derivative code. As an alternative to the dual number approach, forward mode can be implemented in this way.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "\n",
    "What's the difference between forward and reverse mode as far as tape unrolling is concerned?\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "In forward mode, we unroll the tape in the same order it was written (increasing index). Values for the primal code are computed eagerly as part of this unrolling.\n",
    "\n",
    "For reverse mode, we do need to first unroll the tape in the forward direction to compute primal variables. Then we unroll the tape in the *reverse* direction (decreasing index) to accumulate derivatives.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Operator overloading example in Pyadjoint\n",
    "\n",
    "We demonstrate operator overloading using Pyadjoint in the following. We define some utility functions for interrogating the `Tape` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tape():\n",
    "    \"\"\"\n",
    "    Print the current working tape entry-by-entry.\n",
    "    \"\"\"\n",
    "    tape = pyadjoint.get_working_tape()\n",
    "    print(\"Tape:\")\n",
    "    for i, block in enumerate(tape.get_blocks()):\n",
    "        print(f\"{i}: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dag(pos=None, n=4):\n",
    "    \"\"\"\n",
    "    Plot the DAG for the current working tape.\n",
    "\n",
    "    :kwarg pos: list of tuples for the node positions\n",
    "    :kwarg n: number of nodes in final graph for tracking sub-DAGs as they progress\n",
    "    \"\"\"\n",
    "    tape = pyadjoint.get_working_tape()\n",
    "    graph = tape.create_graph()\n",
    "    fig, axes = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Use the default automatic node positioning if none is specified\n",
    "    if pos is not None:\n",
    "        pos = {node: p for node, p in zip(graph.nodes, pos)}\n",
    "\n",
    "    # Plot the DAG with the truncated colormap\n",
    "    nx.draw(graph, pos, ax=axes, node_color=range(len(graph.nodes)), node_size=800, cmap=plt.cm.coolwarm)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We start by enabling annotation of the tape with `pyadjoint.continue_annotation`.\n",
    "\n",
    "Having done so, we create two `AdjFloat`s and assign them the values 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    \"\"\"f function as defined in previous examples.\"\"\"\n",
    "    return x1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable annotation of the tape\n",
    "pyadjoint.continue_annotation();\n",
    "\n",
    "# Create some AdjFloats\n",
    "x1 = pyadjoint.AdjFloat(2.0)\n",
    "x2 = pyadjoint.AdjFloat(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Let's now combine the two `AdjFloat`s using a multiplication operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x1, x2)\n",
    "\n",
    "# Interrogate the tape\n",
    "print_tape()\n",
    "plot_dag(pos=[(0, 0), (1, 0.5), (0, 1), (2, 0.5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "The tape shows an entry for the product, displaying the `float` values for `x1` and `x2`. Note that `AdjFloat` assignments are not shown on the tape in `pyadjoint`, although assignments for finite element fields are. The DAG shows four nodes: two for the input `AdjFloat`s, one for the multiplication operation, and one for the output `AdjFloat`s.\n",
    "\n",
    "Having finished the computation of interest, we can stop annotating the tape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyadjoint.pause_annotation();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "To compute derivatives, we need to specify which are the independent variables. These are declared as `Control`s, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = [pyadjoint.Control(x1), pyadjoint.Control(x2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Having marked variables as independent, we can stop annotating the tape and then compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the docstring for the compute_gradient driver\n",
    "print(\"    \\\"\\\"\\\"\\n\" + pyadjoint.compute_gradient.__doc__ + \"\\n    \\\"\\\"\\\"\\n\")\n",
    "\n",
    "# Compute the gradient of the output with respect to the inputs\n",
    "dydx = pyadjoint.compute_gradient(y, xc)\n",
    "\n",
    "print(f\"x = [x1, x2] = [{float(x1)}, {float(x2)}]\")\n",
    "print(f\"y = x1 * x2 = {float(y)}\")\n",
    "print(f\"dydx = [{float(dydx[0])}, {float(dydx[1])}] ?= [x2, x1] = [{float(x2)}, {float(x1)}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Note that we haven't specified whether to use forward or reverse mode. Pyadjoint is set up such that it will choose which approach to used, depending on whether there are more inputs or outputs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "\n",
    "We've computed first derivatives of $f$. Now let's consider its second derivatives.\n",
    "1. What would you expect the Hessian of $f$ to be?\n",
    "2. Take a look at `pyadjoint.compute_hessian.__doc__`. Compute the compressed Hessian with a single call to the `compute_hessian` driver function.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial^2f}{\\partial x_1^2}=\\frac{\\partial}{\\partial x_1}x_2=0,\n",
    "    \\quad\\frac{\\partial^2f}{\\partial x_1x_2}=\\frac{\\partial}{\\partial x_1}x_1=1,\n",
    "    \\quad\\frac{\\partial^2f}{\\partial x_2x_1}=\\frac{\\partial}{\\partial x_2}x_2=1,\n",
    "    \\quad\\frac{\\partial^2f}{\\partial x_2^2}=\\frac{\\partial}{\\partial x_2}x_1=0,\n",
    "$$\n",
    "so we expect $H(f)=\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "With the code\n",
    "```python\n",
    "# Define seed vector for second derivative\n",
    "xdd = [pyadjoint.AdjFloat(1.0), pyadjoint.AdjFloat(1.0)]\n",
    "\n",
    "# Apply driver function\n",
    "d2ydx2 = pyadjoint.compute_hessian(y, xc, xdd)\n",
    "\n",
    "print(f\"x = [x1, x2] = [{float(x1)}, {float(x2)}]\")\n",
    "print(f\"y = x1 * x2\")\n",
    "\n",
    "# Decompress Hessian before printing\n",
    "print(f\"d2ydx2 = [0.0, {float(d2ydx2[0])}; {float(d2ydx2[1])}, 0.0] ?= [0.0, 1.0; 1.0, 0.0]\")\n",
    "```\n",
    "we should get\n",
    "```default\n",
    "x = [x1, x2] = [2.0, 3.0]\n",
    "y = x1 * x2\n",
    "d2ydx2 = [0.0, 1.0; 1.0, 0.0] ?= [0.0, 1.0; 1.0, 0.0]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Source transformation vs. operator overloading\n",
    "\n",
    "We already evaluated the differences between modes (forward and reverse). How about the differences between approaches?\n",
    "\n",
    "* ST is done as a preprocessing step, whereas OO is done at runtime.\n",
    "* ST is fairly clear, whereas OO is somewhat of a 'black box' (unless you're able to inspect the tape).\n",
    "* OO's tape requires memory.\n",
    "* There are only a few ST tools, but very many OO tools! See below.\n",
    "    * LLVM: [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "    * C/C++: [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial] (about two dozen!)\n",
    "    * Fortran:[Differentia](https://github.com/Nicholaswogan/Differentia), lots of abandonware...\n",
    "    * Python: [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html), [Jax](https://github.com/jax-ml/jax), [PyADOL-C](https://github.com/b45ch1/pyadolc).\n",
    "    * Julia: Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable), [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface) ([about two dozen overall!](https://juliadiff.org/))\n",
    "    * Domain-specific: [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "    * And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "So far, we have considered AD applied to elementary operations (`+`, `-`, `*`, `/`, `**`) applied to intrinsic Fortran types like `real`s and arrays thereof. Most of the early AD tools worked in this way, tracking operations applied to real numbers and chaining together their derivatives. However, code differentiated with a source transformation tool in this manner can quickly become difficult to read for large codes. And under the operator overloading approach, the memory footprint of the tape can become very large.\n",
    "\n",
    "By raising the level of abstraction, some of these difficulties can be avoided.\n",
    "\n",
    "#### Medium-level: API calls\n",
    "\n",
    "Example:\n",
    "\n",
    "* AD in PETSc\n",
    "  * [Latest implementation](https://github.com/SciCompKL/adjoint-PETSc).\n",
    "  * (Note: [old draft implementation](https://gitlab.com/petsc/petsc/-/tree/main/src/ts/tutorials/autodiff) differentiated elementary operators.)\n",
    "\n",
    "#### High-level: operations on fields\n",
    "\n",
    "Examples:\n",
    "\n",
    "* AD in Firedrake using [Pyadjoint/dolfin-adjoint](https://www.dolfin-adjoint.org/en/latest/documentation/index.html).\n",
    "* AD in PSyclone using [PSyAD](https://github.com/stfc/PSyclone/tree/master/examples/psyad).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Showcase</b>\n",
    "\n",
    "Open and work through the following Firedrake Jupyter notebooks that were copied over.\n",
    "```default\n",
    "session2/exercises/firedrake/11-extract-adjoint-solutions.ipynb\n",
    "session2/exercises/firedrake/06-pde-constrained-optimisation.ipynb\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Goal-oriented error estimation\n",
    "\n",
    "Navigate to https://mesh-adaptation.github.io/docs/demos/point_discharge2d.py.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Recall our discussion about tape unrolling and how for reverse mode we first unroll the tape to run the primal code.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "\n",
    "Under what circumstances can we get away without an initial forward pass when running reverse mode?\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Two possible answers:\n",
    "\n",
    "If the variables involved in the forward computation had their values assigned exactly once and those values are still available in memory then we already have the forward data required for the reverse pass.*\n",
    "\n",
    "If the problem is linear in the independent variables then the reverse mode code will be *independent of* those variables. As such, there is no need to compute them.\n",
    "\n",
    "*Although your AD tool might not be aware of this or be able to make use of the information.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "In scientific programming, time-dependent problems are typically solved using a timestepping method such as the theta-method we saw in the first session. When writing code for such methods, it's common practice to overwrite the variable for the approximation at a given timestep as we progress through the steps. In such a formulation, the value is no longer in memory and needs to be recomputed.\n",
    "\n",
    "In some cases, it's possible to keep the full state on the tape at every timestep, whereby all the information required for reverse mode is available - see Figure 4.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/in_memory.png\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 4:</strong> Time-dependent adjoint problem solved without loading checkpoints. Generated using tikZ and $\\LaTeX$.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "This quickly becomes infeasible for real world problems, with the amount of memory required at each timestep taking a large chunk of the available RAM. In the extreme case where only one timestep's worth of data fits in memory, a reverse mode propagation requires checkpointing at each timestep as in Figure 5.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/load_all.png\" width=\"800\" style=\"display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "  <strong>Figure 5:</strong> Time-dependent adjoint problem solved with checkpointing at every timestep. Generated using tikZ and $\\LaTeX$.\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In intermediate cases, checkpointing can be done on the basis of some fixed frequency or in a more intelligent way using the *revolve* algorithm [(Griewank & Walther, 2000)](https://doi.org/10.1145/347837.347846)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Further applications\n",
    "\n",
    "* Sensitivity analysis. <!-- Compute gradients of outputs with respect to parameters of interest -->\n",
    "* Data assimilation. <!-- Uses gradients of cost functions involving mismatches against observations to assimilate the observations. -->\n",
    "* Uncertainty quantification. <!-- Uses Hessians -->\n",
    "* Online training in machine learning. <!-- Requires derivatives of code downstream from machine learning outputs -->\n",
    "* PDE-constrained optimisation.  <!-- Typically uses gradient-based methods -->\n",
    "* Goal-oriented error estimation and mesh adaptation. <!-- Uses adjoint solutions -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In today's session we:\n",
    "\n",
    "* Learnt about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verified the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Calculated higher order derivatives using Tapenade.\n",
    "* Tried out the *Pyadjoint* operator overloading AD tool underpinnning the Firedrake finite element library and saw showcases of more advanced AD usage.\n",
    "* Learnt about *checkpointing* and using AD to compute higher order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146160, 1976, [doi:10.1007/BF01931367](https://doi.org/10.1007/BF01931367).\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980, [doi:10.2172/5254402](https://doi.org/10.2172/5254402).\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:3554, 1992, [doi:10.1080/10556789208805505](https://doi.org/10.1080/10556789208805505).\n",
    "* A. Griewank and Andrea Walther. *Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation.* ACM Transactions on Mathematical Software (TOMS) 26.1: 19-45, 2000, [doi:10.1145/347837.347846](https://doi.org/10.1145/347837.347846).\n",
    "* J. Huckelheim, et al. *A taxonomy of automatic differentiation pitfalls*, WIREs Data Mining Knowl Discovery, 2024, [doi:10.1002/widm.1555](https://doi.org/10.1002/widm.1555)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
