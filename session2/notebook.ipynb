{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a29344e-4054-44d4-8ec3-cb5f38ead232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1ea3e-5481-4dae-98fc-ad2295a15a0a",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation\n",
    "\n",
    "[TODO: intro text]\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "* We will restrict attention to operator overloading\n",
    "* We will restrict attention to PyTorch / Torch / FTorch\n",
    "\n",
    "#### Notation\n",
    "\n",
    "[TODO: Recall derivative notation]\n",
    "\n",
    "[TODO: Recall `_d` notation]\n",
    "\n",
    "[TODO: Mention `_b` notation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29a48d-f172-4bc3-8c14-f0896eda7d68",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Reverse mode (the subject of this session) was discovered by Linnainmaa in the 1970s.\n",
    "* Speelpenning introduced the modern formulation of reverse mode in the late 1980s.\n",
    "* Griewank improved the feasibility of reverse mode in 1992 by introducing checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289132da-3e11-49dd-ac58-d85369d73c3a",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "[TODO: Jacobian Transpose vector product (Again, matrix-free)]\n",
    "\n",
    "[TODO: \"Back-propagation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c87060-bf2d-4046-9f20-240af059f767",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode demo\n",
    "\n",
    "Builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855c7416-81cf-4993-b88a-56ccd96ec19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976c026-81ef-4c3a-883c-f7e4741a2dd9",
   "metadata": {},
   "source": [
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2467c8f-2a60-4b36-9aec-341eca24c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc7695-6e90-48ae-8e1c-fefef4e3071e",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54159cd2-7c00-4f1b-aa08-b3d749c8ac23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q = tensor([-12.,  65.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Q = 3 * (a ** 3 - b * b / 3)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbdca69-ef72-49cd-9d58-4f30e32b76b7",
   "metadata": {},
   "source": [
    "[TODO: Remark on `grad_fn`]\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an external gradient. This defaults to a tensor of ones of the appropriate shape, but can be customised to account for cases where gradient values are propagated from another model or subcomponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4e651f4-057c-47a4-b264-e107412c3e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44f41b-4c5b-4dc7-aba5-344b68157a24",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8517d86d-bcb5-4f24-bdff-83c5debb6842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([36., 81.])\n",
      "b = tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a = {a.grad}\")\n",
    "print(f\"b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0225481-ef43-48cd-8d7f-5d741bc0ae80",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ed05d-5e0c-4cdd-8fe1-bc343c1dbf2a",
   "metadata": {},
   "source": [
    "##  Mathematical background on adjoint methods\n",
    "\n",
    "[TODO]\n",
    "\n",
    "[TODO: Continuous vs. discrete adjoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e3cd4-1440-4796-b68a-99e049a1ce1e",
   "metadata": {},
   "source": [
    "## Differences with forward mode\n",
    "\n",
    "* Forward mode is more appropriate if $\\#inputs\\ll\\#outputs$.\n",
    "* Reverse mode is more appropriate if $\\#inputs\\gg\\#outputs$.\n",
    "    * e.g., ODE/PDE-constrained optimisation (cost fn), ML training (loss fn), goal-oriented (qoi)\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e081b4-3ff4-4d3e-9917-fbe538b54595",
   "metadata": {},
   "source": [
    "[TODO: Simple examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dee839-c19a-4c60-ab0e-ca514ef4fd41",
   "metadata": {},
   "source": [
    "## Validation: Dot product test\n",
    "\n",
    "[TODO: ensure consistency of fwd and rev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a5507-9d2e-4586-a21e-2f658f5fd74b",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a624654-a358-48a6-973b-bf0adb90c3af",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "[TODO: Hessian]\n",
    "\n",
    "[TODO: Forward then reverse]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b4da5-bc0e-4452-a346-849f50452123",
   "metadata": {},
   "source": [
    "## Speelpenning demo\n",
    "\n",
    "[TODO: Example for computing Hessian - see e.g., https://github.com/coin-or/ADOL-C/blob/master/ADOL-C/examples/speelpenning.cpp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d68ae-fbd1-4da9-9491-010650c8bf9c",
   "metadata": {},
   "source": [
    "## Exercise: Sensitivity analysis\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d108d-cc26-4974-bdfc-26ab44d222ab",
   "metadata": {},
   "source": [
    "## Exercise: Online training ML\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90a646f-cfbe-4055-84e4-d980a9d5dfaa",
   "metadata": {},
   "source": [
    "## Further application\n",
    "\n",
    "[TODO]\n",
    "\n",
    "* Data assimilation\n",
    "* Uncertainty quantification\n",
    "* PDE-constrained optimisation\n",
    "    * Demo in Firedrake\n",
    "* Goal-oriented error estimation\n",
    "    * Demo in Firedrake\n",
    "* Perhaps showcase combined adaptation + optimisation\n",
    "    * Demo in Firedrake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d1bb5-c440-4bc3-9ae0-2e2777b492d4",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
