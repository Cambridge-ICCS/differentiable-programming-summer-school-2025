{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpltools import annotation\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation and operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Set up codespace now. (It will take a while!)</b>\n",
    "\n",
    "We will show you where to find this notebook in the repo while you wait.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* Were you interested in yesterday's session but unclear how to make use of differentiable programming in practice?\n",
    "* Have you ever wondered how neural network training actually works under the hood?\n",
    "\n",
    "<!-- Discuss optimisation of parameters in the net -->\n",
    "\n",
    "<img src=\"images/nn.svg\" width=300 style=\"margin-left:auto; margin-right:auto; clip-path: inset(0 50% 0 50%);\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 1:</strong> Neural network schematic created with <a href=\"https://alexlenail.me/NN-SVG/index.html\">https://alexlenail.me/NN-SVG/index.html</a>.</div>\n",
    "\n",
    "<br>\n",
    "Machine learning (ML) models typically have large numbers of parameters to be tuned and small numbers of outputs.\n",
    "Forward mode doesn't seem like such a great fit..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "In today's session we will:\n",
    "* Learn about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verify the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Try out the *autograd* operator overloading AD tool underpinnning PyTorch (as well as libtorch (C++) and FTorch (Fortran)).\n",
    "* Learn about *checkpointing* and using AD to compute higher order derivatives.\n",
    "* See a showcase of some more advanced use cases of AD using Firedrake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "**Recall from session 1**: This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "**Recall from session 1**: For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "**Recall from session 1**: When it comes to *forward* derivatives in code, we use the `_d` notation, which is standard in the AD literature.\n",
    "\n",
    "When it comes to *reverse* mode derivatives in code, we use the `_b` notation (for \"backward\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d658e79-4272-4db1-b3c5-fe84abe56164",
   "metadata": {},
   "source": [
    "## Brief history of reverse mode\n",
    "\n",
    "* Reverse mode (a.k.a. backpropagation) was discovered by Linnainmaa in the 1970s.\n",
    "* The terminology 'back-propagating error correction' had already been introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this.\n",
    "* Speelpenning introduced the modern formulation of reverse mode in the late 1980s.\n",
    "* Griewank improved the feasibility of reverse mode in 1992 by introducing checkpointing.\n",
    "\n",
    "<img src=\"images/jake.png\" width=500 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 2:</strong> Schematic from B. Speelpenning, <em>Compiling fast partial derivatives of functions given by algorithms</em> (1980), University of Illinois.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "> Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "> $$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    ">\n",
    "> Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    ">\n",
    "> Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Jacobian-transpose-vector product (JTVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb{R}^m.$$\n",
    "\n",
    "Given $\\mathbf{x}\\in A$ and a seed vector $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, reverse mode AD allows us to compute the *transpose action* (transposed matrix-vector product)\n",
    "$$\\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}.$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that the JTVP is well defined.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "We have $\\nabla\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{m\\times n}$, so $\\nabla\\mathbf{f}(\\mathbf{x})^T\\in\\mathbb{R}^{n\\times m}$. Since $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, the dimensions are appropriate to take the JTVP.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"background-color: rgb(236,176,146); border: 2px solid rgb(213,104,79); color: rgb(64,64,64);\">\n",
    "    \n",
    "<b>Note</b>\n",
    "Again, the computation is <em>matrix-free</em>. We don't actually need the Jacobian or its transpose when we compute this product.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Forward mode vs. reverse mode\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "* Forward mode is more appropriate if $n\\ll m$, i.e., $\\#inputs\\ll\\#outputs$.\n",
    "  * e.g., sensitivity analysis or optimisation w.r.t. a small number of parameters.\n",
    "* Reverse mode is more appropriate if $n\\gg m$, i.e., $\\#inputs\\gg\\#outputs$.\n",
    "  * e.g., ODE/PDE-constrained optimisation (cost function), machine learning training (loss function), goal-oriented error estimation (quantity of interest).\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6985ffd-0410-4a97-b882-4fcc01d1f876",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: Directed Acyclic Graph\n",
    "\n",
    "Recall the introductory example from yesterday and the DAG representations of the $f$ and $g$ functions.\n",
    "\n",
    "Recalling that\n",
    "$$f(x,y)=xy$$\n",
    "and\n",
    "$$g(z)=(\\sin(z),\\cos(z)),$$\n",
    "we have\n",
    "\n",
    "<img src=\"../../session1/images/f_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;\"><strong>Figure 3:</strong> Directed Acyclic Graph (DAG) for the $f$ function in the $f$ & $g$ example.</div>\n",
    "\n",
    "<img src=\"../../session1/images/g_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;><strong>Figure 4:</strong> Directed Acyclic Graph (DAG) for the $g$ function in the $f$ & $g$ example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fc762-4e1d-47bf-ba13-b4f9e82de7c5",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: reverse mode seed vectors\n",
    "\n",
    "In reverse mode, gradient information propagates in the opposite direction.\n",
    "\n",
    "<img src=\"images/full_reverse_dag.png\" width=400 style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<div style=\"text-align: center;><strong>Figure 5:</strong> Directed Acyclic Graph (DAG) for the composition of the functions in the $f$ & $g$ example.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: reverse mode\n",
    "\n",
    "The Fortran code for the two functions is copied in the repository at `session2/exercises/fg/f.f90` and `session2/exercises/fg/g.f90`.\n",
    "\n",
    "```fortran\n",
    "subroutine f(x, y, z)\n",
    "  implicit none\n",
    "  real, intent(in)  :: x, y\n",
    "  real, intent(out) :: z\n",
    "  z = x * y\n",
    "end subroutine f\n",
    "```\n",
    "\n",
    "```fortran\n",
    "subroutine g(z, v)\n",
    "  implicit none\n",
    "  real, intent(in)  :: z\n",
    "  real, intent(out), dimension(2) :: v\n",
    "  v = [sin(z), cos(z)]\n",
    "end subroutine g\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Run `tapenade -h` to review the options.\n",
    "2. Apply Tapenade to each of these subroutines **in reverse mode**, which will compute the JTVP for some seed vector. Inspect the output files `f_b.f90` and `g_b.f90` and check they are as you expect.\n",
    "3. (Optional) Inspect the message files `f_b.msg` and `g_b.msg`.\n",
    "\n",
    "*Note that you will need to install Java if you don't already have it installed.\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -h\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    " Builds a differentiated program.\n",
    " Usage: tapenade [options]* filenames\n",
    "  options:\n",
    "   -head, -root <proc>     set the differentiation root procedure(s)\n",
    "                           See FAQ for refined invocation syntax, e.g.\n",
    "                           independent and dependent arguments, multiple heads...\n",
    "   -tangent, -d            differentiate in forward/tangent mode (default)\n",
    "   -reverse, -b            differentiate in reverse/adjoint mode\n",
    "   -vector, -multi         turn on \"vector\" mode (i.e. multi-directional)\n",
    "   -specializeactivity <unit_names or %all%>  Allow for several activity patterns per routine\n",
    "   -primal, -p             turn off differentiation. Show pointer destinations\n",
    "   -output, -o <file>      put all generated code into a single <file>\n",
    "   -splitoutputfiles       split generated code, one file per top unit\n",
    "   -outputdirectory, -O <directory>  put all generated files in <directory> (default: .)\n",
    "   -I <includePath>        add a new search path for include files\n",
    "   -tgtvarname <str>       set extension for tangent variables  (default %d)\n",
    "   -tgtfuncname <str>      set extension for tangent procedures (default %_d)\n",
    "   -tgtmodulename <str>    set extension for tangent modules and types (default %_diff)\n",
    "   -adjvarname <str>       set extension for adjoint variables  (default %b)\n",
    "   -adjfuncname <str>      set extension for adjoint procedures (default %_b)\n",
    "   -adjmodulename <str>    set extension for adjoint modules and types (default %_diff)\n",
    "   -modulename <str>       set extension for tangent&adjoint modules and types (default %_diff)\n",
    "   -inputlanguage <lang>   language of  input files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -outputlanguage <lang>  language of output files (fortran, fortran90,\n",
    "                           fortran95, or C)\n",
    "   -ext <file>             incorporate external library description <file>\n",
    "   -nolib                  don't load standard libraries descriptions\n",
    "   -i<n>                   count <n> bytes for an integer (default -i4)\n",
    "   -r<n>                   count <n> bytes for a real (default -r4)\n",
    "   -dr<n>                  count <n> bytes for a double real (default -dr8)\n",
    "   -p<n>                   count <n> bytes for a pointer (default -p8)\n",
    "   -fixinterface           don't use activity to filter user-given (in)dependent vars\n",
    "   -noinclude              inline include files\n",
    "   -debugTGT               insert instructions for debugging tangent mode\n",
    "   -debugADJ               insert instructions for debugging adjoint mode\n",
    "   -tracelevel <n>         set the level of detail of trace milestones\n",
    "   -msglevel <n>           set the level of detail of error messages\n",
    "   -msginfile              insert error messages in output files\n",
    "   -dump <file>            write a dump <file>\n",
    "   -html                   display results in a web browser\n",
    "   -nooptim <str>          turn off optimization <str> (in {activity, difftypes,\n",
    "                           diffarguments, stripprimalmodules, spareinit, splitdiff, \n",
    "                           mergediff, saveonlyused, tbr, snapshot, diffliveness,\n",
    "                           deadcontrol, recomputeintermediates,\n",
    "                           everyoptim}\n",
    "   -version                display Tapenade version information\n",
    " Report bugs to <tapenade@inria.fr>.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/fg\n",
    "$ ls\n",
    "f.f90 g.f90\n",
    "$ tapenade -b f.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine f as default differentiation root\n",
    "@@ Created ./f_b.f90 \n",
    "@@ Created ./f_b.msg\n",
    "$ cat f_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of f in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: x y z\n",
    "!   with respect to varying inputs: x y z\n",
    "!   RW status of diff variables: x:incr y:incr z:in-zero\n",
    "SUBROUTINE F_B(x, xb, y, yb, z, zb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: x, y\n",
    "  REAL :: xb, yb\n",
    "  REAL :: z\n",
    "  REAL :: zb\n",
    "  xb = xb + y*zb\n",
    "  yb = yb + x*zb\n",
    "  zb = 0.0\n",
    "END SUBROUTINE F_B\n",
    "```\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ tapenade tapenade -b g.f90 \n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "Command: Took subroutine g as default differentiation root\n",
    "@@ Created ./g_b.f90 \n",
    "@@ Created ./g_b.msg\n",
    "$ cat g_b.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of g in reverse (adjoint) mode:\n",
    "!   gradient     of useful results: v z\n",
    "!   with respect to varying inputs: v z\n",
    "!   RW status of diff variables: v:in-zero z:incr\n",
    "SUBROUTINE G_B(z, zb, v, vb)\n",
    "  IMPLICIT NONE\n",
    "  REAL, INTENT(IN) :: z\n",
    "  REAL :: zb\n",
    "  REAL, DIMENSION(2) :: v\n",
    "  REAL, DIMENSION(2) :: vb\n",
    "  INTRINSIC COS\n",
    "  INTRINSIC SIN\n",
    "  zb = zb + COS(z)*vb(1) - SIN(z)*vb(2)\n",
    "  vb = 0.0\n",
    "END SUBROUTINE G_B\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Verification: the dot product test\n",
    "\n",
    "We have approaches for computing derivatives with both forward mode and reverse mode. But how do we know the outputs are consistent? This can be verified using the so-called *dot product test*.\n",
    "\n",
    "In the dot product test, we define a quantity $\\psi:=\\bar{\\mathbf{y}}\\dot{\\mathbf{y}}$, where $\\dot{\\mathbf{y}}=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$ is the output of forward mode and check that $\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}$, where $\\bar{\\mathbf{x}}^T=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}$ is the output of reverse mode.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "    \n",
    "Prove that we do expect $\\psi=\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}$.\n",
    "\n",
    "<b>Solution</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "$$\n",
    "    \\psi:=\\bar{\\mathbf{y}}\\dot{\\mathbf{y}}\n",
    "    =\\bar{\\mathbf{y}}(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}})\n",
    "    =(\\bar{\\mathbf{y}}\\nabla\\mathbf{f}(\\mathbf{x}))\\dot{\\mathbf{x}}\n",
    "    =\\bar{\\mathbf{x}}\\dot{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## $f$ \\& $g$ example: dot product test\n",
    "\n",
    "We've already computed forward mode and reverse mode derivatives for the function $f$ using Tapenade. We can run a dot product test using the code in `session2/exercises/fg/dot_product_test.f90`.\n",
    "\n",
    "```fortran\n",
    "include \"f_d.f90\"\n",
    "include \"f_b.f90\"\n",
    "\n",
    "! Program for verifying the consistency of the forward mode and reverse mode derivatives of the\n",
    "! function f.\n",
    "program dot_product_test\n",
    "  implicit none\n",
    "\n",
    "  real :: x, y   ! Primal inputs\n",
    "  real :: xd, yd ! Forward mode seeds\n",
    "  real :: xb, yb ! Reverse mode derivatives\n",
    "  real :: z      ! Primal output\n",
    "  real :: zd     ! Forward mode derivative\n",
    "  real :: zb     ! Reverse mode seed\n",
    "\n",
    "  real :: result1                 ! LHS of dot product test\n",
    "  real :: result2                 ! RHS of dot product test\n",
    "  real, parameter :: atol = 1e-05 ! Absolute tolerance for the dot product test\n",
    "\n",
    "  ! Set arbitrary primal input\n",
    "  x = 1.2\n",
    "  y = -2.3\n",
    "\n",
    "  ! Call forward mode with some arbitrary seeds\n",
    "  xd = 4.2\n",
    "  yd = -0.7\n",
    "  call f_d(x, xd, y, yd, z, zd)\n",
    "\n",
    "  ! Choose a seed for reverse mode and evaluate the first result\n",
    "  zb = 3.0\n",
    "  result1 = zd * zb\n",
    "\n",
    "  ! Call reverse mode and evaluate the second result\n",
    "  xb = 0.0\n",
    "  yb = 0.0\n",
    "  call f_b(x, xb, y, yb, z, zb)\n",
    "  result2 = xd * xb + yd * yb\n",
    "\n",
    "  ! Check the two results match within the prespecified tolerance\n",
    "  if (abs(result1 - result2) < atol) then\n",
    "    write(unit=6, fmt=\"('PASS')\")\n",
    "  else\n",
    "    write(unit=6, fmt=\"('FAIL with atol=',e10.4)\") atol\n",
    "  end if\n",
    "\n",
    "end program dot_product_test\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "    \n",
    "1. Build and run the `dot_product_test` program.\n",
    "2. Why does the test fail if the `result1` assignment is moved to after the call to `f_b`?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ cd exercises/fg\n",
    "$ gfortran dot_product_test.f90 -o dot_product_test\n",
    "$ ./dot_product_test\n",
    "PASS\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Because `zb` gets reset to zero by `f_b`. This is done by default because it's almost always the Right Thing to do.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "For seed vectors $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$ and $\\bar{\\mathbf{y}}\\in\\mathbb{R}^m$, forward mode and reverse mode compute\n",
    "\n",
    "$$\n",
    "    \\text{JVP}(\\mathbf{f},\\mathbf{x},\\dot{\\mathbf{x}}):=\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}\n",
    "    \\quad\\text{and}\\quad\n",
    "    \\text{JTVP}(\\mathbf{f},\\mathbf{x},\\bar{\\mathbf{y}}):=\\nabla\\mathbf{f}(\\mathbf{x})^T\\bar{\\mathbf{y}}\n",
    "$$\n",
    "respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "    \n",
    "What are two ways we can we use these to compute the Hessian of $f$?\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply forward mode to compute the gradient of *this* (i.e., apply forward mode to the *forward mode derivative code*). Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Given a seed vector $\\dot{\\mathbf{x}}$, first apply forward mode to compute $\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}$. Then apply reverse mode to compute the gradient of *this* (i.e., apply reverse mode to the *forward mode derivative code*). That is, $(\\nabla(\\nabla\\mathbf{f}(\\mathbf{x})\\dot{\\mathbf{x}}))^T\\bar{\\mathbf{y}}=\\dot{\\mathbf{x}}^T\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})\\bar{\\mathbf{y}}$. Here the Hessian $\\mathbf{H}(\\mathbf{f}):=\\nabla^T\\nabla\\mathbf{f}(\\mathbf{x})$ is symmetric and so the two applications give the Hessian-vector product with the seed. Use vector mode (preferably with compression!) to get the full Hessian.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Speelpenning example\n",
    "\n",
    "We compute the Hessian for the classic test case introduced by Speelpenning, which amounts to a reduction of a vector using the product:\n",
    "$$\n",
    "    f(\\mathbf{x})=\\prod_{i=1}^nx_i.\n",
    "$$\n",
    "This test case is of interest because its Hessian is dense.\n",
    "\n",
    "The Speelpenning function is implemented as a Fortran subroutine in `session2/exercises/speelpenning/speelpenning.f90`:\n",
    "```fortran\n",
    "! Classic 'speelpenning' test function, which computes the product of all entries of a vector\n",
    "subroutine speelpenning(x, f, n)\n",
    "  implicit none\n",
    "\n",
    "  integer, intent(in) :: n            ! Size of the input vector\n",
    "  real, dimension(n), intent(in) :: x ! Input vector\n",
    "  real, intent(out) :: f              ! Output value, product of all entries\n",
    "  integer :: i                        ! Dummy loop index\n",
    "\n",
    "  f = 1.0\n",
    "  do i = 1, n\n",
    "    f = f * x(i)\n",
    "  end do\n",
    "end subroutine speelpenning\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercises</b>\n",
    "\n",
    "1. Compute the Hessian of the `speelpenning` subroutine using Tapenade.*\n",
    "2. Build and run the `view_hessian` program in `session2/exercises/speelpenning/view_hessian.f90`.\n",
    "3. Derive the expected Hessian and convince yourself that the output is as you expect.\n",
    "\n",
    "*Hint: One of the two approaches is much easier than the other!\n",
    "\n",
    "<b>Solution 1</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "```sh\n",
    "$ tapenade -vector speelpenning.f90\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Command: Took subroutine speelpenning as default differentiation root\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv.f90 \n",
    "@@ Created ./speelpenning_dv.msg\n",
    "$ cat speelpenning_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV(x, xd, f, fd, n, nbdirs)\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV\n",
    "```\n",
    "We need to define the `diffsizes` module. Create `diffsizes.f90` containing\n",
    "```fortran\n",
    "module diffsizes\n",
    "  implicit none\n",
    "  integer, parameter :: nbdirsmax = 7\n",
    "end module diffsizes\n",
    "```\n",
    "\n",
    "Inline this with the VJP code:\n",
    "```sh\n",
    "$ cat diffsizes.f90 speelpenning_dv.f90 > tmp.f90\n",
    "```\n",
    "\n",
    "Then we can apply vector forward mode:\n",
    "```sh\n",
    "$ tapenade -vector -head \"speelpenning_dv(fd)/(x)\" tmp.f90 -o speelpenning_dv\n",
    "Tapenade 3.16 (develop) - 23 Apr 2025 13:39 - Java 21.0.7 Linux\n",
    "@@ TAPENADE_HOME=/home/joe/software/tools/tapenade_3.16/bin/..\n",
    "@@ Options:  multiDirectional\n",
    "Diff-liveness analysis turned off\n",
    "@@ Created ./speelpenning_dv_dv.f90 \n",
    "@@ Created ./speelpenning_dv_dv.msg\n",
    "$ cat speelpenning_dv_dv.f90\n",
    "```\n",
    "gives\n",
    "```fortran\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning_dv in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: fd\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: x:in fd:out\n",
    "!        Generated by TAPENADE     (INRIA, Ecuador team)\n",
    "!  Tapenade 3.16 (develop) - 23 Apr 2025 13:39\n",
    "!\n",
    "!  Differentiation of speelpenning in forward (tangent) mode (with options multiDirectional):\n",
    "!   variations   of useful results: f\n",
    "!   with respect to varying inputs: x\n",
    "!   RW status of diff variables: f:out x:in\n",
    "SUBROUTINE SPEELPENNING_DV_DV(x, xd0, xd, f, fd, fdd, n, nbdirs, nbdirs0&\n",
    "&)\n",
    "  USE DIFFSIZES\n",
    "  USE DIFFSIZES\n",
    "!  Hint: nbdirsmax0 should be the maximum number of differentiation directions\n",
    "  IMPLICIT NONE\n",
    "  INTEGER, INTENT(IN) :: n\n",
    "  REAL, DIMENSION(n), INTENT(IN) :: x\n",
    "  REAL, DIMENSION(nbdirsmax0, n), INTENT(IN) :: xd0\n",
    "  REAL, DIMENSION(nbdirsmax, n), INTENT(IN) :: xd\n",
    "  REAL, INTENT(OUT) :: f\n",
    "  REAL, DIMENSION(nbdirsmax0) :: fd0\n",
    "  REAL, DIMENSION(nbdirsmax), INTENT(OUT) :: fd\n",
    "  REAL, DIMENSION(nbdirsmax0, nbdirsmax), INTENT(OUT) :: fdd\n",
    "  INTEGER :: i\n",
    "  INTEGER :: nd\n",
    "  INTEGER :: nbdirs\n",
    "  INTEGER :: nd0\n",
    "  INTEGER :: nbdirs0\n",
    "  f = 1.0\n",
    "  fd = 0.0\n",
    "  fd0 = 0.0\n",
    "  fdd = 0.0\n",
    "  DO i=1,n\n",
    "    DO nd=1,nbdirs\n",
    "      DO nd0=1,nbdirs0\n",
    "        fdd(nd0, nd) = fd(nd)*xd0(nd0, i) + x(i)*fdd(nd0, nd) + xd(nd, i&\n",
    "&         )*fd0(nd0)\n",
    "      END DO\n",
    "      fd(nd) = x(i)*fd(nd) + f*xd(nd, i)\n",
    "    END DO\n",
    "    DO nd0=1,nbdirs0\n",
    "      fd0(nd0) = x(i)*fd0(nd0) + f*xd0(nd0, i)\n",
    "    END DO\n",
    "    f = f*x(i)\n",
    "  END DO\n",
    "END SUBROUTINE SPEELPENNING_DV_DV\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 2</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "Running\n",
    "```sh\n",
    "$ cd session2/exercises/speelpenning\n",
    "$ gfortran view_hessian.f90 -o view_hessian\n",
    "$ ./view_hessian\n",
    "```\n",
    "should give the output\n",
    "```default\n",
    "   0.0 2520.0 1680.0 1260.0 1008.0  840.0  720.0\n",
    "2520.0    0.0  840.0  630.0  504.0  420.0  360.0\n",
    "1680.0  840.0    0.0  420.0  336.0  280.0  240.0\n",
    "1260.0  630.0  420.0    0.0  252.0  210.0  180.0\n",
    "1008.0  504.0  336.0  252.0    0.0  168.0  144.0\n",
    " 840.0  420.0  280.0  210.0  168.0    0.0  120.0\n",
    " 720.0  360.0  240.0  180.0  144.0  120.0    0.0\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "<b>Solution 3</b>\n",
    "    \n",
    "<details>\n",
    "\n",
    "It makes sense that the diagonal entries are zero. The matrix is symmetric, as expected.\n",
    "The $(i,j)^{th}$ entry of the Hessian is given by\n",
    "$$\n",
    "\\frac{\\partial^2}{\\partial x_i\\partial x_j}\\prod_{k=1}^nx_k\n",
    "=\\frac{\\partial}{\\partial x_i}\\prod_{k=1,\\,j\\neq k}^nx_k\n",
    "=\\prod_{k=1,\\,j\\neq k,\\,j\\neq i}^nx_k\n",
    "$$\n",
    "As such, the $(6,7)^{th}$ and $(7,6)^{th}$ entries are both \n",
    "$$120=5!=\\prod_{i=1}^5x_i=\\frac{\\partial^2}{\\partial x_6\\partial x_7}\\prod_{i=1}^7x_i.$$\n",
    "Further checks left as an exercise to the reader.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Solution for reverse-then-forward approach.</b>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "The *operator overloading* approach is fundamentally different to source transformation. It does not generate additional source files and instead generates derivatives *at runtime*.\n",
    "\n",
    "The general idea is to record all operations applied to variables involved in the computation so that derivatives can be computed by applying the chain rule to the derivatives of those operations.\n",
    "\n",
    "Depending on the AD tool, the user experience is something like the following:\n",
    "1. Mark (input) variables to compute derivatives with respect to as *independent*.\n",
    "2. Mark (output) variables to compute derivatives of as *dependent*.\n",
    "3. Run the program.\n",
    "4. Call driver methods supported by the AD tool to compute derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Operator overloading\n",
    "\n",
    "The approach fits well with another 'OO': *object orientation*. In classic implementations such as [ADOL-C](https://github.com/coin-or/ADOL-C), basic types are enriched by introducing corresponding structs (derived types in Fortran) that hold both the standard (forward) value and also the gradient. For example,\n",
    "```fortran\n",
    "type adouble\n",
    "  double precision :: val\n",
    "  double precision :: grad\n",
    "end type adouble\n",
    "```\n",
    "\n",
    "In order to compute derivatives of expressions involving `adouble`s, we need to overload the operators that act on `double`s so that they operate on the `val` attribute and also provide code for the derivative(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Operator overloading: tape\n",
    "\n",
    "A fundamental concept related to the operator overloading AD approach is *tape*. This is the name used for the record of all the operations that have been executed. Consider the following lines of Python code:\n",
    "```python\n",
    "a = 1.0\n",
    "b = 2.0\n",
    "c = a + b\n",
    "d = c ** 2\n",
    "e = 3 * d\n",
    "```\n",
    "Conceptually, the tape would be something like:\n",
    "\n",
    "| Index | Operation      | Input 1 | Input 2 | Output |\n",
    "| ----- | -------------- | ------- | ------- | ------ |\n",
    "| 0     | Addition       | a       | b       | c      |\n",
    "| 1     | Exponent       | c       | 2       | d      |\n",
    "| 2     | Multiplication | 3       | d       | e      |\n",
    "\n",
    "That is, it records each (binary) operation in order, as well as the relevant inputs and outputs. In practice, tapes are much more sophisticated than this, but hopefully the idea is clear.\n",
    "\n",
    "Given that we know the derivatives of addition, exponentiation, and multiplication, we can *unroll* the tape to compute the desired derivative code.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "\n",
    "What's the difference between forward and reverse mode as far as tape unrolling is concerned?\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "In forward mode, we unroll the tape in the same order it was written (increasing index). Values for the primal code are computed eagerly as part of this unrolling.\n",
    "\n",
    "For reverse mode, we do need to first unroll the tape in the forward direction to compute primal variables. Then we unroll the tape in the *reverse* direction (decreasing index) to accumulate derivatives.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Source transformation vs. operator overloading\n",
    "\n",
    "We already evaluated the differences between modes (forward and reverse). How about the differences between approaches?\n",
    "\n",
    "* ST is done as a preprocessing step, whereas OO is done at runtime.\n",
    "* ST is fairly clear, whereas OO is somewhat of a 'black box' (unless you're able to inspect the tape).\n",
    "* OO's tape requires memory.\n",
    "* There are only a few ST tools, but very many OO tools! See below.\n",
    "    * LLVM: [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "    * C/C++: [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial] (about two dozen!)\n",
    "    * Fortran:[Differentia](https://github.com/Nicholaswogan/Differentia), lots of abandonware...\n",
    "    * Python: [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html), [Jax](https://github.com/jax-ml/jax), [PyADOL-C](https://github.com/b45ch1/pyadolc).\n",
    "    * Julia: Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable), [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface) ([about two dozen overall!](https://juliadiff.org/))\n",
    "    * Domain-specific: [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "    * And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode example\n",
    "\n",
    "This example builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html.\n",
    "\n",
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"a = {a}\")\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)\n",
    "print(f\"b = {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors and print the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_math_expression(a, b):\n",
    "    return 3 * (a ** 3 - b * b / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = some_math_expression(a, b)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Note that `Q` is displayed as having a `grad_fn` attribute. This corresponds to the last operation that was applied.\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an *external gradient*. This is just another term for the seed vector. For the purposes of this example, it's sufficient to use a seed vector of ones.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "\n",
    "Why is it sufficient to use a seed vector of ones for this problem?\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Because the effect of the inputs on the output are independent. (Recall compressed Jacobian computation for sparse AD.)\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dQda = a.grad\n",
    "dQdb = b.grad\n",
    "print(f\"dQda = {dQda}\")\n",
    "print(f\"dQdb = {dQdb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode example: exercises\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial Q}{\\partial a}=\\frac{\\partial}{\\partial a}3(a^3-b^2/3)=9a^2,\n",
    "    \\quad\\frac{\\partial Q}{\\partial b}=\\frac{\\partial}{\\partial b}3(a^3-b^2/3)=-2b.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "    9a^2=9\\begin{bmatrix}2 & 3\\end{bmatrix}^2=\\begin{bmatrix}36 & 81\\end{bmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    -2b=-2\\begin{bmatrix}6 & 4\\end{bmatrix}=\\begin{bmatrix}-12 & -8\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>\n",
    "\n",
    "Below is the code for the Taylor test for $\\frac{\\partial Q}{\\partial a}$.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Exercise</b>\n",
    "\n",
    "Copy the cell and modify it to also perform the Taylor test for $\\frac{\\partial Q}{\\partial b}$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "```python\n",
    "# Run the Taylor test over several spacing values\n",
    "spacings = [1.0, 0.1, 0.01, 0.001]\n",
    "errors = []\n",
    "for h in spacings:\n",
    "    error = torch.linalg.norm(some_math_expression(a, b + h) - some_math_expression(a, b) - h * dQdb)\n",
    "    errors.append(error.detach().numpy())\n",
    "\n",
    "# Plot the solution, demonstrating that the expected quadratic convergence is achieved\n",
    "fig, axes = plt.subplots()\n",
    "axes.loglog(spacings, errors, \"--x\")\n",
    "annotation.slope_marker((1e-2, 1e-2), 2, ax=axes, invert=True)\n",
    "axes.grid()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Taylor test over several spacing values\n",
    "spacings = [1.0, 0.1, 0.01, 0.001]\n",
    "errors = []\n",
    "for h in spacings:\n",
    "    error = torch.linalg.norm(some_math_expression(a + h, b) - some_math_expression(a, b) - h * dQda)\n",
    "    errors.append(error.detach().numpy())\n",
    "\n",
    "# Plot the solution, demonstrating that the expected quadratic convergence is achieved\n",
    "fig, axes = plt.subplots()\n",
    "axes.loglog(spacings, errors, \"--x\")\n",
    "axes.set_xlabel(r\"$h$ spacing\")\n",
    "axes.set_ylabel(r\"$\\ell_2$ error\")\n",
    "annotation.slope_marker((1e-2, 1e-2), 2, ax=axes, invert=True)\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 'Training' with backpropagation\n",
    "\n",
    "Let's consider a simple optimisation example using backpropagation in PyTorch.\n",
    "\n",
    "Suppose we have an equation,\n",
    "$$\\mathbf{f}(\\mathbf{x};\\mathbf{a})=\\mathbf{b}\\in\\mathbb{R}^4,$$\n",
    "to be solved for $\\mathbf{x}\\in\\mathbb{R}^4$, which is parametrised by some vector $\\mathbf{a}\\in\\mathbb{R}^4$.\n",
    "\n",
    "In particular, consider the function\n",
    "$$\\mathbf{f}(\\mathbf{x};\\mathbf{a})=\\mathbf{a}\\bullet\\mathbf{x}\\equiv\\begin{bmatrix}a_1x_1\\\\a_2x_2\\\\a_3x_3\\\\a_4x_4\\end{bmatrix}.$$\n",
    "Using the notation\n",
    "$\\mathbf{f}(\\mathbf{x};\\mathbf{a})=\\begin{bmatrix}f_1 & f_2 & f_3 & f_4\\end{bmatrix}^T,$ we have\n",
    "$$f_i=a_ix_i,\\quad\\forall i=1,2,3,4.$$\n",
    "\n",
    "Given the loss function $\\ell:\\mathbb{R}^4\\rightarrow[0,\\infty)$ given by the mean-square-error $\\ell(\\mathbf{a})=\\overline{(\\mathbf{f}(\\mathbf{x};\\mathbf{a})-\\mathbf{b})^2}$ and the RHS $\\mathbf{b}=\\begin{bmatrix}1,2,3,4\\end{bmatrix}^T$, we seek to solve the optimisation problem\n",
    "$$\\min_{\\mathbf{a}\\in\\mathbb{R}^4}\\ell(\\mathbf{a}).$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Given that $\\mathbf{x}:=\\begin{bmatrix}1,1,1,1\\end{bmatrix}^T$, convince yourself that the optimal solution is given by $\\mathbf{a}=\\begin{bmatrix}1,2,3,4\\end{bmatrix}^T$.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "We have $f_i=x_ia_i=1\\times a_i=a_i,\\forall i=1,2,3,4$.\n",
    "\n",
    "The loss function is minimised (as zero) when $\\mathbf{f}(\\mathbf{x};\\mathbf{a})=\\mathbf{b}$, whereby $a_i=b_i=i,\\forall i=1,2,3,4$.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596d04a-ff9f-4988-abc9-1c017bd3862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define:\n",
    "#  - the input as as a vector of ones,\n",
    "#  - the target as a vector where each element is the index value,\n",
    "#  - a tensor to transform from input to target by elementwise multiplication\n",
    "#    initialised as a vector of ones\n",
    "# This is a contrived example, but provides a simple demo of optimizer functionality\n",
    "input_vec = torch.ones(4)\n",
    "target_vec = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "scaling_tensor = torch.ones(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2adc87-576e-4e81-8acf-c466a0a862e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer as torch's stochastic gradient descent (SGD)\n",
    "# The parameters to tune will be the values of `tensor`, and we also set a learning rate\n",
    "# Since this is a simple elemetwise example we can get away with a large learning rate\n",
    "optimizer = torch.optim.SGD([scaling_tensor], lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5faf57-91b7-4926-9845-26fce2fea333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# Run n_iter times printing every n_print steps\n",
    "maxiter = 15\n",
    "loss_progress = []\n",
    "for epoch in range(maxiter + 1):\n",
    "    # Zero any previously stored gradients ready for a new iteration\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: multiply the input of ones by the tensor (elementwise)\n",
    "    output = input_vec * scaling_tensor\n",
    "\n",
    "    # Create a loss tensor as computed mean square error (MSE) between target and input\n",
    "    # Then perform backward step on loss to propogate gradients using autograd\n",
    "    #\n",
    "    # We could use the following 2 lines to do this by explicitly specifying a\n",
    "    # gradient of ones to start the process:\n",
    "    # loss = ((output - target) ** 2) / 4.0\n",
    "    # loss.backward(gradient=torch.ones(4))\n",
    "    #\n",
    "    # However, we can avoid explicitly passing an initial gradient and instead do this\n",
    "    # implicitly by aggregating the loss vector into a scalar value:\n",
    "    loss = ((output - target_vec) ** 2).mean()\n",
    "    loss.backward()\n",
    "    loss_progress.append(float(loss))\n",
    "\n",
    "    # Step the optimizer to update the values in `tensor`\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"========================\")\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"\\tOutput: {output}\")\n",
    "    print(f\"\\tloss: {loss}\")\n",
    "    print(f\"\\ttensor gradient: {scaling_tensor.grad}\")\n",
    "    print(f\"\\tscaling_tensor: {scaling_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e257e-035e-4641-b8b4-a5dd26ece517",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "axes.loglog(np.arange(1, len(loss_progress)+1), loss_progress, \"--x\")\n",
    "axes.set_xlabel(\"Epoch\")\n",
    "axes.set_ylabel(\"Loss\")\n",
    "axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Recall our discussion about tape unrolling and how for reverse mode we first unroll the tape to run the primal code.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Question</b>\n",
    "\n",
    "Under what circumstances can we get away without an initial forward pass when running reverse mode?\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "\n",
    "Two possible answers:\n",
    "\n",
    "If the variables involved in the forward computation had their values assigned exactly once and those values are still available in memory then we already have the forward data required for the reverse pass.*\n",
    "\n",
    "If the problem is linear in the independent variables then the reverse mode code will be *independent of* those variables. As such, there is no need to compute them.\n",
    "\n",
    "*Although your AD tool might not be aware of this or be able to make use of the information.\n",
    "\n",
    "</details>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "In scientific programming, time-dependent problems are typically solved using a timestepping method such as the theta-method we saw in the first session. When writing code for such methods, it's common practice to overwrite the variable for the approximation at a given timestep as we progress through the steps. In such a formulation, the value is no longer in memory and needs to be recomputed.\n",
    "\n",
    "While we could rewrite timestepping methods to use different variables for the approximation at each timestep, this ends up requiring an infeasible amount of memory for large-scale problems.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: sketch out the three main approaches with diagrams: 1. checkpoint everything; 2. recompute everything; 3. revolve.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Further applications\n",
    "\n",
    "* Sensitivity analysis. <!-- Compute gradients of outputs with respect to parameters of interest -->\n",
    "* Data assimilation. <!-- Uses gradients of cost functions involving mismatches against observations to assimilate the observations. -->\n",
    "* Uncertainty quantification. <!-- Uses Hessians -->\n",
    "* Online training in machine learning. <!-- Requires derivatives of code downstream from machine learning outputs -->\n",
    "* PDE-constrained optimisation.  <!-- Typically uses gradient-based methods -->\n",
    "* Goal-oriented error estimation and mesh adaptation. <!-- Uses adjoint solutions -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Levels of abstraction\n",
    "\n",
    "So far, we have considered AD applied to elementary operations (`+`, `-`, `*`, `/`, `**`) applied to intrinsic Fortran types like `real`s and arrays thereof. Most of the early AD tools worked in this way, tracking operations applied to real numbers and chaining together their derivatives. However, code differentiated with a source transformation tool in this manner can quickly become difficult to read for large codes. And under the operator overloading approach, the memory footprint of the tape can become very large.\n",
    "\n",
    "By raising the level of abstraction, some of these difficulties can be avoided.\n",
    "\n",
    "#### Medium-level: API calls\n",
    "\n",
    "Example:\n",
    "* AD in PETSc\n",
    "  * [Latest implementation](https://github.com/SciCompKL/adjoint-PETSc).\n",
    "  * (Note: [old draft implementation](https://gitlab.com/petsc/petsc/-/tree/main/src/ts/tutorials/autodiff) differentiated elementary operators.)\n",
    "\n",
    "#### High-level: operations on fields\n",
    "\n",
    "Examples:\n",
    "* AD in Firedrake using [Pyadjoint/dolfin-adjoint](https://www.dolfin-adjoint.org/en/latest/documentation/index.html).\n",
    "* AD in PSyclone using [PSyAD](https://github.com/stfc/PSyclone/tree/master/examples/psyad).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: rgb(195,223,220); border: 2px solid rgb(157,204,199); color: rgb(0,100,100);\">\n",
    "\n",
    "<b>Showcase</b>\n",
    "\n",
    "Open and work through the following Firedrake Jupyter notebooks that were copied over.\n",
    "```default\n",
    "session2/exercises/firedrake/11-extract-adjoint-solutions.ipynb\n",
    "session2/exercises/firedrake/06-pde-constrained-optimisation.ipynb\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Goal-oriented error estimation\n",
    "\n",
    "Navigate to https://mesh-adaptation.github.io/docs/demos/point_discharge2d.py.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In today's session we:\n",
    "* Learnt about *reverse mode* and the *operator overloading* approach, comparing them with forward mode and source transformation, respectively.\n",
    "* Verified the consistency of code generated by Tapenade under forward and reverse mode using the *dot product test*.\n",
    "* Tried out the *autograd* operator overloading AD tool underpinnning PyTorch (as well as libtorch (C++) and FTorch (Fortran)).\n",
    "* Learnt about *checkpointing* and using AD to compute higher order derivatives.\n",
    "* Saw a showcase of some more advanced use cases of AD using Firedrake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992.\n",
    "* A. Griewank and Andrea Walther. *Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation.* ACM Transactions on Mathematical Software (TOMS) 26.1: 19-45, 2000.\n",
    "* J. Huckelheim, et al. *A taxonomy of automatic differentiation pitfalls*, WIREs Data Mining Knowl Discovery, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e5e71f-e90c-4af8-87ab-97d72e93483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
