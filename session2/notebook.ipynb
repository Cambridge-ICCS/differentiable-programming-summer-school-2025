{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation and operator overloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1580a-b086-4762-86a8-512fe9bfeab7",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "**Recall from session 1**: This course introduces the concept of *differentiable programming*, a.k.a. *automatic differentiation (AD)*, or *algorithmic differentiation*. We will use the acronym AD henceforth.\n",
    "\n",
    "#### Notation\n",
    "\n",
    "**Recall from session 1**: For a differentiable *mathematical* function $f:A\\rightarrow\\mathbb{R}$ with scalar input (i.e., a single value) from $A\\subseteq\\mathbb{R}$, we make use of both the Lagrange notation $f'(x)$ and Leibniz notation $\\frac{\\mathrm{d}f}{\\mathrm{d}x}$ for its derivative.\n",
    "We **do not** use the physics notation for derivatives, so if you ever see (e.g.) $\\dot{x}$ then this is just a variable name, not the derivative of $x$.\n",
    "\n",
    "**Recall from session 1**: When it comes to *forward* derivatives in code, we use the `_d` notation, which is standard in the AD literature.\n",
    "\n",
    "When it comes to *reverse* mode derivatives in code, we use the `_b` notation (for \"backward\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d986e-8ed9-46f0-bf2b-5d2b71e6c3bb",
   "metadata": {},
   "source": [
    "## Recall: directional derivative, a.k.a. Jacobian-vector product (JVP)\n",
    "\n",
    "> Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "> $$\\mathbf{f}:A\\rightarrow\\mathbb R^m.$$\n",
    ">\n",
    "> Given input $\\mathbf{x}\\in A$ and a *seed vector* $\\dot{\\mathbf{x}}\\in\\mathbb{R}^n$, forward mode AD allows us to compute the *action* (matrix-vector product)\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})\\,\\dot{\\mathbf{x}}.$$\n",
    "Again, think of the seed vector as being an input from outside of the part of the program being differentiated.\n",
    ">\n",
    "> Here $\\nabla\\mathbf{f}$ is referred to as the *Jacobian* for the map, so the above is known as a *Jacobian-vector product (JVP)*.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "The computation is <em>matrix-free</em>. We don't actually need the Jacobian when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8b497-3e5a-4777-98db-68da526f1257",
   "metadata": {},
   "source": [
    "## Jacobian-transpose-vector product (JTVP)\n",
    "\n",
    "Consider a vector-valued function $\\mathbf{f}$ mapping from a subspace $A\\subseteq\\mathbb{R}^n$ into $\\mathbb{R}^m$, for some $m,n\\in\\mathbb{N}$:\n",
    "$$\\mathbf{f}:A\\rightarrow\\mathbb{R}^m.$$\n",
    "\n",
    "Given $\\mathbf{x}\\in A$ and a seed vector $\\dot{\\mathbf{y}}\\in\\mathbb{R}^m$, reverse mode AD allows us to compute the *transpose action* (transposed matrix-vector product)\n",
    "$$\\nabla\\mathbf{f}(\\mathbf{x})^T\\dot{\\mathbf{y}}.$$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional exercise</b>\n",
    "\n",
    "Convince yourself that the JTVP is well defined.\n",
    "\n",
    "<b>Solution</b>\n",
    "\n",
    "<details>\n",
    "We have $\\nabla\\mathbf{f}(\\mathbf{x})\\in\\mathbb{R}^{m\\times n}$, so $\\nabla\\mathbf{f}(\\mathbf{x})^T\\in\\mathbb{R}^{n\\times m}$. Since $\\dot{\\mathbf{y}}\\in\\mathbb{R}^m, the dimensions are appropriate to take the JTVP.\n",
    "</details>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note</b>\n",
    "Again, the computation is <em>matrix-free</em>. We don't actually need the Jacobian or its transpose when we compute this product.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbc96e-e069-41f3-ae52-0d411b3e570c",
   "metadata": {},
   "source": [
    "## Reverse mode: differences with forward mode\n",
    "\n",
    "* Forward mode is more appropriate if $\\#inputs\\ll\\#outputs$.\n",
    "* Reverse mode is more appropriate if $\\#inputs\\gg\\#outputs$.\n",
    "    * e.g., ODE/PDE-constrained optimisation (cost fn), ML training (loss fn), goal-oriented (qoi)\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51afc1-153a-4abf-aa79-63a05197053c",
   "metadata": {},
   "source": [
    "[TODO: Simple examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e1f40-2462-446e-8db8-6cdaa1e163d4",
   "metadata": {},
   "source": [
    "## Validation: the dot product test\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: define test; check consistency of Tapenade output</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee4c63-4eef-4fef-a738-4e323c16b334",
   "metadata": {},
   "source": [
    "## Approach 2: Operator overloading\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: gradient values, tape, overloaded operators, tape unrolling</b>\n",
    "</div>\n",
    "\n",
    "#### Operator overloading tools:\n",
    "\n",
    "* LLVM\n",
    "    * [Enzyme](https://enzyme.mit.edu) <!-- is a plugin that performs automatic differentiation (AD) of statically analyzable LLVM. By operating on the LLVM level Enzyme is able to perform AD across a variety of languages (C/C++, Fortran , Julia, etc.) and perform optimization prior to AD -->\n",
    "* C/C++\n",
    "    * About 2 dozen AD tools!\n",
    "    * e.g., [ADIC](https://www.mcs.anl.gov/research/projects/adic), [ADOL-C](https://github.com/coin-or/ADOL-C), [Torch Autograd](https://pytorch.org/tutorials/advanced/cpp_autograd.html), [CoDiPack](https://github.com/SciCompKL/CoDiPack), [Sacado](https://docs.trilinos.org/dev/packages/sacado/doc/html/index.html), [dco/c++](https://nag.com/automatic-differentiation) [commercial]\n",
    "* Fortran\n",
    "    * [Differentia](https://github.com/Nicholaswogan/Differentia), [lots of abandonware...]\n",
    "* Python\n",
    "    * [PyADOL-C](https://github.com/b45ch1/pyadolc), [Jax](https://github.com/jax-ml/jax), [PyTorch Autograd](https://pytorch.org/docs/stable/autograd.html)\n",
    "* Julia\n",
    "    * About 2 dozen AD tools! https://juliadiff.org/\n",
    "    * e.g., Enzyme, [Zygote](https://fluxml.ai/Zygote.jl/stable), [ForwardDiff](https://juliadiff.org/ForwardDiff.jl/stable)\n",
    "    * [DifferentiationInterface](https://www.juliapackages.com/p/differentiationinterface)\n",
    "* Domain-specific\n",
    "    * [dolfin-adjoint/pyadjoint](https://github.com/dolfin-adjoint/pyadjoint) (Python/UFL - Firedrake & FEniCS)\n",
    "* And many more! https://autodiff.org/?module=Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode demo\n",
    "\n",
    "Builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * (a ** 3 - b * b / 3)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: Remark on `grad_fn`</b>\n",
    "</div>\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an external gradient. This defaults to a tensor of ones of the appropriate shape, but can be customised to account for cases where gradient values are propagated from another model or subcomponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.grad}\")\n",
    "print(f\"b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: how to calculate Hessian using forward then reverse </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Speelpenning demo\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: example for computing Hessian - see e.g., https://github.com/coin-or/ADOL-C/blob/master/ADOL-C/examples/speelpenning.cpp</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c050804-74bd-4a90-ab24-5769159645c1",
   "metadata": {},
   "source": [
    "##  Mathematical background on adjoint methods\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: derivation; continuous vs. discrete </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d46e52-4d76-4977-9c02-df6cacf2e264",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO: overview </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Exercise: Sensitivity analysis\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Exercise: Online training ML\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Further application\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>TODO </b>\n",
    "</div>\n",
    "\n",
    "* Data assimilation\n",
    "* Uncertainty quantification\n",
    "* PDE-constrained optimisation\n",
    "    * Demo in Firedrake\n",
    "* Goal-oriented error estimation\n",
    "    * Demo in Firedrake\n",
    "* Perhaps showcase combined adaptation + optimisation\n",
    "    * Demo in Firedrake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
