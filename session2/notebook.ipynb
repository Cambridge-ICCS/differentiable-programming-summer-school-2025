{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Session 2: Reverse mode differentiation\n",
    "\n",
    "[TODO: intro text]\n",
    "\n",
    "#### Remarks\n",
    "\n",
    "* We will restrict attention to operator overloading\n",
    "* We will restrict attention to PyTorch / Torch / FTorch\n",
    "\n",
    "#### Notation\n",
    "\n",
    "[TODO: Recall derivative notation]\n",
    "\n",
    "[TODO: Recall `_d` notation]\n",
    "\n",
    "[TODO: Mention `_b` notation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## History\n",
    "\n",
    "* Reverse mode (the subject of this session) was discovered by Linnainmaa in the 1970s.\n",
    "* Speelpenning introduced the modern formulation of reverse mode in the late 1980s.\n",
    "* Griewank improved the feasibility of reverse mode in 1992 by introducing checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "[TODO: Jacobian Transpose vector product (Again, matrix-free)]\n",
    "\n",
    "[TODO: \"Back-propagation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## PyTorch reverse mode demo\n",
    "\n",
    "Builds upon the PyTorch Autograd tutorial: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Consider two tensors comprised of vectors of length 2. We pass `requires_grad=True` to the constructor to mark these tensors as active for differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "b = torch.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Construct another tensor as a mathematical combination of the first two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3 * (a ** 3 - b * b / 3)\n",
    "print(f\"Q = {Q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "[TODO: Remark on `grad_fn`]\n",
    "\n",
    "Next, we compute derivatives of `Q` with its `backward` method. The backward method accepts an optional argument for an external gradient. This defaults to a tensor of ones of the appropriate shape, but can be customised to account for cases where gradient values are propagated from another model or subcomponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_gradient = torch.ones((2,))\n",
    "Q.backward(gradient=external_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We call the `backward` method on tensor `Q` to compute gradients with respect to any of its dependencies that were constructed with the `requires_grad=True` setting. As such, we may compute $\\mathrm{d}Q/\\mathrm{d}a$ and $\\mathrm{d}Q/\\mathrm{d}b$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"a = {a.grad}\")\n",
    "print(f\"b = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Convince yourself that these gradient values are correct. That is, differentiate the expression\n",
    "$$Q=3(a^3-b^2/3)$$\n",
    "with respect to $a$, substitute the values $a=(2,3)$ and $b=(6,4)$, and check the values match those above. Then do the same thing differentiating with respect to $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "##  Mathematical background on adjoint methods\n",
    "\n",
    "[TODO]\n",
    "\n",
    "[TODO: Continuous vs. discrete adjoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Differences with forward mode\n",
    "\n",
    "* Forward mode is more appropriate if $\\#inputs\\ll\\#outputs$.\n",
    "* Reverse mode is more appropriate if $\\#inputs\\gg\\#outputs$.\n",
    "    * e.g., ODE/PDE-constrained optimisation (cost fn), ML training (loss fn), goal-oriented (qoi)\n",
    "* Forward mode is computed *eagerly*, whereas reverse mode is done separately from the primal run.\n",
    "* Reverse mode tends to have higher memory requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "[TODO: Simple examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Validation: Dot product test\n",
    "\n",
    "[TODO: ensure consistency of fwd and rev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Second order\n",
    "\n",
    "[TODO: Hessian]\n",
    "\n",
    "[TODO: Forward then reverse]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Speelpenning demo\n",
    "\n",
    "[TODO: Example for computing Hessian - see e.g., https://github.com/coin-or/ADOL-C/blob/master/ADOL-C/examples/speelpenning.cpp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Exercise: Sensitivity analysis\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Exercise: Online training ML\n",
    "\n",
    "[TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Further application\n",
    "\n",
    "[TODO]\n",
    "\n",
    "* Data assimilation\n",
    "* Uncertainty quantification\n",
    "* PDE-constrained optimisation\n",
    "    * Demo in Firedrake\n",
    "* Goal-oriented error estimation\n",
    "    * Demo in Firedrake\n",
    "* Perhaps showcase combined adaptation + optimisation\n",
    "    * Demo in Firedrake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* S. Linnainmaa. *Taylor expansion of the accumulated rounding error*. BIT,\n",
    "16(2):146–160, 1976.\n",
    "* B. Speelpenning. *Compiling fast partial derivatives of functions given by algorithms*.\n",
    "University of Illinois, 1980.\n",
    "* A. Griewank. *Achieving logarithmic growth of temporal and spatial complexity in\n",
    "reverse automatic differentiation.* Optimization Methods & Software, 1:35–54, 1992."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
